{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Connaisseur \u2693\ufe0e A Kubernetes admission controller to integrate container image signature verification and trust pinning into a cluster. What is Connaisseur? \u2693\ufe0e Connaisseur ensures integrity and provenance of container images in a Kubernetes cluster. To do so, it intercepts resource creation or update requests sent to the Kubernetes cluster, identifies all container images and verifies their signatures against pre-configured public keys. Based on the result, it either accepts or denies those requests. Connaisseur is developed under three core values: Security , Usability , Compatibility . It is built to be extendable and currently aims to support the following signing solutions: Notary (V1) / Docker Content Trust Sigstore / Cosign (EXPERIMENTAL) Notary V2 (PLANNED) It provides several additional features: Detection Mode : warn but do not block invalid images Namespaced Validation : restrict validation to dedicated namespaces Alerting : send alerts based on verification result Feel free to reach out via GitHub Discussions ! Quick Start \u2693\ufe0e Getting started to verify image signatures is only a matter of minutes: Only try this out on a test cluster as deployments with unsigned images will be blocked. Connaisseur comes pre-configured with public keys for its own repository and Docker's official images (official images can be found here ). It can be fully configured via helm/values.yaml . For a quick start, clone the Connaisseur repository: git clone https://github.com/sse-secure-systems/connaisseur.git Next, install Connaisseur via Helm : helm install connaisseur helm --atomic --create-namespace --namespace connaisseur Once installation has finished, you are good to go. Successful verification can be tested via official Docker images like hello-world : kubectl run hello-world --image = docker.io/hello-world Or our signed testimage : kubectl run demo --image = docker.io/securesystemsengineering/testimage:signed Both will return pod/<name> created . However, when trying to deploy an unsigned image: kubectl run demo --image = docker.io/securesystemsengineering/testimage:unsigned Connaisseur returns an error (...) Unable to find signed digest (...) . Since the images above are signed using Docker Content Trust, you can inspect the trust data using docker trust inspect --pretty <image-name> . To uninstall Connaisseur use: helm uninstall connaisseur --namespace connaisseur Congrats you just validated the first images in your cluster! To get started configuring and verifying your own images and signatures, please follow our setup guide . How does it work? \u2693\ufe0e Integrity and provenance of container images deployed to a Kubernetes cluster can be ensured via digital signatures. On a very basic level, this requires two steps: Signing container images after building Verifying the image signatures before deployment Connaisseur aims to solve step two. This is achieved by implementing several validators , i.e. configurable signature verification modules for different signing schemes (e.g. Notary V1). While the detailed security considerations mainly depend on the applied scheme, Connaisseur in general verifies the signature over the container image content against a trust anchor (e.g. public key) and thus let's you ensure that images have not been tampered with (integrity) and come from a valid source (provenance). Trusted digests \u2693\ufe0e But what is actually verified? Container images can be referenced in two different ways based on their registry, repository, image name ( <registry>/<repository>/<image name> ) followed by either tag or digest: tag: docker.io/library/nginx: 1.20.1 digest: docker.io/library/nginx@ sha256:af9c...69ce While the tag is a mutable, human readable description, the digest is an immutable, inherent property of the image, namely the SHA256 hash of its content. This also means that a tag can correspond to varying digests whereas digests are unique for each image. The container runtime (e.g. containerd) compares the image content with the received digest before spinning up the container. As a result, Connaisseur just needs to make sure that only trusted digests (signed by a trusted entity) are passed to the container runtime. Depending on how an image for deployment is referenced, it will either attempt to translate the tag to a trusted digest or validate whether the digest is trusted. How the digest is signed in detail, where the signature is stored, what it is verfied against and how different image distribution and updating attacks are mitigated depends on the signature schemes. Mutating Admission controller \u2693\ufe0e How to validate images before deployment to a cluster? The Kubernetes API is the fundamental fabric behind the control plane. It allows operators and cluster components to communicate with each other and, for example, query, create, modify or delete Kubernetes resources. Each request passes through several phases such as authentication and authorization before it is persisted. Among those phases are two steps of admission control : mutating and validating admission. In those phases the API sends admission requests to configured webhooks (admission controllers) and receives admission responses (admit, deny, or modify). Connaisseur uses a mutating admission webhook, as requests are not only admitted or denied based on the validation result but might also require modification of contained images referenced by tags to trusted digests. The webhook is configured to only forward resource creation or update requests to the Connaisseur service running inside the cluster, since only deployments of images to the cluster are relevant for signature verification. This allows Connaisseur to intercept requests before deployment and based on the validation: admit if all images are referenced by trusted digests modify if all images can be translated to trusted digests deny if at least one of the requested images does not have a trusted digest Image Policy and Validators \u2693\ufe0e Now, how does Connaisseur process admission requests? A newly received request is first inspected for container image references that need to be validated (1). The resulting list of images referenced by tag or digest is passed to the image policy (2). The image policy matches the identified images to the configured validators and corresponding trust anchors (e.g. public keys) to be used for verification. Image policy and validator configuration form the central logic behind Connaisseur and are described in detail under usage/basics . Validation is the step where the actual signature verification takes place (3). For each image, the required trust data is retrieved from external sources such as Notary server, registry or Sigstore transparency log and validated against the preconfigured trust anchor (e.g. public key). This forms the basis for deciding on the request (4). In case no trusted digest is found for any of the images (i.e. either no signed digest available or no signature matching the public key), the whole request will be denied. Otherwise, Connaisseur will translate all image references in the original request to trusted digests and admit it (5). Compatibility \u2693\ufe0e Supported signature solutions and configuration options are documented under Validators . Connaisseur is expected to be compatible with most Kubernetes services. It has been successfully tested with: K3s \u2705 kind \u2705 MicroK8s \u2705 (enable DNS addon via sudo microk8s enable dns ) minikube \u2705 Amazon Elastic Kubernetes Service (EKS) \u2705 Azure Kubernetes Service (AKS) \u2705 Google Kubernetes Engine \u2705 SysEleven MetaKube \u2705 All registry interactions use the OCI Distribution Specification that is based on the Docker Registry HTTP API V2 which is the standard for all common image registries. For using Notary V1 as a signature solution, only some registries provide the required Notary server attached to the registry with e.g. shared authentication. Connaisseur has been tested with the following Notary V1 supporting image registries: Docker Hub \u2705 Harbor \u2705 (check our configuration notes ) Azure Container Registry (ACR) \u2705 (check our configuration notes ) In case you identify any incompatibilities, please create an issue Versions \u2693\ufe0e The latest stable version of Connaisseur is available on the master branch. Releases follow semantic versioning standards to facilitate compatibility. For each release, a signed container image tagged with the version is published in the Connaisseur Docker Hub repository . Latest developments are available on the develop branch, but should be considered unstable and no pre-built container image is provided. Development \u2693\ufe0e Connaisseur is open source and open development. We try to make major changes transparent via Architecture Decision Records (ADRs) and announce developments via GitHub Discussions . Information on responsible disclosure of vulnerabilities and tracking of past findings is available in the Security Policy . Bug reports should be filed as GitHub issues to share status and potential fixes with other users. We hope to get as many direct contributions and insights from the community as possible to steer further development. Please refer to our contributing guide , create an issue or reach out to us via GitHub Discussions Resources \u2693\ufe0e Several resources are available to learn more about connaisseur and related topics: \" Container Image Signatures in Kubernetes \" - blog post (full introduction) \" Integrity of Docker images \" - talk at Berlin Crypto Meetup ( The Update Framework , Notary , Docker Content Trust & Connaisseur [live demo]) \" Verifying Container Image Signatures from an OCI Registry in Kubernetes \" - blog post (experimental support of Sigstore / Cosign )","title":"Overview"},{"location":"#welcome-to-connaisseur","text":"A Kubernetes admission controller to integrate container image signature verification and trust pinning into a cluster.","title":"Welcome to Connaisseur"},{"location":"#what-is-connaisseur","text":"Connaisseur ensures integrity and provenance of container images in a Kubernetes cluster. To do so, it intercepts resource creation or update requests sent to the Kubernetes cluster, identifies all container images and verifies their signatures against pre-configured public keys. Based on the result, it either accepts or denies those requests. Connaisseur is developed under three core values: Security , Usability , Compatibility . It is built to be extendable and currently aims to support the following signing solutions: Notary (V1) / Docker Content Trust Sigstore / Cosign (EXPERIMENTAL) Notary V2 (PLANNED) It provides several additional features: Detection Mode : warn but do not block invalid images Namespaced Validation : restrict validation to dedicated namespaces Alerting : send alerts based on verification result Feel free to reach out via GitHub Discussions !","title":"What is Connaisseur?"},{"location":"#quick-start","text":"Getting started to verify image signatures is only a matter of minutes: Only try this out on a test cluster as deployments with unsigned images will be blocked. Connaisseur comes pre-configured with public keys for its own repository and Docker's official images (official images can be found here ). It can be fully configured via helm/values.yaml . For a quick start, clone the Connaisseur repository: git clone https://github.com/sse-secure-systems/connaisseur.git Next, install Connaisseur via Helm : helm install connaisseur helm --atomic --create-namespace --namespace connaisseur Once installation has finished, you are good to go. Successful verification can be tested via official Docker images like hello-world : kubectl run hello-world --image = docker.io/hello-world Or our signed testimage : kubectl run demo --image = docker.io/securesystemsengineering/testimage:signed Both will return pod/<name> created . However, when trying to deploy an unsigned image: kubectl run demo --image = docker.io/securesystemsengineering/testimage:unsigned Connaisseur returns an error (...) Unable to find signed digest (...) . Since the images above are signed using Docker Content Trust, you can inspect the trust data using docker trust inspect --pretty <image-name> . To uninstall Connaisseur use: helm uninstall connaisseur --namespace connaisseur Congrats you just validated the first images in your cluster! To get started configuring and verifying your own images and signatures, please follow our setup guide .","title":"Quick Start"},{"location":"#how-does-it-work","text":"Integrity and provenance of container images deployed to a Kubernetes cluster can be ensured via digital signatures. On a very basic level, this requires two steps: Signing container images after building Verifying the image signatures before deployment Connaisseur aims to solve step two. This is achieved by implementing several validators , i.e. configurable signature verification modules for different signing schemes (e.g. Notary V1). While the detailed security considerations mainly depend on the applied scheme, Connaisseur in general verifies the signature over the container image content against a trust anchor (e.g. public key) and thus let's you ensure that images have not been tampered with (integrity) and come from a valid source (provenance).","title":"How does it work?"},{"location":"#trusted-digests","text":"But what is actually verified? Container images can be referenced in two different ways based on their registry, repository, image name ( <registry>/<repository>/<image name> ) followed by either tag or digest: tag: docker.io/library/nginx: 1.20.1 digest: docker.io/library/nginx@ sha256:af9c...69ce While the tag is a mutable, human readable description, the digest is an immutable, inherent property of the image, namely the SHA256 hash of its content. This also means that a tag can correspond to varying digests whereas digests are unique for each image. The container runtime (e.g. containerd) compares the image content with the received digest before spinning up the container. As a result, Connaisseur just needs to make sure that only trusted digests (signed by a trusted entity) are passed to the container runtime. Depending on how an image for deployment is referenced, it will either attempt to translate the tag to a trusted digest or validate whether the digest is trusted. How the digest is signed in detail, where the signature is stored, what it is verfied against and how different image distribution and updating attacks are mitigated depends on the signature schemes.","title":"Trusted digests"},{"location":"#mutating-admission-controller","text":"How to validate images before deployment to a cluster? The Kubernetes API is the fundamental fabric behind the control plane. It allows operators and cluster components to communicate with each other and, for example, query, create, modify or delete Kubernetes resources. Each request passes through several phases such as authentication and authorization before it is persisted. Among those phases are two steps of admission control : mutating and validating admission. In those phases the API sends admission requests to configured webhooks (admission controllers) and receives admission responses (admit, deny, or modify). Connaisseur uses a mutating admission webhook, as requests are not only admitted or denied based on the validation result but might also require modification of contained images referenced by tags to trusted digests. The webhook is configured to only forward resource creation or update requests to the Connaisseur service running inside the cluster, since only deployments of images to the cluster are relevant for signature verification. This allows Connaisseur to intercept requests before deployment and based on the validation: admit if all images are referenced by trusted digests modify if all images can be translated to trusted digests deny if at least one of the requested images does not have a trusted digest","title":"Mutating Admission controller"},{"location":"#image-policy-and-validators","text":"Now, how does Connaisseur process admission requests? A newly received request is first inspected for container image references that need to be validated (1). The resulting list of images referenced by tag or digest is passed to the image policy (2). The image policy matches the identified images to the configured validators and corresponding trust anchors (e.g. public keys) to be used for verification. Image policy and validator configuration form the central logic behind Connaisseur and are described in detail under usage/basics . Validation is the step where the actual signature verification takes place (3). For each image, the required trust data is retrieved from external sources such as Notary server, registry or Sigstore transparency log and validated against the preconfigured trust anchor (e.g. public key). This forms the basis for deciding on the request (4). In case no trusted digest is found for any of the images (i.e. either no signed digest available or no signature matching the public key), the whole request will be denied. Otherwise, Connaisseur will translate all image references in the original request to trusted digests and admit it (5).","title":"Image Policy and Validators"},{"location":"#compatibility","text":"Supported signature solutions and configuration options are documented under Validators . Connaisseur is expected to be compatible with most Kubernetes services. It has been successfully tested with: K3s \u2705 kind \u2705 MicroK8s \u2705 (enable DNS addon via sudo microk8s enable dns ) minikube \u2705 Amazon Elastic Kubernetes Service (EKS) \u2705 Azure Kubernetes Service (AKS) \u2705 Google Kubernetes Engine \u2705 SysEleven MetaKube \u2705 All registry interactions use the OCI Distribution Specification that is based on the Docker Registry HTTP API V2 which is the standard for all common image registries. For using Notary V1 as a signature solution, only some registries provide the required Notary server attached to the registry with e.g. shared authentication. Connaisseur has been tested with the following Notary V1 supporting image registries: Docker Hub \u2705 Harbor \u2705 (check our configuration notes ) Azure Container Registry (ACR) \u2705 (check our configuration notes ) In case you identify any incompatibilities, please create an issue","title":"Compatibility"},{"location":"#versions","text":"The latest stable version of Connaisseur is available on the master branch. Releases follow semantic versioning standards to facilitate compatibility. For each release, a signed container image tagged with the version is published in the Connaisseur Docker Hub repository . Latest developments are available on the develop branch, but should be considered unstable and no pre-built container image is provided.","title":"Versions"},{"location":"#development","text":"Connaisseur is open source and open development. We try to make major changes transparent via Architecture Decision Records (ADRs) and announce developments via GitHub Discussions . Information on responsible disclosure of vulnerabilities and tracking of past findings is available in the Security Policy . Bug reports should be filed as GitHub issues to share status and potential fixes with other users. We hope to get as many direct contributions and insights from the community as possible to steer further development. Please refer to our contributing guide , create an issue or reach out to us via GitHub Discussions","title":"Development"},{"location":"#resources","text":"Several resources are available to learn more about connaisseur and related topics: \" Container Image Signatures in Kubernetes \" - blog post (full introduction) \" Integrity of Docker images \" - talk at Berlin Crypto Meetup ( The Update Framework , Notary , Docker Content Trust & Connaisseur [live demo]) \" Verifying Container Image Signatures from an OCI Registry in Kubernetes \" - blog post (experimental support of Sigstore / Cosign )","title":"Resources"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct \u2693\ufe0e Our Pledge \u2693\ufe0e In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u2693\ufe0e Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u2693\ufe0e Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u2693\ufe0e This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u2693\ufe0e Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at connaisseur@securesystems.dev . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u2693\ufe0e This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at connaisseur@securesystems.dev . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing \u2693\ufe0e We hope to steer development of Connaisseur from demand of the community and are excited about direct contributions to improve the tool! The following guide is meant to help you get started with contributing to Connaisseur. In case of questions or feedback, feel free to reach out to us . We are committed to positive interactions between all contributors of the project. To ensure this, please follow the Code of Conduct in all communications. Discuss Problems, Raise Bugs and Propose Feature Ideas \u2693\ufe0e We are happy you made it here! In case you want to share your feedback, need support, want to discuss issues from using Connaisseur in your own projects, have ideas for new features or just want to connect with us, please reach out via GitHub Discussions . If you want to raise any bugs you found or make a feature request, feel free to create an issue with an informative title and description. While issues are a great way to discuss problems, bugs and new features, a direct proposal via a pull request can sometimes say more than a thousand words. So be bold and contribute to the code as described in the next section ! In case you require a more private communication, you can reach us via connaisseur@securesystems.dev . Contribute to Source Code \u2693\ufe0e The following steps will help you make code contributions to Connaisseur and ensure good code quality and workflow. This includes the following steps: Setup your environment : Setup up your local environment to best interact with the code. Further information is given below . Make atomic changes : Changes should be atomic. As such, pull requests should contain only few commits, and each commit should only fix one issue or implement one feature, with a concise commit message. Test your changes : Test any changes locally for code quality and functionality and add new tests for any additional code. How to test is described below . Create semantic, conventional and signed commits : Any commits should follow a simple semantic convention to help structure the work on Connaisseur. The convention is described below . For security reasons and since integrity is at the core of this project, code merged into master must be signed. How we achieve this is described below . Create Pull Request : We consider code review central to quality and security of code. Therefore, a pull request (PR) to the develop branch should be created for each contribution. It will be reviewed, and potential improvements may be discussed within the PR. After approval, changes will be merged and moved to the master branch with the next release. Setup the Environment \u2693\ufe0e To start contributing, you will need to setup your local environment. First step is to get the source code by cloning this repository: git clone git@github.com:sse-secure-systems/connaisseur.git In order to review the effects of your changes, you should create your own Kubernetes cluster and install Connaisseur. This is described in the getting started . A simple starting point may be a minikube cluster with e.g. a Docker Hub repository for maintaining your test images and trust data. In case you make changes to the Connaisseur container image itself or code for that matter, you need to re-build the image and install it locally for testing. This requires a few steps: In helm/values.yaml , set imagePullPolicy to IfNotPresent . Configure your local environment to use the Kubernetes Docker daemon. In minikube, this can be done via eval (minikube docker-env) . Build the Connaisseur container image via make docker . Install Connaisseur as usual via make install . Test Changes \u2693\ufe0e Tests and linting are important to ensure code quality, functionality and security. We therefore aim to keep the code coverage high. We are running several automated tests in the CI pipeline . Application code is tested via pytest and linted via pylint . When making changes to the application code, please directly provide tests for your changes. We recommend using black for autoformatting to simplify linting and reduce review effort. It can be installed via: pip3 install black To autoformat the code: black <path-to-repository>/connaisseur Changes can also be tested locally. We recommend the following approach for running pytest in a container: docker run -it --rm -v <path-to-repository>:/data --entrypoint=ash python:alpine cd data pip3 install -r requirements_dev.txt pytest --cov=connaisseur --cov-report=xml tests/ This helps identify bugs in changes before pushing. INFO We believe that testing should not only ensure functionality, but also aim to test for expected security issues like injections and appreciate if security tests are added with new functionalities. Besides the unit testing and before any PR can be merged, an integration test is carried out whereby: Connaisseur is successfully installed in a test cluster a non-signed image is deployed to the cluster and denied an image signed with an unrelated key is denied a signed image is deployed to the cluster and passed Connaisseur is successfully uninstalled You can also run this integration test on a local cluster. There is a more detailed guided on how to do that. If you are changing documentation, you can simply inspect your changes locally via: docker run --rm -it -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material Signed Commits and Pull Requests \u2693\ufe0e All changes to the develop and master branch must be signed which is enforced via branch protection . This can be achieved by only fast-forwarding signed commits or signing of merge commits by a contributor. Consequently, while we appreciate signed commits in PRs, we do not require it. A general introduction into signing commits can for example be found in the With Blue Ink blog . For details on setting everything up for GitHub, please follow the steps in the Documentation . Once you have generated your local GPG key, added it to your GitHub account and informed Git about it, you are setup to create signed commits. We recommend to configure Git to sign commits by default via: git config commit.gpgsign true This avoids forgetting to use the -S flag when committing changes. In case it happens anyways, you can always rebase to sign earlier commits: git rebase -i master You can then mark all commits that need to be signed as edit and sign them without any other changes via: git commit -S --amend --no-edit Finally, you force push to overwrite the unsigned commits via git push -f . Semantic and Conventional Commits \u2693\ufe0e For Connaisseur, we want to use semantic and conventional commits to ensure good readability of code changes. A good introduction to the topic can be found in this blog post . Commit messages should consist of header, body and footer. Such a commit message takes the following form: git commit -m \"<header>\" -m \"<body>\" -m \"<footer>\" The three parts should consist of the following: header : Comprises of a commit type (common types are described below) and a concise description of the actual change, e.g. fix: extend registry validation regex to custom ports . body (optional): Contains information on the motivation behind the change and considerations for the resolution, The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number. . footer (optional): Used to reference PRs, issues or contributors and mark consequences such as breaking changes, e.g. Fix #<issue-number> We want to use the following common types in the header: build : changes to development and building ci : CI related changes docs : changes in the documentation feat : adding of new features fix : fixing an issue or bug refactor : adjustment of code base to improve code quality or performance but not adding a feature or fixing a bug test : testing related changes A complete commit message could therefore look as follows: git commit -m \"fix: extend registry validation regex to custom ports\" -m \"The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number.\" -m \"Fix #3\" Enjoy! \u2693\ufe0e Please be bold and contribute!","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We hope to steer development of Connaisseur from demand of the community and are excited about direct contributions to improve the tool! The following guide is meant to help you get started with contributing to Connaisseur. In case of questions or feedback, feel free to reach out to us . We are committed to positive interactions between all contributors of the project. To ensure this, please follow the Code of Conduct in all communications.","title":"Contributing"},{"location":"CONTRIBUTING/#discuss-problems-raise-bugs-and-propose-feature-ideas","text":"We are happy you made it here! In case you want to share your feedback, need support, want to discuss issues from using Connaisseur in your own projects, have ideas for new features or just want to connect with us, please reach out via GitHub Discussions . If you want to raise any bugs you found or make a feature request, feel free to create an issue with an informative title and description. While issues are a great way to discuss problems, bugs and new features, a direct proposal via a pull request can sometimes say more than a thousand words. So be bold and contribute to the code as described in the next section ! In case you require a more private communication, you can reach us via connaisseur@securesystems.dev .","title":"Discuss Problems, Raise Bugs and Propose Feature Ideas"},{"location":"CONTRIBUTING/#contribute-to-source-code","text":"The following steps will help you make code contributions to Connaisseur and ensure good code quality and workflow. This includes the following steps: Setup your environment : Setup up your local environment to best interact with the code. Further information is given below . Make atomic changes : Changes should be atomic. As such, pull requests should contain only few commits, and each commit should only fix one issue or implement one feature, with a concise commit message. Test your changes : Test any changes locally for code quality and functionality and add new tests for any additional code. How to test is described below . Create semantic, conventional and signed commits : Any commits should follow a simple semantic convention to help structure the work on Connaisseur. The convention is described below . For security reasons and since integrity is at the core of this project, code merged into master must be signed. How we achieve this is described below . Create Pull Request : We consider code review central to quality and security of code. Therefore, a pull request (PR) to the develop branch should be created for each contribution. It will be reviewed, and potential improvements may be discussed within the PR. After approval, changes will be merged and moved to the master branch with the next release.","title":"Contribute to Source Code"},{"location":"CONTRIBUTING/#setup-the-environment","text":"To start contributing, you will need to setup your local environment. First step is to get the source code by cloning this repository: git clone git@github.com:sse-secure-systems/connaisseur.git In order to review the effects of your changes, you should create your own Kubernetes cluster and install Connaisseur. This is described in the getting started . A simple starting point may be a minikube cluster with e.g. a Docker Hub repository for maintaining your test images and trust data. In case you make changes to the Connaisseur container image itself or code for that matter, you need to re-build the image and install it locally for testing. This requires a few steps: In helm/values.yaml , set imagePullPolicy to IfNotPresent . Configure your local environment to use the Kubernetes Docker daemon. In minikube, this can be done via eval (minikube docker-env) . Build the Connaisseur container image via make docker . Install Connaisseur as usual via make install .","title":"Setup the Environment"},{"location":"CONTRIBUTING/#test-changes","text":"Tests and linting are important to ensure code quality, functionality and security. We therefore aim to keep the code coverage high. We are running several automated tests in the CI pipeline . Application code is tested via pytest and linted via pylint . When making changes to the application code, please directly provide tests for your changes. We recommend using black for autoformatting to simplify linting and reduce review effort. It can be installed via: pip3 install black To autoformat the code: black <path-to-repository>/connaisseur Changes can also be tested locally. We recommend the following approach for running pytest in a container: docker run -it --rm -v <path-to-repository>:/data --entrypoint=ash python:alpine cd data pip3 install -r requirements_dev.txt pytest --cov=connaisseur --cov-report=xml tests/ This helps identify bugs in changes before pushing. INFO We believe that testing should not only ensure functionality, but also aim to test for expected security issues like injections and appreciate if security tests are added with new functionalities. Besides the unit testing and before any PR can be merged, an integration test is carried out whereby: Connaisseur is successfully installed in a test cluster a non-signed image is deployed to the cluster and denied an image signed with an unrelated key is denied a signed image is deployed to the cluster and passed Connaisseur is successfully uninstalled You can also run this integration test on a local cluster. There is a more detailed guided on how to do that. If you are changing documentation, you can simply inspect your changes locally via: docker run --rm -it -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material","title":"Test Changes"},{"location":"CONTRIBUTING/#signed-commits-and-pull-requests","text":"All changes to the develop and master branch must be signed which is enforced via branch protection . This can be achieved by only fast-forwarding signed commits or signing of merge commits by a contributor. Consequently, while we appreciate signed commits in PRs, we do not require it. A general introduction into signing commits can for example be found in the With Blue Ink blog . For details on setting everything up for GitHub, please follow the steps in the Documentation . Once you have generated your local GPG key, added it to your GitHub account and informed Git about it, you are setup to create signed commits. We recommend to configure Git to sign commits by default via: git config commit.gpgsign true This avoids forgetting to use the -S flag when committing changes. In case it happens anyways, you can always rebase to sign earlier commits: git rebase -i master You can then mark all commits that need to be signed as edit and sign them without any other changes via: git commit -S --amend --no-edit Finally, you force push to overwrite the unsigned commits via git push -f .","title":"Signed Commits and Pull Requests"},{"location":"CONTRIBUTING/#semantic-and-conventional-commits","text":"For Connaisseur, we want to use semantic and conventional commits to ensure good readability of code changes. A good introduction to the topic can be found in this blog post . Commit messages should consist of header, body and footer. Such a commit message takes the following form: git commit -m \"<header>\" -m \"<body>\" -m \"<footer>\" The three parts should consist of the following: header : Comprises of a commit type (common types are described below) and a concise description of the actual change, e.g. fix: extend registry validation regex to custom ports . body (optional): Contains information on the motivation behind the change and considerations for the resolution, The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number. . footer (optional): Used to reference PRs, issues or contributors and mark consequences such as breaking changes, e.g. Fix #<issue-number> We want to use the following common types in the header: build : changes to development and building ci : CI related changes docs : changes in the documentation feat : adding of new features fix : fixing an issue or bug refactor : adjustment of code base to improve code quality or performance but not adding a feature or fixing a bug test : testing related changes A complete commit message could therefore look as follows: git commit -m \"fix: extend registry validation regex to custom ports\" -m \"The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number.\" -m \"Fix #3\"","title":"Semantic and Conventional Commits"},{"location":"CONTRIBUTING/#enjoy","text":"Please be bold and contribute!","title":"Enjoy!"},{"location":"RELEASING/","text":"Releasing \u2693\ufe0e Releasing a new version of Connaisseur includes the following steps: adding a new version tag creating a changelog from commit messages creating a PR from develop (new version) to master (current version) pushing a new version of the Connaisseur image to Dockerhub merging in the PR & push tag creating release page shoot some trouble Adding new Tag \u2693\ufe0e Before adding the new tag, make sure the Connaisseur version is updated in the helm/values.yaml and applies the semantic versioning guidelines: fixes increment PATCH version, non-breaking features increment MINOR version, breaking features increment MAJOR version. Then add the tag (on develop branch) with git tag v<new-conny-version> (e.g. git tag v1.4.6 ). Creating Changelog \u2693\ufe0e A changelog text, including all new commits from one to another version, can be automatically generated using the scrips/changelogger.py script. You have to update the two ref1 and ref2 variables in the main method with the the old and new git tags. So if you e.g. want to release a new version from v1.4.5 to v1.4.6 , then you have to set ref1 = \"v1.4.5\" and ref2 = \"v1.4.6\" . Then simply run python scripts/changelogger.py > CHANGELOG.md , storing the changelog in a new file CHANGELOG.md (we won't keep this file, it's just for convenient storing purpose). This file will include all new commits, categorized by their type (e.g. fix, feat, docs, etc.), but may include some mistakes so take a manual look if everything looks in order. Things to look out for: multiple headings for the same category broken pull request links None appended on end of line Creating PR \u2693\ufe0e Create a PR from develop to master , putting the changelog text as description and wait for someone to approve it. Push new Connaisseur Image \u2693\ufe0e When the PR is approved and ready to be merged, first push the new Connaisseur image to Dockerhub, as it will be used in the release pipeline. Run make docker to build the new version of the docker image and then DOCKER_CONTENT_TRUST=1 docker image push securesystemsengineering/connaisseur:<new-version> to push and sign it. You'll obviously need the right private key and passphrase for doing so. You also need to be in the list of valid signers for Connaisseur. If not already (you can check with docker trust inspect securesystemsengineering/connaisseur --pretty ) you'll need to contact Philipp Belitz . Merge PR \u2693\ufe0e Run git checkout master to switch to the master branch and then run git merge develop to merge develop in. Then run git push and git push --tags to publish all changes and the new tag. Create Release Page \u2693\ufe0e Finally a release on Github should be created. Go to the Connaisseur releases page , then click Draft a new release . There you have to enter the new tag version, a title (usually Version <new-version> ) and the changelog text as description. Then click Publish release and you're done! (You can delete the CHANGELOG.md file now. Go and do it.) Shoot Trouble \u2693\ufe0e Be aware that this isn't a completely fleshed out, highly available, hyper scalable and fully automated workflow, backed up by state-of-the-art blockchain technology and 24/7 incident response team coverage with global dominance! Not yet at least. For now things will probably break, so make sure that in the end everything looks to be in order and the new release can be seen on the Github page, tagged with Latest release and pointing to the correct version of Connaisseur. Good Luck!","title":"Releasing"},{"location":"RELEASING/#releasing","text":"Releasing a new version of Connaisseur includes the following steps: adding a new version tag creating a changelog from commit messages creating a PR from develop (new version) to master (current version) pushing a new version of the Connaisseur image to Dockerhub merging in the PR & push tag creating release page shoot some trouble","title":"Releasing"},{"location":"RELEASING/#adding-new-tag","text":"Before adding the new tag, make sure the Connaisseur version is updated in the helm/values.yaml and applies the semantic versioning guidelines: fixes increment PATCH version, non-breaking features increment MINOR version, breaking features increment MAJOR version. Then add the tag (on develop branch) with git tag v<new-conny-version> (e.g. git tag v1.4.6 ).","title":"Adding new Tag"},{"location":"RELEASING/#creating-changelog","text":"A changelog text, including all new commits from one to another version, can be automatically generated using the scrips/changelogger.py script. You have to update the two ref1 and ref2 variables in the main method with the the old and new git tags. So if you e.g. want to release a new version from v1.4.5 to v1.4.6 , then you have to set ref1 = \"v1.4.5\" and ref2 = \"v1.4.6\" . Then simply run python scripts/changelogger.py > CHANGELOG.md , storing the changelog in a new file CHANGELOG.md (we won't keep this file, it's just for convenient storing purpose). This file will include all new commits, categorized by their type (e.g. fix, feat, docs, etc.), but may include some mistakes so take a manual look if everything looks in order. Things to look out for: multiple headings for the same category broken pull request links None appended on end of line","title":"Creating Changelog"},{"location":"RELEASING/#creating-pr","text":"Create a PR from develop to master , putting the changelog text as description and wait for someone to approve it.","title":"Creating PR"},{"location":"RELEASING/#push-new-connaisseur-image","text":"When the PR is approved and ready to be merged, first push the new Connaisseur image to Dockerhub, as it will be used in the release pipeline. Run make docker to build the new version of the docker image and then DOCKER_CONTENT_TRUST=1 docker image push securesystemsengineering/connaisseur:<new-version> to push and sign it. You'll obviously need the right private key and passphrase for doing so. You also need to be in the list of valid signers for Connaisseur. If not already (you can check with docker trust inspect securesystemsengineering/connaisseur --pretty ) you'll need to contact Philipp Belitz .","title":"Push new Connaisseur Image"},{"location":"RELEASING/#merge-pr","text":"Run git checkout master to switch to the master branch and then run git merge develop to merge develop in. Then run git push and git push --tags to publish all changes and the new tag.","title":"Merge PR"},{"location":"RELEASING/#create-release-page","text":"Finally a release on Github should be created. Go to the Connaisseur releases page , then click Draft a new release . There you have to enter the new tag version, a title (usually Version <new-version> ) and the changelog text as description. Then click Publish release and you're done! (You can delete the CHANGELOG.md file now. Go and do it.)","title":"Create Release Page"},{"location":"RELEASING/#shoot-trouble","text":"Be aware that this isn't a completely fleshed out, highly available, hyper scalable and fully automated workflow, backed up by state-of-the-art blockchain technology and 24/7 incident response team coverage with global dominance! Not yet at least. For now things will probably break, so make sure that in the end everything looks to be in order and the new release can be seen on the Github page, tagged with Latest release and pointing to the correct version of Connaisseur. Good Luck!","title":"Shoot Trouble"},{"location":"SECURITY/","text":"Security Policy \u2693\ufe0e Supported Versions \u2693\ufe0e While all known vulnerabilities are listed below and we intent to fix vulnerabilities as soon as we become aware, both, Python and OS packages of the Connaisseur image may become vulnerable over time and we suggest to frequently update to the latest version of Connaisseur or rebuilding the image from source yourself. At present, we only support the latest version. We stick to semantic versioning, so unless the major version changes, updating Conaisseur should never break your installation. Known Vulnerabilities \u2693\ufe0e Title Affected versions Fixed version Description initContainers not validated \u2264 1.3.0 1.3.1 prior to version 1.3.1 Connaisseur did not validate initContainers which allows unverified images to the cluster Reporting a Vulnerability \u2693\ufe0e We are very grateful for reports on vulnerabilities discovered in the project, specifically as it is intended to increase security for the community. We aim to investigate and fix these as soon as possible. Please submit vulnerabilities to connaisseur@securesystems.dev .","title":"Security Policy"},{"location":"SECURITY/#security-policy","text":"","title":"Security Policy"},{"location":"SECURITY/#supported-versions","text":"While all known vulnerabilities are listed below and we intent to fix vulnerabilities as soon as we become aware, both, Python and OS packages of the Connaisseur image may become vulnerable over time and we suggest to frequently update to the latest version of Connaisseur or rebuilding the image from source yourself. At present, we only support the latest version. We stick to semantic versioning, so unless the major version changes, updating Conaisseur should never break your installation.","title":"Supported Versions"},{"location":"SECURITY/#known-vulnerabilities","text":"Title Affected versions Fixed version Description initContainers not validated \u2264 1.3.0 1.3.1 prior to version 1.3.1 Connaisseur did not validate initContainers which allows unverified images to the cluster","title":"Known Vulnerabilities"},{"location":"SECURITY/#reporting-a-vulnerability","text":"We are very grateful for reports on vulnerabilities discovered in the project, specifically as it is intended to increase security for the community. We aim to investigate and fix these as soon as possible. Please submit vulnerabilities to connaisseur@securesystems.dev .","title":"Reporting a Vulnerability"},{"location":"basics/","text":"Basics \u2693\ufe0e This page explains how to configure, setup and manage Connaisseur. In addition, the core concepts are introduced. Admission control, validators and image policy \u2693\ufe0e Connaisseur works as a mutating admission controller . It intercepts all CREATE and UPDATE resource requests for Pods , Deployments , ReplicationControllers , ReplicaSets , DaemonSets , StatefulSets , Jobs , and CronJobs and extracts all image references for validation. Validation relies on two core concepts: image policy and validators. A validator is a set of configuration options required for validation like the type of signature, public key to use for verification, path to signature data, or authentication. The image policy defines a set of rules which maps different images to those validators. This is done via glob matching of the image name which for example allows to use different validators for different registries, repositories, images or even tags. This is specifically useful when using public or external images from other entities like Docker's official images or different keys in a more complex development team. Using Connaisseur \u2693\ufe0e Some general administration tasks like deployment or uninstallation when using Connaisseur are described in this section. Requirements \u2693\ufe0e Using Connaisseur requires Git , a Kubernetes cluster, Helm and Docker (only for local setups) to be installed and set up. Get the code \u2693\ufe0e The Connaisseur source code can be cloned directly from GitHub and includes the application and Helm charts in a single repository. git clone https://github.com/sse-secure-systems/connaisseur.git Configure \u2693\ufe0e The configuration of Connaisseur is completely done in the helm/values.yaml . The upper deployment section offers some general Kubernetes typical configurations like image version or resources. The configuration of validators and image policy is described in detail below and stepwise in the getting started guide . Features are described on the respective pages . Connaisseur ships with a pre-configuration that does not need any adjustments for testing. However, validating your own images requires additional configuration. Deploy \u2693\ufe0e Install Connaisseur by using Helm (alternatively use Makefile ). helm install connaisseur helm --atomic --create-namespace --namespace connaisseur This will install Connaisseur in its own namespace called connaisseur . The installation itself may take some minutes, as the installation order of the Connaisseur components is critical. Only when the Connaisseur Pods are ready and running, the Admission Webhook can be applied for intercepting requests. Check \u2693\ufe0e When everything is installed, you can check whether all the pods are up by running kubectl get all -n connaisseur . $ kubectl get all -n connaisseur NAME READY STATUS RESTARTS AGE pod/connaisseur-deployment-78d8975596-42tkw 1 /1 Running 0 22s pod/connaisseur-deployment-78d8975596-5c4c6 1 /1 Running 0 22s pod/connaisseur-deployment-78d8975596-kvrj6 1 /1 Running 0 22s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/connaisseur-svc ClusterIP 10 .108.220.34 <none> 443 /TCP 22s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/connaisseur-deployment 3 /3 3 3 22s NAME DESIRED CURRENT READY AGE replicaset.apps/connaisseur-deployment-78d8975596 3 3 3 22s Use \u2693\ufe0e To use Connaisseur, simply try running some images or apply a deployment. In case you use the pre-configuration, you could for example run the following commands: kubectl run demo --image = docker.io/securesystemsengineering/testimage:unsigned > Error from server: admission webhook \"connaisseur-svc.connaisseur.svc\" denied the request ( ... ) . kubectl run hello-world --image = docker.io/hello-world > pod/hello-world created Delete \u2693\ufe0e Just like for installation, Helm can also be used to delete Connaisseur from your cluster. helm uninstall connaisseur -n connaisseur In case uninstallation fails or problems occur during subsequent installation, you can manually remove all resources: kubectl delete all,mutatingwebhookconfigurations,clusterroles,clusterrolebindings,configmaps,imagepolicies,secrets,serviceaccounts,customresourcedefinitions -lapp.kubernetes.io/instance = connaisseur kubectl delete namespaces connaisseur Connaisseur for example also installs a CutstomResourceDefinition imagepolicies.connaisseur.policy that validates its configuration. In case of major releases, the configuration structure might change which causes installation to fail and you might have to delete it manually. Makefile \u2693\ufe0e Alternatively to using Helm, you can also run the Makefile for installing, deleting and more. Here the available commands: make install -- Runs the helm install command. make uninstall -- Switches the namespace to connaisseur , uses the helm uninstall command and deletes the connaisseur namespace. make annihilate -- Deletes all resources labeled with app.kubernetes.io/instance=connaisseur . This command is usually helpful, should the normal make uninstall not work. make docker -- Builds the connaisseur and connaisseur:helm-hook (relevant for installing Connaisseur) docker images. Detailed configuration \u2693\ufe0e All configuration is done in the helm/values.yaml . The configuration of features is only described in the corresponding section . Validators \u2693\ufe0e The validators are configured in the validators field, which defines a list of validator objects. A validator defines what kind of signatures are to be expected, how signatures are to be validated, against which trust root and how to access the signature data. For example, images might be signed with Docker Content Trust and reside in a private registry. Thus the validator would need to specify notaryv1 as type, the notary host and the required credentials. The specific validator type should be chosen based on the use case. A list of supported validator types can be found here . All validators share a similar structure for configuration. For specifics and additional options, please review the dedicated page of the validator type. Connaisseur comes with a few validators preconfigured including one for Docker's official images. The preconfigured validators can be removed. However, when removing the securesystemsengineering_official make sure to also exclude Connaisseur from validation either via the static allow validator or namespaced validation . The special case of static validators used to simply allow or deny images without verification is described below. Configuration options \u2693\ufe0e .validators[*] in helm/values.yaml supports the following keys: key default required description name - The name of the validator, which is referenced in the image policy. It can only contain letters, numbers and '-'. If the name is default , it will be used if no validator is specified. type - The type of the validator which is dependent on the validator . trust_roots - A list of trust anchors to validate the signatures with. In practice, this is typically a list of public keys. trust_roots.[*].name - The name of the trust anchor, which is referenced in the image policy. If the name is default , it will be used if no key is specified. trust_roots.[*].key - The value of the trust anchor, most commonly a PEM encoded public key. auth - - Credentials that should be used in case authentication is required for validation. Details are provided on validator-specific pages. Further configuration fields specific to the validator type are described in the respective section. There is a special behavior, when a validator or one of the trust roots is named default . In this case, should an image policy rule not specify a validator or trust root to use, the one named default will be used instead. This also means there can only be one validator named default and for the trust roots, there can only be one called default within a single validator. Example \u2693\ufe0e validators : - name : default type : notaryv1 host : notary.docker.io trust_roots : - name : default type : key key : | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsx28WV7BsQfnHF1kZmpdCTTLJaWe d0CA+JOi8H4REuBaWSZ5zPDe468WuOJ6f71E7WFg3CVEVYHuoZt2UYbN/Q== -----END PUBLIC KEY----- auth : username : superuser password : lookatmeimjumping - name : cosigner type : cosign trust_roots : - name : mykey key : | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEIFXO1w6oj0oI2Fk9SiaNJRKTiO9d ksm6hFczQAq+FDdw0istEdCwcHO61O/0bV+LC8jqFoomA28cT+py6FcSYw== -----END PUBLIC KEY----- Static validators \u2693\ufe0e Static validators are a special type of validator that does not validate any signatures. Depending on the approve value being true or false , they either allow or deny all images for which they are specified as validator. This for example allows to implement allowlist or denylist . Configuration options \u2693\ufe0e key default required description name - The name of the validator, which will be used to reference it in the image policy. type - Needs to be set to static . approve - Set to true to allow all images, or to false to deny. Example \u2693\ufe0e validators : - name : allow type : static approve : true - name : deny type : static approve : false Image policy \u2693\ufe0e The image policy is defined in the policy field and acts as a list of rule objects to determine which image should be validated by which validator (and potentially some further configurations). Each rule provides a pattern that is used to identify the images it applies to and a reference to a validator. In case no validator is specified, the validator with name default is used if it exists. If no key is specified for a validator, the trust root with name default is used if it exists. For each image in the admission request, only a single rule in the image policy will apply, the one with the most specific matching pattern field. This is determined by the following algorithm: A given image is matched against all rule patterns. All matching patterns are compared to one another to determine the most specific one (see below). Only two patterns are compared at a time; the more specific one then is compared to the next one and so forth. Specificity is determined as follows: Patterns are split into components (delimited by \"/\"). The pattern that has a higher number of components wins (is considered more specific). Should the two patterns that are being compared have equal number of components, the longest common prefix between each pattern component and corresponding image component are calculated (for this purpose, image identifiers are also split into components). The pattern with the longest common prefix in one component, starting from the leftmost, wins. Should all longest common prefixes of all components between the two compared patterns be equal, the pattern with a longer component, starting from the leftmost, wins. The rule whose pattern has won all comparisons is considered the most specific rule. Return the most specific rule. Note: Should an image have no rules to match against, an error will be raised, which is why it is recommended to always have an \"catch-all\" rule like *:* . Configuration options \u2693\ufe0e .policy[*] in helm/values.yaml supports the following keys: key default required description pattern - Globbing pattern to match an image name against. validator default The name of a validator in the validators list. If not provided, the validator with name default is used if it exists. with - Additional parameters to use for a validator. See more specifics in validator section . with.key default - The name of a trust root, which is specified within the referenced validator. If not provided, the trust root with name default is used if it exists. Example \u2693\ufe0e policy : - pattern : \"*:*\" - pattern : \"docker.io/myrepo/*:*\" validator : cosigner with : key : mykey - pattern : \"docker.io/myrepo/deniedimage:*\" validator : deny - pattern : \"docker.io/myrepo/allowedimage:v*\" validator : allow","title":"Basics"},{"location":"basics/#basics","text":"This page explains how to configure, setup and manage Connaisseur. In addition, the core concepts are introduced.","title":"Basics"},{"location":"basics/#admission-control-validators-and-image-policy","text":"Connaisseur works as a mutating admission controller . It intercepts all CREATE and UPDATE resource requests for Pods , Deployments , ReplicationControllers , ReplicaSets , DaemonSets , StatefulSets , Jobs , and CronJobs and extracts all image references for validation. Validation relies on two core concepts: image policy and validators. A validator is a set of configuration options required for validation like the type of signature, public key to use for verification, path to signature data, or authentication. The image policy defines a set of rules which maps different images to those validators. This is done via glob matching of the image name which for example allows to use different validators for different registries, repositories, images or even tags. This is specifically useful when using public or external images from other entities like Docker's official images or different keys in a more complex development team.","title":"Admission control, validators and image policy"},{"location":"basics/#using-connaisseur","text":"Some general administration tasks like deployment or uninstallation when using Connaisseur are described in this section.","title":"Using Connaisseur"},{"location":"basics/#requirements","text":"Using Connaisseur requires Git , a Kubernetes cluster, Helm and Docker (only for local setups) to be installed and set up.","title":"Requirements"},{"location":"basics/#get-the-code","text":"The Connaisseur source code can be cloned directly from GitHub and includes the application and Helm charts in a single repository. git clone https://github.com/sse-secure-systems/connaisseur.git","title":"Get the code"},{"location":"basics/#configure","text":"The configuration of Connaisseur is completely done in the helm/values.yaml . The upper deployment section offers some general Kubernetes typical configurations like image version or resources. The configuration of validators and image policy is described in detail below and stepwise in the getting started guide . Features are described on the respective pages . Connaisseur ships with a pre-configuration that does not need any adjustments for testing. However, validating your own images requires additional configuration.","title":"Configure"},{"location":"basics/#deploy","text":"Install Connaisseur by using Helm (alternatively use Makefile ). helm install connaisseur helm --atomic --create-namespace --namespace connaisseur This will install Connaisseur in its own namespace called connaisseur . The installation itself may take some minutes, as the installation order of the Connaisseur components is critical. Only when the Connaisseur Pods are ready and running, the Admission Webhook can be applied for intercepting requests.","title":"Deploy"},{"location":"basics/#check","text":"When everything is installed, you can check whether all the pods are up by running kubectl get all -n connaisseur . $ kubectl get all -n connaisseur NAME READY STATUS RESTARTS AGE pod/connaisseur-deployment-78d8975596-42tkw 1 /1 Running 0 22s pod/connaisseur-deployment-78d8975596-5c4c6 1 /1 Running 0 22s pod/connaisseur-deployment-78d8975596-kvrj6 1 /1 Running 0 22s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/connaisseur-svc ClusterIP 10 .108.220.34 <none> 443 /TCP 22s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/connaisseur-deployment 3 /3 3 3 22s NAME DESIRED CURRENT READY AGE replicaset.apps/connaisseur-deployment-78d8975596 3 3 3 22s","title":"Check"},{"location":"basics/#use","text":"To use Connaisseur, simply try running some images or apply a deployment. In case you use the pre-configuration, you could for example run the following commands: kubectl run demo --image = docker.io/securesystemsengineering/testimage:unsigned > Error from server: admission webhook \"connaisseur-svc.connaisseur.svc\" denied the request ( ... ) . kubectl run hello-world --image = docker.io/hello-world > pod/hello-world created","title":"Use"},{"location":"basics/#delete","text":"Just like for installation, Helm can also be used to delete Connaisseur from your cluster. helm uninstall connaisseur -n connaisseur In case uninstallation fails or problems occur during subsequent installation, you can manually remove all resources: kubectl delete all,mutatingwebhookconfigurations,clusterroles,clusterrolebindings,configmaps,imagepolicies,secrets,serviceaccounts,customresourcedefinitions -lapp.kubernetes.io/instance = connaisseur kubectl delete namespaces connaisseur Connaisseur for example also installs a CutstomResourceDefinition imagepolicies.connaisseur.policy that validates its configuration. In case of major releases, the configuration structure might change which causes installation to fail and you might have to delete it manually.","title":"Delete"},{"location":"basics/#makefile","text":"Alternatively to using Helm, you can also run the Makefile for installing, deleting and more. Here the available commands: make install -- Runs the helm install command. make uninstall -- Switches the namespace to connaisseur , uses the helm uninstall command and deletes the connaisseur namespace. make annihilate -- Deletes all resources labeled with app.kubernetes.io/instance=connaisseur . This command is usually helpful, should the normal make uninstall not work. make docker -- Builds the connaisseur and connaisseur:helm-hook (relevant for installing Connaisseur) docker images.","title":"Makefile"},{"location":"basics/#detailed-configuration","text":"All configuration is done in the helm/values.yaml . The configuration of features is only described in the corresponding section .","title":"Detailed configuration"},{"location":"basics/#validators","text":"The validators are configured in the validators field, which defines a list of validator objects. A validator defines what kind of signatures are to be expected, how signatures are to be validated, against which trust root and how to access the signature data. For example, images might be signed with Docker Content Trust and reside in a private registry. Thus the validator would need to specify notaryv1 as type, the notary host and the required credentials. The specific validator type should be chosen based on the use case. A list of supported validator types can be found here . All validators share a similar structure for configuration. For specifics and additional options, please review the dedicated page of the validator type. Connaisseur comes with a few validators preconfigured including one for Docker's official images. The preconfigured validators can be removed. However, when removing the securesystemsengineering_official make sure to also exclude Connaisseur from validation either via the static allow validator or namespaced validation . The special case of static validators used to simply allow or deny images without verification is described below.","title":"Validators"},{"location":"basics/#configuration-options","text":".validators[*] in helm/values.yaml supports the following keys: key default required description name - The name of the validator, which is referenced in the image policy. It can only contain letters, numbers and '-'. If the name is default , it will be used if no validator is specified. type - The type of the validator which is dependent on the validator . trust_roots - A list of trust anchors to validate the signatures with. In practice, this is typically a list of public keys. trust_roots.[*].name - The name of the trust anchor, which is referenced in the image policy. If the name is default , it will be used if no key is specified. trust_roots.[*].key - The value of the trust anchor, most commonly a PEM encoded public key. auth - - Credentials that should be used in case authentication is required for validation. Details are provided on validator-specific pages. Further configuration fields specific to the validator type are described in the respective section. There is a special behavior, when a validator or one of the trust roots is named default . In this case, should an image policy rule not specify a validator or trust root to use, the one named default will be used instead. This also means there can only be one validator named default and for the trust roots, there can only be one called default within a single validator.","title":"Configuration options"},{"location":"basics/#example","text":"validators : - name : default type : notaryv1 host : notary.docker.io trust_roots : - name : default type : key key : | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsx28WV7BsQfnHF1kZmpdCTTLJaWe d0CA+JOi8H4REuBaWSZ5zPDe468WuOJ6f71E7WFg3CVEVYHuoZt2UYbN/Q== -----END PUBLIC KEY----- auth : username : superuser password : lookatmeimjumping - name : cosigner type : cosign trust_roots : - name : mykey key : | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEIFXO1w6oj0oI2Fk9SiaNJRKTiO9d ksm6hFczQAq+FDdw0istEdCwcHO61O/0bV+LC8jqFoomA28cT+py6FcSYw== -----END PUBLIC KEY-----","title":"Example"},{"location":"basics/#static-validators","text":"Static validators are a special type of validator that does not validate any signatures. Depending on the approve value being true or false , they either allow or deny all images for which they are specified as validator. This for example allows to implement allowlist or denylist .","title":"Static validators"},{"location":"basics/#configuration-options_1","text":"key default required description name - The name of the validator, which will be used to reference it in the image policy. type - Needs to be set to static . approve - Set to true to allow all images, or to false to deny.","title":"Configuration options"},{"location":"basics/#example_1","text":"validators : - name : allow type : static approve : true - name : deny type : static approve : false","title":"Example"},{"location":"basics/#image-policy","text":"The image policy is defined in the policy field and acts as a list of rule objects to determine which image should be validated by which validator (and potentially some further configurations). Each rule provides a pattern that is used to identify the images it applies to and a reference to a validator. In case no validator is specified, the validator with name default is used if it exists. If no key is specified for a validator, the trust root with name default is used if it exists. For each image in the admission request, only a single rule in the image policy will apply, the one with the most specific matching pattern field. This is determined by the following algorithm: A given image is matched against all rule patterns. All matching patterns are compared to one another to determine the most specific one (see below). Only two patterns are compared at a time; the more specific one then is compared to the next one and so forth. Specificity is determined as follows: Patterns are split into components (delimited by \"/\"). The pattern that has a higher number of components wins (is considered more specific). Should the two patterns that are being compared have equal number of components, the longest common prefix between each pattern component and corresponding image component are calculated (for this purpose, image identifiers are also split into components). The pattern with the longest common prefix in one component, starting from the leftmost, wins. Should all longest common prefixes of all components between the two compared patterns be equal, the pattern with a longer component, starting from the leftmost, wins. The rule whose pattern has won all comparisons is considered the most specific rule. Return the most specific rule. Note: Should an image have no rules to match against, an error will be raised, which is why it is recommended to always have an \"catch-all\" rule like *:* .","title":"Image policy"},{"location":"basics/#configuration-options_2","text":".policy[*] in helm/values.yaml supports the following keys: key default required description pattern - Globbing pattern to match an image name against. validator default The name of a validator in the validators list. If not provided, the validator with name default is used if it exists. with - Additional parameters to use for a validator. See more specifics in validator section . with.key default - The name of a trust root, which is specified within the referenced validator. If not provided, the trust root with name default is used if it exists.","title":"Configuration options"},{"location":"basics/#example_2","text":"policy : - pattern : \"*:*\" - pattern : \"docker.io/myrepo/*:*\" validator : cosigner with : key : mykey - pattern : \"docker.io/myrepo/deniedimage:*\" validator : deny - pattern : \"docker.io/myrepo/allowedimage:v*\" validator : allow","title":"Example"},{"location":"getting_started/","text":"Getting Started \u2693\ufe0e This guide offers a simple default configuration for setting up Connaisseur using public infrastructure and verifying your first self-signed images. You will learn how to: Create signing key pairs Configure Connaisseur Deploy Connaisseur Test Connaisseur (and sign images) Cleanup In the tutorial, you can choose to use either Docker Content Trust (DCT) based on Notary V1 or Cosign from the Sigstore project as a signing solution referred to as DCT and Cosign from here on. Furthermore we will work with public images on Docker Hub as a container registry and microk8s as a test cluster. However, feel free to bring your own solutions for registry or cluster and checkout our notes on compatibility . In general, Connaisseur can be fully configured via helm/values.yaml , so feel free to take a look and try for yourself. For more advanced usage in more complex cases (e.g. authentication, multiple registries, signers, validators, additional features), we strongly advise to review the following pages: Basics : understanding and configuring Connaisseur (e.g. image policy and validators) Validators : applying different signature solutions and specific configurations Features : using additional features (e.g. alerting) In case you need help, feel free to reach out via GitHub Discussions Requirements \u2693\ufe0e You should have a Kubernetes test cluster running. Furthermore, docker , git , helm and kubectl should be installed and usable, i.e. having run docker login and switched to the appropriate kubectl context. Create signing key pairs \u2693\ufe0e Before getting started with Connaisseur, we need to create our signing key pair. This obviously depends on the signing solution. Here, we will walk you through it for DCT and Cosign. In case you have already worked with Docker Content Trust or Cosign before and already possess key pairs, you can skip this step (how to retrieve a previously created DCT key is described here . Otherwise, pick your preferred signing scheme below. In case you are uncertain which solution to go with, you might be better of to start with DCT, as it comes packaged with docker . Cosign on the other hand is somewhat more straightforward to use. Docker Content Trust General usage of DCT is described in the docker documentation . For now, we just need to generate a public-private root key pair via: docker trust key generate root You will be prompted for a password, the private key is automatically imported and a root.pub file is created in your current folder that contains your public key which should look similar to: -----BEGIN PUBLIC KEY----- role: root MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELDzXwqie/P66R3gVpFDWMhxOyol5 YWD/KWnAaEIcJVTYUR+21NJSZz0yL7KLGrv50H9kHai5WWVsVykOZNoZYQ == -----END PUBLIC KEY----- We will only need the actual base64 encoded part of the key later. Cosign Usage of Cosign is very well described in the docs . As Cosign is in pre-release state, make sure to check the currently supported version in our Makefile . The corresponding version to download can be found on the Cosign GitHub . A keypair is generated via: cosign generate-key-pair You will be prompted to set a password, after which a private ( cosign.key ) and public ( cosign.pub ) key are created. In the next step, we will need the public key that should look similar to: -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA == -----END PUBLIC KEY----- Configure Connaisseur \u2693\ufe0e Now, we will need to configure Connaisseur. Let's first clone the repository: git clone https://github.com/sse-secure-systems/connaisseur.git cd connaisseur Connaisseur is configured via helm/values.yaml , so we will start there. We need to set Connaisseur to use our previously created public key for validation. To do so, go to the .validators and find the default validator. This one will be used if no validator is specified in the image policy ( .policy ). We need to uncomment the trust root with name default and add our previously created public key. The result should look similar to this: Docker Content Trust # the `default` validator is used if no validator is specified in image policy - name : default type : notaryv1 # or other supported validator (e.g. \"cosign\") host : notary.docker.io # configure the notary server to be used trust_roots : # the `default` key is used if no key is specified in image policy - name : default key : | # enter your key below -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELDzXwqie/P66R3gVpFDWMhxOyol5 YWD/KWnAaEIcJVTYUR+21NJSZz0yL7KLGrv50H9kHai5WWVsVykOZNoZYQ== -----END PUBLIC KEY----- # cert: | # for communication, using a selfsigned certificate # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- # auth: # basic authentication against the notary server # # either # username: notaryuser # password: notarypass # # or # secret_name: notarysecret # define a k8s secret with `username` and `password` fields Cosign For Cosign, the type needs to be set to cosign and the host is not required. # the `default` validator is used if no validator is specified in image policy - name : default type : cosign # or other supported validator (e.g. \"cosign\") trust_roots : # the `default` key is used if no key is specified in image policy - name : default key : | # enter your key below -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA== -----END PUBLIC KEY----- # cert: | # for communication, using a selfsigned certificate # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- # auth: # basic authentication against the notary server # # either # username: notaryuser # password: notarypass # # or # secret_name: notarysecret # define a k8s secret with `username` and `password` fields We have now configured the validator default with trust root default . This will automatically be used if no validator and trust root is specified in the image policy. Per default, Connaisseur's image policy under .policy in helm/values.yaml comes with a pattern \"*:*\" that does not specify a validator or trust root and thus all images that do not meet any of the more specific pre-configured patterns will be verified using this validator. Consequently, we leave the rest untouched in this tutorial, but strongly recommend to read the basics to leverage the full potential of Connaisseur. Deploy Connaisseur \u2693\ufe0e So let's deploy Connaisseur to the cluster: helm install connaisseur helm --atomic --create-namespace --namespace connaisseur This can take a few minutes. You should be prompted something like: NAME: connaisseur LAST DEPLOYED: Fri Jul 9 20 :43:10 2021 NAMESPACE: connaisseur STATUS: deployed REVISION: 1 TEST SUITE: None Afterwards, we can check that Connaisseur is running via kubectl get all -n connaisseur which should show something similar to: NAME READY STATUS RESTARTS AGE pod/connaisseur-deployment-6876c87c8c-txrkj 1 /1 Running 0 2m9s pod/connaisseur-deployment-6876c87c8c-wvr7q 1 /1 Running 0 2m9s pod/connaisseur-deployment-6876c87c8c-rnc7k 1 /1 Running 0 2m9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/connaisseur-svc ClusterIP 10 .152.183.166 <none> 443 /TCP 2m10s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/connaisseur-deployment 3 /3 3 3 2m9s NAME DESIRED CURRENT READY AGE replicaset.apps/connaisseur-deployment-6876c87c8c 3 3 3 2m9s Test Connaisseur \u2693\ufe0e Now that we created our key pairs, configured and deployed Connaisseur, the next step is to test our setup. So let's create and push a testimage. Feel free to use our simple test Dockerfile under tests/Dockerfile (make sure to set your own IMAGE name): # Typically, IMAGE=<REGISTRY>/<REPOSITORY-NAME>/<IMAGE-NAME>:<TAG>, like IMAGE = docker.io/xoph/demo:test docker build -f tests/Dockerfile -t ${ IMAGE } . docker push ${ IMAGE } In case you have DCT turned on per default via environment variable DOCKER_CONTENT_TRUST=1 , you should disable for now during the docker push by adding the --disable-content-trust=true . If we try to deploy this unsigned image: kubectl run test --image = ${ IMAGE } Connaisseur denies the request due to lack of trust data or signed digest, e.g.: Error from server: admission webhook \"connaisseur-svc.connaisseur.svc\" denied the request: could not find signed digest for image \"docker.io/xoph/demo:test\" in trust data. # or Error from server: admission webhook \"connaisseur-svc.connaisseur.svc\" denied the request: No trust data for image \"docker.io/xoph/demo:test\" . So let's sign the image and try again. Docker Content Trust In DCT signing works via docker push using the --disable-content-trust flag: docker push ${ IMAGE } --disable-content-trust = false You will be prompted to provide your password and might be asked to set a new repository key. The trust data will then be pushed to the DockerHub Notary server. Cosign For Cosign, we use the private key file from the first step: cosign sign -key cosign.key ${ IMAGE } You will be asked to enter your password after wich the signature data will be pushed to your repository. After successful signing, we try again: kubectl run test --image = ${ IMAGE } Now, the request is admitted to the cluster and Kubernetes returns: pod/test created You did it you just verified your first signed images in your Kuberenetes cluster Read on to learn how to fully configure Connaisseur Cleanup \u2693\ufe0e To uninstall Connaisseur, use: helm uninstall connaisseur --namespace connaisseur Uninstallation can take a moment as Connaisseur needs to validate the deletion webhook","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"This guide offers a simple default configuration for setting up Connaisseur using public infrastructure and verifying your first self-signed images. You will learn how to: Create signing key pairs Configure Connaisseur Deploy Connaisseur Test Connaisseur (and sign images) Cleanup In the tutorial, you can choose to use either Docker Content Trust (DCT) based on Notary V1 or Cosign from the Sigstore project as a signing solution referred to as DCT and Cosign from here on. Furthermore we will work with public images on Docker Hub as a container registry and microk8s as a test cluster. However, feel free to bring your own solutions for registry or cluster and checkout our notes on compatibility . In general, Connaisseur can be fully configured via helm/values.yaml , so feel free to take a look and try for yourself. For more advanced usage in more complex cases (e.g. authentication, multiple registries, signers, validators, additional features), we strongly advise to review the following pages: Basics : understanding and configuring Connaisseur (e.g. image policy and validators) Validators : applying different signature solutions and specific configurations Features : using additional features (e.g. alerting) In case you need help, feel free to reach out via GitHub Discussions","title":"Getting Started"},{"location":"getting_started/#requirements","text":"You should have a Kubernetes test cluster running. Furthermore, docker , git , helm and kubectl should be installed and usable, i.e. having run docker login and switched to the appropriate kubectl context.","title":"Requirements"},{"location":"getting_started/#create-signing-key-pairs","text":"Before getting started with Connaisseur, we need to create our signing key pair. This obviously depends on the signing solution. Here, we will walk you through it for DCT and Cosign. In case you have already worked with Docker Content Trust or Cosign before and already possess key pairs, you can skip this step (how to retrieve a previously created DCT key is described here . Otherwise, pick your preferred signing scheme below. In case you are uncertain which solution to go with, you might be better of to start with DCT, as it comes packaged with docker . Cosign on the other hand is somewhat more straightforward to use. Docker Content Trust General usage of DCT is described in the docker documentation . For now, we just need to generate a public-private root key pair via: docker trust key generate root You will be prompted for a password, the private key is automatically imported and a root.pub file is created in your current folder that contains your public key which should look similar to: -----BEGIN PUBLIC KEY----- role: root MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELDzXwqie/P66R3gVpFDWMhxOyol5 YWD/KWnAaEIcJVTYUR+21NJSZz0yL7KLGrv50H9kHai5WWVsVykOZNoZYQ == -----END PUBLIC KEY----- We will only need the actual base64 encoded part of the key later. Cosign Usage of Cosign is very well described in the docs . As Cosign is in pre-release state, make sure to check the currently supported version in our Makefile . The corresponding version to download can be found on the Cosign GitHub . A keypair is generated via: cosign generate-key-pair You will be prompted to set a password, after which a private ( cosign.key ) and public ( cosign.pub ) key are created. In the next step, we will need the public key that should look similar to: -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA == -----END PUBLIC KEY-----","title":"Create signing key pairs"},{"location":"getting_started/#configure-connaisseur","text":"Now, we will need to configure Connaisseur. Let's first clone the repository: git clone https://github.com/sse-secure-systems/connaisseur.git cd connaisseur Connaisseur is configured via helm/values.yaml , so we will start there. We need to set Connaisseur to use our previously created public key for validation. To do so, go to the .validators and find the default validator. This one will be used if no validator is specified in the image policy ( .policy ). We need to uncomment the trust root with name default and add our previously created public key. The result should look similar to this: Docker Content Trust # the `default` validator is used if no validator is specified in image policy - name : default type : notaryv1 # or other supported validator (e.g. \"cosign\") host : notary.docker.io # configure the notary server to be used trust_roots : # the `default` key is used if no key is specified in image policy - name : default key : | # enter your key below -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELDzXwqie/P66R3gVpFDWMhxOyol5 YWD/KWnAaEIcJVTYUR+21NJSZz0yL7KLGrv50H9kHai5WWVsVykOZNoZYQ== -----END PUBLIC KEY----- # cert: | # for communication, using a selfsigned certificate # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- # auth: # basic authentication against the notary server # # either # username: notaryuser # password: notarypass # # or # secret_name: notarysecret # define a k8s secret with `username` and `password` fields Cosign For Cosign, the type needs to be set to cosign and the host is not required. # the `default` validator is used if no validator is specified in image policy - name : default type : cosign # or other supported validator (e.g. \"cosign\") trust_roots : # the `default` key is used if no key is specified in image policy - name : default key : | # enter your key below -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA== -----END PUBLIC KEY----- # cert: | # for communication, using a selfsigned certificate # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- # auth: # basic authentication against the notary server # # either # username: notaryuser # password: notarypass # # or # secret_name: notarysecret # define a k8s secret with `username` and `password` fields We have now configured the validator default with trust root default . This will automatically be used if no validator and trust root is specified in the image policy. Per default, Connaisseur's image policy under .policy in helm/values.yaml comes with a pattern \"*:*\" that does not specify a validator or trust root and thus all images that do not meet any of the more specific pre-configured patterns will be verified using this validator. Consequently, we leave the rest untouched in this tutorial, but strongly recommend to read the basics to leverage the full potential of Connaisseur.","title":"Configure Connaisseur"},{"location":"getting_started/#deploy-connaisseur","text":"So let's deploy Connaisseur to the cluster: helm install connaisseur helm --atomic --create-namespace --namespace connaisseur This can take a few minutes. You should be prompted something like: NAME: connaisseur LAST DEPLOYED: Fri Jul 9 20 :43:10 2021 NAMESPACE: connaisseur STATUS: deployed REVISION: 1 TEST SUITE: None Afterwards, we can check that Connaisseur is running via kubectl get all -n connaisseur which should show something similar to: NAME READY STATUS RESTARTS AGE pod/connaisseur-deployment-6876c87c8c-txrkj 1 /1 Running 0 2m9s pod/connaisseur-deployment-6876c87c8c-wvr7q 1 /1 Running 0 2m9s pod/connaisseur-deployment-6876c87c8c-rnc7k 1 /1 Running 0 2m9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/connaisseur-svc ClusterIP 10 .152.183.166 <none> 443 /TCP 2m10s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/connaisseur-deployment 3 /3 3 3 2m9s NAME DESIRED CURRENT READY AGE replicaset.apps/connaisseur-deployment-6876c87c8c 3 3 3 2m9s","title":"Deploy Connaisseur"},{"location":"getting_started/#test-connaisseur","text":"Now that we created our key pairs, configured and deployed Connaisseur, the next step is to test our setup. So let's create and push a testimage. Feel free to use our simple test Dockerfile under tests/Dockerfile (make sure to set your own IMAGE name): # Typically, IMAGE=<REGISTRY>/<REPOSITORY-NAME>/<IMAGE-NAME>:<TAG>, like IMAGE = docker.io/xoph/demo:test docker build -f tests/Dockerfile -t ${ IMAGE } . docker push ${ IMAGE } In case you have DCT turned on per default via environment variable DOCKER_CONTENT_TRUST=1 , you should disable for now during the docker push by adding the --disable-content-trust=true . If we try to deploy this unsigned image: kubectl run test --image = ${ IMAGE } Connaisseur denies the request due to lack of trust data or signed digest, e.g.: Error from server: admission webhook \"connaisseur-svc.connaisseur.svc\" denied the request: could not find signed digest for image \"docker.io/xoph/demo:test\" in trust data. # or Error from server: admission webhook \"connaisseur-svc.connaisseur.svc\" denied the request: No trust data for image \"docker.io/xoph/demo:test\" . So let's sign the image and try again. Docker Content Trust In DCT signing works via docker push using the --disable-content-trust flag: docker push ${ IMAGE } --disable-content-trust = false You will be prompted to provide your password and might be asked to set a new repository key. The trust data will then be pushed to the DockerHub Notary server. Cosign For Cosign, we use the private key file from the first step: cosign sign -key cosign.key ${ IMAGE } You will be asked to enter your password after wich the signature data will be pushed to your repository. After successful signing, we try again: kubectl run test --image = ${ IMAGE } Now, the request is admitted to the cluster and Kubernetes returns: pod/test created You did it you just verified your first signed images in your Kuberenetes cluster Read on to learn how to fully configure Connaisseur","title":"Test Connaisseur"},{"location":"getting_started/#cleanup","text":"To uninstall Connaisseur, use: helm uninstall connaisseur --namespace connaisseur Uninstallation can take a moment as Connaisseur needs to validate the deletion webhook","title":"Cleanup"},{"location":"threat_model/","text":"Threat Model \u2693\ufe0e OUTDATED The STRIDE threat model has been used as a reference for threat modeling. Each of the STRIDE threats were matched to all entities relevant to Connaisseur, including Connaisseur itself. A description of how a threat on an entity manifests itself is given as well as a possible counter measure. images created by monkik from Noun Project (1) Developer/User \u2693\ufe0e Threat Description Counter Measure Spoofing A developer could be tricked into signing a malicious image, which subsequently will be accepted by Connaisseur. Security Awareness: Developers need to be aware of these attacks, so they can spot any attempts. Elevation of privilege An attacker could acquire the credentials of a developer or trick her into performing malicious actions, hence elevating their privileges to those of the victim. Depending on the victim's privileges, other attacks may be mounted. RBAC & Security Awareness: With proper Role-Based Access Control (RBAC), the effects of compromising an individual's account would help limit its impact and may mitigate the privilege escalation, of course depending on the victim's access level. Other than that, a security awareness training for developers can help minimize the chances of losing critical credentials. (2) Connaisseur Service \u2693\ufe0e Threat Description Counter Measure Spoofing An attacker could stop the original Connaisseur service and start their own version, to take over the admission controller's responsibilities. That way, the functionality of Connaisseur could be completely disabled or altered at will. RBAC: By only permitting a carefully selected group of people to start and stop services in the Connaisseur namespace, such attacks can be prevented. Tampering Given an attacker has access to the Connaisseur container, she could tamper with its source code, leading to forged responses or full compromise. The attacker could also stop the the original Connaisseur process and handle incoming requests some other way, which would be similar to the spoofing threat, but from inside the Connaisseur container. RBAC + Monitoring: Access to the inside of the container can be restricted with RBAC, so an attacker never gets there in the first place. In case the attacker already is inside the container, there are specific monitoring tools (e.g. falco ), which are able to register changes inside containers and notify you, should Connaisseur be compromised. Tampering An attacker could modify Connaisseur's image policy to bypass signature verification and inject malicious images. Alternatively, the public root key could be replaced, allowing fake trust data to pass as legit. Lastly, the admission controller could be simply deactivated by deleting the webhook. RBAC + Monitoring: An RBAC system can prevent unauthorized changes to both the image policy and public root key. Additionally, the Connaisseur readiness probe checks the availability of the webhook and will be set to Not Ready should the webhook not be present. Monitoring should still be used to keep track of the admission controller's webhook availability status, as setting up a fake connaisseur-bootstrap-sentinel pod in the connaisseur namespace can bypass that readiness probe check. More on that in an upcoming architectural decision record. Denial of service When sending an extraordinary amount of requests to Connaisseur or triggering unexpected behavior, Connaisseur might become unresponsive or crash. As a result, image signatures can't be verified. Failure Policy: The webhook that is connected to Connaisseur denies all request automatically, should the Connaisseur service be unavailable. Thus, malicious images cannot enter the cluster. Additionally, multiple instances of Connaisseur can be run for better load balancing. Elevation of privilege Since Connaisseur interacts with the Kubernetes API, an attacker located inside the Connaisseur container can act on its behalf and use its permissions. RBAC: Per default, the Connaisseur service account only has read permissions to a few non-critical objects. (3) Notary Server \u2693\ufe0e Threat Description Counter Measure Spoofing An attacker could mount a Monster-in-the-Middle attack between Notary and the Connaisseur service and act as a fake Notary, sending back false trust data. TLS: A TLS connection between Connaisseur and Notary ensures the Notary server's authenticity. Tampering With full control over the Notary server, the stored trust data can be manipulated to include digests of malicious images. Signatures: Changing the trust data would invalidate the signatures and thus fail the image verification. Additionally, the keys needed to create valid signatures are not stored in Notary, but offline on client side. Information disclosure As Notary is responsible for creating the snapshot and timestamp signatures, an attacker could steal those private keys, and create valid snapshot and timestamp signatures. Key rotation: The snapshot and timestamp keys can easily be rotated and changed frequently. The more cirtical root and target key are not stored on server side. Denial of service An extraordinary amount of requests to the Notary server could bring it down so that the Connaisseur service has no more trust data available to work with. Health Probe: Connaisseur's readiness and liveness probes check the Notary server's health every few seconds. Should Notary be unavailable, Connaisseur will switch into a not-ready state. As a consequence, the failure policy will automatically deny all requests. (4) Registry \u2693\ufe0e Threat Description Counter Measure Spoofing An attacker could mount a Monster-in-the-Middle attack between the registry and the Kubernetes cluster and act as a fake registry, sending back malicious images. TLS: A TLS connection between the Kubernetes cluster and the registry ensures that the registry is authentic. Tampering With full control over the registry, an attacker may introduce malicious images or change the layers of existing ones and thus inject malicious content. Image Digests: Introducing new images does not work as Connaisseur selects them by digest. An attacker would have to change the content of the corresponding digest layer, while the changes need to produce the same digest. Such a hash collision is considered practically impossible. If digests differ, the docker daemon underlying the cluster will deny the image. Denial of service An extraordinary amount of requests to the registry could bring it down, so that no images can be pulled from it. Out of scope: This threat is specific to registries, not Connaisseur.","title":"Threat Model"},{"location":"threat_model/#threat-model","text":"OUTDATED The STRIDE threat model has been used as a reference for threat modeling. Each of the STRIDE threats were matched to all entities relevant to Connaisseur, including Connaisseur itself. A description of how a threat on an entity manifests itself is given as well as a possible counter measure. images created by monkik from Noun Project","title":"Threat Model"},{"location":"threat_model/#1-developeruser","text":"Threat Description Counter Measure Spoofing A developer could be tricked into signing a malicious image, which subsequently will be accepted by Connaisseur. Security Awareness: Developers need to be aware of these attacks, so they can spot any attempts. Elevation of privilege An attacker could acquire the credentials of a developer or trick her into performing malicious actions, hence elevating their privileges to those of the victim. Depending on the victim's privileges, other attacks may be mounted. RBAC & Security Awareness: With proper Role-Based Access Control (RBAC), the effects of compromising an individual's account would help limit its impact and may mitigate the privilege escalation, of course depending on the victim's access level. Other than that, a security awareness training for developers can help minimize the chances of losing critical credentials.","title":"(1) Developer/User"},{"location":"threat_model/#2-connaisseur-service","text":"Threat Description Counter Measure Spoofing An attacker could stop the original Connaisseur service and start their own version, to take over the admission controller's responsibilities. That way, the functionality of Connaisseur could be completely disabled or altered at will. RBAC: By only permitting a carefully selected group of people to start and stop services in the Connaisseur namespace, such attacks can be prevented. Tampering Given an attacker has access to the Connaisseur container, she could tamper with its source code, leading to forged responses or full compromise. The attacker could also stop the the original Connaisseur process and handle incoming requests some other way, which would be similar to the spoofing threat, but from inside the Connaisseur container. RBAC + Monitoring: Access to the inside of the container can be restricted with RBAC, so an attacker never gets there in the first place. In case the attacker already is inside the container, there are specific monitoring tools (e.g. falco ), which are able to register changes inside containers and notify you, should Connaisseur be compromised. Tampering An attacker could modify Connaisseur's image policy to bypass signature verification and inject malicious images. Alternatively, the public root key could be replaced, allowing fake trust data to pass as legit. Lastly, the admission controller could be simply deactivated by deleting the webhook. RBAC + Monitoring: An RBAC system can prevent unauthorized changes to both the image policy and public root key. Additionally, the Connaisseur readiness probe checks the availability of the webhook and will be set to Not Ready should the webhook not be present. Monitoring should still be used to keep track of the admission controller's webhook availability status, as setting up a fake connaisseur-bootstrap-sentinel pod in the connaisseur namespace can bypass that readiness probe check. More on that in an upcoming architectural decision record. Denial of service When sending an extraordinary amount of requests to Connaisseur or triggering unexpected behavior, Connaisseur might become unresponsive or crash. As a result, image signatures can't be verified. Failure Policy: The webhook that is connected to Connaisseur denies all request automatically, should the Connaisseur service be unavailable. Thus, malicious images cannot enter the cluster. Additionally, multiple instances of Connaisseur can be run for better load balancing. Elevation of privilege Since Connaisseur interacts with the Kubernetes API, an attacker located inside the Connaisseur container can act on its behalf and use its permissions. RBAC: Per default, the Connaisseur service account only has read permissions to a few non-critical objects.","title":"(2) Connaisseur Service"},{"location":"threat_model/#3-notary-server","text":"Threat Description Counter Measure Spoofing An attacker could mount a Monster-in-the-Middle attack between Notary and the Connaisseur service and act as a fake Notary, sending back false trust data. TLS: A TLS connection between Connaisseur and Notary ensures the Notary server's authenticity. Tampering With full control over the Notary server, the stored trust data can be manipulated to include digests of malicious images. Signatures: Changing the trust data would invalidate the signatures and thus fail the image verification. Additionally, the keys needed to create valid signatures are not stored in Notary, but offline on client side. Information disclosure As Notary is responsible for creating the snapshot and timestamp signatures, an attacker could steal those private keys, and create valid snapshot and timestamp signatures. Key rotation: The snapshot and timestamp keys can easily be rotated and changed frequently. The more cirtical root and target key are not stored on server side. Denial of service An extraordinary amount of requests to the Notary server could bring it down so that the Connaisseur service has no more trust data available to work with. Health Probe: Connaisseur's readiness and liveness probes check the Notary server's health every few seconds. Should Notary be unavailable, Connaisseur will switch into a not-ready state. As a consequence, the failure policy will automatically deny all requests.","title":"(3) Notary Server"},{"location":"threat_model/#4-registry","text":"Threat Description Counter Measure Spoofing An attacker could mount a Monster-in-the-Middle attack between the registry and the Kubernetes cluster and act as a fake registry, sending back malicious images. TLS: A TLS connection between the Kubernetes cluster and the registry ensures that the registry is authentic. Tampering With full control over the registry, an attacker may introduce malicious images or change the layers of existing ones and thus inject malicious content. Image Digests: Introducing new images does not work as Connaisseur selects them by digest. An attacker would have to change the content of the corresponding digest layer, while the changes need to produce the same digest. Such a hash collision is considered practically impossible. If digests differ, the docker daemon underlying the cluster will deny the image. Denial of service An extraordinary amount of requests to the registry could bring it down, so that no images can be pulled from it. Out of scope: This threat is specific to registries, not Connaisseur.","title":"(4) Registry"},{"location":"adr/","text":"Architecture Decision Records \u2693\ufe0e We strive to make decisions taken during the devlopment of Connaisseur transparent, whenever they may seem weird or unintuitive towards someone new to the project. Hence, when encountering a problem that took either considerable time to find a solution for or that spawned a lot of discussion, be it internal or from the community, the decision with the factors leading up to the particular choice should be documented. Additionally, we should make clear what other options were under consideration and why they were discarded to help both with making the decision comprehensible to people not involved at the time but also to not repeat discussions at a later point in time. Since each Architecture Decision may be slightly different, the format is not completely set in stone. However, you should give at least title, status, some context, decisions taken and options discarded and some reasoning as to why one option was deemed better than the others.","title":"Architecture Decision Records"},{"location":"adr/#architecture-decision-records","text":"We strive to make decisions taken during the devlopment of Connaisseur transparent, whenever they may seem weird or unintuitive towards someone new to the project. Hence, when encountering a problem that took either considerable time to find a solution for or that spawned a lot of discussion, be it internal or from the community, the decision with the factors leading up to the particular choice should be documented. Additionally, we should make clear what other options were under consideration and why they were discarded to help both with making the decision comprehensible to people not involved at the time but also to not repeat discussions at a later point in time. Since each Architecture Decision may be slightly different, the format is not completely set in stone. However, you should give at least title, status, some context, decisions taken and options discarded and some reasoning as to why one option was deemed better than the others.","title":"Architecture Decision Records"},{"location":"adr/ADR-1_bootstrap-sentinel/","text":"ADR 1: Bootstrap Sentinel \u2693\ufe0e Status \u2693\ufe0e Amended in ADR-3 . Context \u2693\ufe0e Connaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods. In #3 it was noted that prior to version 1.1.5 of Connaisseur when looking at the Ready status of Connaisseur Pods, they could report Ready while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed after the Connaisseur Pods, which was solved by checking the Ready state of said Pods. If one were to add a dependency to this Ready state, such that it only shows Ready when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration. Considered options \u2693\ufe0e Option 1 \u2693\ufe0e At the start of the Helm deployment, one can create a Pod named connaisseur-bootstrap-sentinel that will run for 5 minutes (which is also the installation timeout by helm). Connaisseur Pods will report Ready if they can 1) access notary AND 2) the MutatingWebhookConfiguration exists OR 3) the connaisseur-bootstrap-sentinel Pod is still running. If 1) AND 2) both hold true, the sentinel is killed even if the 5 minutes have not passed yet. Option 2 \u2693\ufe0e Let Connaisseur's Pod readiness stay non-indicative of Connaisseur functioning and advertise that someone running Connaisseur has to monitor the MutatingWebhookConfiguration in order to ensure proper working. Option 3 \u2693\ufe0e Deploy MutatingWebhookConfiguration through Helm when Connaisseur Pods are healthy instead of when ready. Require Pod started and working notary connection for health and require additionally the existence of the MutatingWebhookConfiguration for readiness. Decision outcome \u2693\ufe0e We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it. Positive Consequences \u2693\ufe0e If the Connaisseur Pods report Ready during the connaisseur-bootstrap-sentinel 's runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the helm deployment will fail after its timeout period (default: 5min), since there won't be a running connaisseur-bootstrap-sentinel Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the Ready state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working. Negative Consequences \u2693\ufe0e On the other hand, if an adversary can deploy a Pod named connaisseur-bootstrap-sentinel to Connaisseur's Namespace, the Connaisseur Pods will always show Ready regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the connaisseur-bootstrap-sentinel Pod being left behind, however since it has a very limited use-case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.","title":"ADR 1: Bootstrap Sentinel"},{"location":"adr/ADR-1_bootstrap-sentinel/#adr-1-bootstrap-sentinel","text":"","title":"ADR 1: Bootstrap Sentinel"},{"location":"adr/ADR-1_bootstrap-sentinel/#status","text":"Amended in ADR-3 .","title":"Status"},{"location":"adr/ADR-1_bootstrap-sentinel/#context","text":"Connaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods. In #3 it was noted that prior to version 1.1.5 of Connaisseur when looking at the Ready status of Connaisseur Pods, they could report Ready while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed after the Connaisseur Pods, which was solved by checking the Ready state of said Pods. If one were to add a dependency to this Ready state, such that it only shows Ready when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.","title":"Context"},{"location":"adr/ADR-1_bootstrap-sentinel/#considered-options","text":"","title":"Considered options"},{"location":"adr/ADR-1_bootstrap-sentinel/#option-1","text":"At the start of the Helm deployment, one can create a Pod named connaisseur-bootstrap-sentinel that will run for 5 minutes (which is also the installation timeout by helm). Connaisseur Pods will report Ready if they can 1) access notary AND 2) the MutatingWebhookConfiguration exists OR 3) the connaisseur-bootstrap-sentinel Pod is still running. If 1) AND 2) both hold true, the sentinel is killed even if the 5 minutes have not passed yet.","title":"Option 1"},{"location":"adr/ADR-1_bootstrap-sentinel/#option-2","text":"Let Connaisseur's Pod readiness stay non-indicative of Connaisseur functioning and advertise that someone running Connaisseur has to monitor the MutatingWebhookConfiguration in order to ensure proper working.","title":"Option 2"},{"location":"adr/ADR-1_bootstrap-sentinel/#option-3","text":"Deploy MutatingWebhookConfiguration through Helm when Connaisseur Pods are healthy instead of when ready. Require Pod started and working notary connection for health and require additionally the existence of the MutatingWebhookConfiguration for readiness.","title":"Option 3"},{"location":"adr/ADR-1_bootstrap-sentinel/#decision-outcome","text":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.","title":"Decision outcome"},{"location":"adr/ADR-1_bootstrap-sentinel/#positive-consequences","text":"If the Connaisseur Pods report Ready during the connaisseur-bootstrap-sentinel 's runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the helm deployment will fail after its timeout period (default: 5min), since there won't be a running connaisseur-bootstrap-sentinel Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the Ready state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.","title":"Positive Consequences"},{"location":"adr/ADR-1_bootstrap-sentinel/#negative-consequences","text":"On the other hand, if an adversary can deploy a Pod named connaisseur-bootstrap-sentinel to Connaisseur's Namespace, the Connaisseur Pods will always show Ready regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the connaisseur-bootstrap-sentinel Pod being left behind, however since it has a very limited use-case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.","title":"Negative Consequences"},{"location":"adr/ADR-2_release-management/","text":"ADR 2: Release Management \u2693\ufe0e Status \u2693\ufe0e Proposed Context \u2693\ufe0e During its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart. A single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility. Considered options \u2693\ufe0e Choice 1 \u2693\ufe0e What branches to maintain Option 1 \u2693\ufe0e Continue with PRs from personal feature branches to master . Option 2 \u2693\ufe0e Have a development branch against which to create pull requests (during usual development, hotfixes may be different). Sub-options: - a develop (or similar) branch that will exist continuously - a v.1.5.0_dev (or similar) branch for each respective version Choice 2 \u2693\ufe0e Where to sign the images Option 1 \u2693\ufe0e Have the pipeline build, sign and push the images. Option 2 \u2693\ufe0e Have a maintainer build, sign and push the images. Decision outcome \u2693\ufe0e For choice 1, we decided to go for two branches. On the one hand, master being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a develop branch that hosts the current state of development and will be merged to master whenever we want to create a new release. This way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release. In the process of automating most of the release process, we will run an integration test with locally built images for pull requests to master . Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the master branch referencing the new release version. After the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working. We decided for this option as it does not expose credentials to Github Actions, which we wanted to avoid especially in light of the recent Github Actions injection attacks and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys. Positive Consequences \u2693\ufe0e We can develop without having to ship changes immediatly. Release process does not expose credentials to Github Actions. Code gets git tags. Negative Consequences \u2693\ufe0e Process from code to release for a single change is more cumbersome than right now. Release still requires human intervention.","title":"ADR 2: Release Management"},{"location":"adr/ADR-2_release-management/#adr-2-release-management","text":"","title":"ADR 2: Release Management"},{"location":"adr/ADR-2_release-management/#status","text":"Proposed","title":"Status"},{"location":"adr/ADR-2_release-management/#context","text":"During its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart. A single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.","title":"Context"},{"location":"adr/ADR-2_release-management/#considered-options","text":"","title":"Considered options"},{"location":"adr/ADR-2_release-management/#choice-1","text":"What branches to maintain","title":"Choice 1"},{"location":"adr/ADR-2_release-management/#option-1","text":"Continue with PRs from personal feature branches to master .","title":"Option 1"},{"location":"adr/ADR-2_release-management/#option-2","text":"Have a development branch against which to create pull requests (during usual development, hotfixes may be different). Sub-options: - a develop (or similar) branch that will exist continuously - a v.1.5.0_dev (or similar) branch for each respective version","title":"Option 2"},{"location":"adr/ADR-2_release-management/#choice-2","text":"Where to sign the images","title":"Choice 2"},{"location":"adr/ADR-2_release-management/#option-1_1","text":"Have the pipeline build, sign and push the images.","title":"Option 1"},{"location":"adr/ADR-2_release-management/#option-2_1","text":"Have a maintainer build, sign and push the images.","title":"Option 2"},{"location":"adr/ADR-2_release-management/#decision-outcome","text":"For choice 1, we decided to go for two branches. On the one hand, master being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a develop branch that hosts the current state of development and will be merged to master whenever we want to create a new release. This way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release. In the process of automating most of the release process, we will run an integration test with locally built images for pull requests to master . Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the master branch referencing the new release version. After the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working. We decided for this option as it does not expose credentials to Github Actions, which we wanted to avoid especially in light of the recent Github Actions injection attacks and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.","title":"Decision outcome"},{"location":"adr/ADR-2_release-management/#positive-consequences","text":"We can develop without having to ship changes immediatly. Release process does not expose credentials to Github Actions. Code gets git tags.","title":"Positive Consequences"},{"location":"adr/ADR-2_release-management/#negative-consequences","text":"Process from code to release for a single change is more cumbersome than right now. Release still requires human intervention.","title":"Negative Consequences"},{"location":"adr/ADR-3_multi_notary_config/","text":"ADR 3: Multiple Notary Configuration \u2693\ufe0e Status \u2693\ufe0e Accepted Context \u2693\ufe0e Previously Connaisseur only supported the configuration of a single notary, where all signature data had to reside in. Unfortunately this is rather impractical, as one doesn't create all signatures for all images one uses in a cluster. There is a need to access signature data from multiple places, like in a setup where most images come from a private registry + notary and some from DockerHub and their notary. There is also the problem that a single notary instance could use multiple root keys, used for creating the signatures, like in the case of DockerHub. Connaisseur also only supports a single root key to be trust pinned, thus making it impractical. That's why the decision was made to support more than one notary and multiple keys per notary, which leads to the question how the new configuration should look like. This also has implications on the notary health check, which is important for Connaisseur's own readiness check. Considered options \u2693\ufe0e Choice 1 \u2693\ufe0e The overall notary configuration setup in helm/values.yaml . Option 1 (Per Notary) \u2693\ufe0e The notary field becomes a list and changes to notaries . Per to be used notary instance, there will be one entry in this list. The entry will have the following data fields ( bold are mandatory): name -- A unique identifier for the notary configuration, which will be used in the image policy. host -- The host address of the notary instance. pub_root_keys -- A list of public root keys, which are to be used for signature verification. name -- An identifier for a single public root key, which will be used in the image policy. key -- The actual public root key in PEM format. selfsigned_cert -- A self-signed certificate in PEM format, for making secure TLS connection to the notary. auth -- Authentication details, should the notary require some. user -- Username to authenticate with. password -- Password to authenticate with. secretName -- Kubernetes secret reference to use INSTEAD of user/password combination. is_acr -- Marks the notary as being part of an Azure Container Registry. The image policy will have two additional fields per rule entry (in \"quotes\" are already present fields): \" pattern \" -- Image pattern to match against, for rule to apply. \" verify \" -- Whether the images should be verified or not. \" delegations \" -- List of required delegation roles. notary -- Which notary to use for any matching image. This has to correspond to a name field of one configured notary. What happens if none is given, is defined by the result of choice 2. key -- Which key to use for doing the signature verification. This has to correspond to a name field of one of the public keys configured for the notary corresponding to the image policy's notary field. What happens if none is given, is defined by the result of choice 2. Option 2 (Per Notary + Key) \u2693\ufe0e The notary field becomes a list and changes to notaries . Per notary + public root key combination, there is one entry. Meaning, for example, there will be one entry for DockerHub and the public key for all official images and there will be another entry for DockerHub and the public key for some private images. The entries will look identical to the one's from option 1, with two exceptions. The pub_root_keys field of the notary configurations won't be a list and only has a single entry, without needing to specify a key name. The image policy will only address the notary configuration to be chosen with the notary field, without the need for a key field. Choice 2 \u2693\ufe0e Default values for notary (and key ) inside the image policy. Option 1 (First item) \u2693\ufe0e When no notary is specified in a image policy rule, the first entry in the notaries configuration list is taken. The same goes for the public root key list, should option 1 for choice 1 be chosen. Problem: Might get inconsistent, should list ordering in python get shuffled around Option 2 (Explicit default) \u2693\ufe0e One of the notary configuration will be given a default field, which marks it as the default value. Problem: No real problems here, just an extra field that the user has to care about. Option 3 (Mandatory Notary) \u2693\ufe0e The notary (and potentially key ) field is mandatory for the image policy. Problem: Creates configuration overhead if many image policies use the same notary/key combination. Option 4 (Default name) \u2693\ufe0e If no notary or key are given in the image policy, it is assumed that one of the elements in the notary list or key list has name: \"default\" , which will then be taken. Should the assumption be wrong, an error is raised. Choice 3 \u2693\ufe0e Previously, the readiness probe for connaisseur also considered the notary's health for its own status. With multiple notary instances configured, this behavior changes. Option 1 (Ignore Notary) \u2693\ufe0e The readiness probe of Connaisseur will no longer be dependent on any notary health checks. The are completely decoupled. Problem: No knowledge that Connaisseur will automatically fail because of an unreachable notary, before one tries to deploy an image. Option 2 (Health check on all) \u2693\ufe0e In order for connaisseur to be ready, all configured notaries must be healthy and reachable. Problem: A single unreachable notary will \"disable\" Connaisseur's access to all others. Option 3 (Log Notary status) \u2693\ufe0e A mix of option 1 and 2, whereas the readiness of Connaisseur is independent of the notaries health check, but they are still being made, so unhealthy notaries can be logged. Problem: At what interval should be logged? Decision outcome \u2693\ufe0e Choice 1 \u2693\ufe0e Option 1 was chosen, to keep configurational duplication at a minimum. Choice 2 \u2693\ufe0e Option 4 was chosen. If more than one notary configuration or key within a configuration are present, one of those can be called \"default\" (setting the name field). That way it should be obvious enough, which configuration or key will be used, if not further specified within the image policy, while keeping configuration effort low. Choice 3 \u2693\ufe0e Option 3 was chosen. Notary and Connaisseur will be completely decoupled, with Connaisseur logging all notaries it can't reach. This way Connaisseur can still be operational, even with all notaries being unreachable. Otherwise Connaisseur would have blocked even images that were allowlisted. This is a breaking change, but we agreed that it is better as it allows e.g. deployments for which the respective image policy specifies verify: false .","title":"ADR 3: Multiple Notary Configuration"},{"location":"adr/ADR-3_multi_notary_config/#adr-3-multiple-notary-configuration","text":"","title":"ADR 3: Multiple Notary Configuration"},{"location":"adr/ADR-3_multi_notary_config/#status","text":"Accepted","title":"Status"},{"location":"adr/ADR-3_multi_notary_config/#context","text":"Previously Connaisseur only supported the configuration of a single notary, where all signature data had to reside in. Unfortunately this is rather impractical, as one doesn't create all signatures for all images one uses in a cluster. There is a need to access signature data from multiple places, like in a setup where most images come from a private registry + notary and some from DockerHub and their notary. There is also the problem that a single notary instance could use multiple root keys, used for creating the signatures, like in the case of DockerHub. Connaisseur also only supports a single root key to be trust pinned, thus making it impractical. That's why the decision was made to support more than one notary and multiple keys per notary, which leads to the question how the new configuration should look like. This also has implications on the notary health check, which is important for Connaisseur's own readiness check.","title":"Context"},{"location":"adr/ADR-3_multi_notary_config/#considered-options","text":"","title":"Considered options"},{"location":"adr/ADR-3_multi_notary_config/#choice-1","text":"The overall notary configuration setup in helm/values.yaml .","title":"Choice 1"},{"location":"adr/ADR-3_multi_notary_config/#option-1-per-notary","text":"The notary field becomes a list and changes to notaries . Per to be used notary instance, there will be one entry in this list. The entry will have the following data fields ( bold are mandatory): name -- A unique identifier for the notary configuration, which will be used in the image policy. host -- The host address of the notary instance. pub_root_keys -- A list of public root keys, which are to be used for signature verification. name -- An identifier for a single public root key, which will be used in the image policy. key -- The actual public root key in PEM format. selfsigned_cert -- A self-signed certificate in PEM format, for making secure TLS connection to the notary. auth -- Authentication details, should the notary require some. user -- Username to authenticate with. password -- Password to authenticate with. secretName -- Kubernetes secret reference to use INSTEAD of user/password combination. is_acr -- Marks the notary as being part of an Azure Container Registry. The image policy will have two additional fields per rule entry (in \"quotes\" are already present fields): \" pattern \" -- Image pattern to match against, for rule to apply. \" verify \" -- Whether the images should be verified or not. \" delegations \" -- List of required delegation roles. notary -- Which notary to use for any matching image. This has to correspond to a name field of one configured notary. What happens if none is given, is defined by the result of choice 2. key -- Which key to use for doing the signature verification. This has to correspond to a name field of one of the public keys configured for the notary corresponding to the image policy's notary field. What happens if none is given, is defined by the result of choice 2.","title":"Option 1 (Per Notary)"},{"location":"adr/ADR-3_multi_notary_config/#option-2-per-notary-key","text":"The notary field becomes a list and changes to notaries . Per notary + public root key combination, there is one entry. Meaning, for example, there will be one entry for DockerHub and the public key for all official images and there will be another entry for DockerHub and the public key for some private images. The entries will look identical to the one's from option 1, with two exceptions. The pub_root_keys field of the notary configurations won't be a list and only has a single entry, without needing to specify a key name. The image policy will only address the notary configuration to be chosen with the notary field, without the need for a key field.","title":"Option 2 (Per Notary + Key)"},{"location":"adr/ADR-3_multi_notary_config/#choice-2","text":"Default values for notary (and key ) inside the image policy.","title":"Choice 2"},{"location":"adr/ADR-3_multi_notary_config/#option-1-first-item","text":"When no notary is specified in a image policy rule, the first entry in the notaries configuration list is taken. The same goes for the public root key list, should option 1 for choice 1 be chosen. Problem: Might get inconsistent, should list ordering in python get shuffled around","title":"Option 1 (First item)"},{"location":"adr/ADR-3_multi_notary_config/#option-2-explicit-default","text":"One of the notary configuration will be given a default field, which marks it as the default value. Problem: No real problems here, just an extra field that the user has to care about.","title":"Option 2 (Explicit default)"},{"location":"adr/ADR-3_multi_notary_config/#option-3-mandatory-notary","text":"The notary (and potentially key ) field is mandatory for the image policy. Problem: Creates configuration overhead if many image policies use the same notary/key combination.","title":"Option 3 (Mandatory Notary)"},{"location":"adr/ADR-3_multi_notary_config/#option-4-default-name","text":"If no notary or key are given in the image policy, it is assumed that one of the elements in the notary list or key list has name: \"default\" , which will then be taken. Should the assumption be wrong, an error is raised.","title":"Option 4 (Default name)"},{"location":"adr/ADR-3_multi_notary_config/#choice-3","text":"Previously, the readiness probe for connaisseur also considered the notary's health for its own status. With multiple notary instances configured, this behavior changes.","title":"Choice 3"},{"location":"adr/ADR-3_multi_notary_config/#option-1-ignore-notary","text":"The readiness probe of Connaisseur will no longer be dependent on any notary health checks. The are completely decoupled. Problem: No knowledge that Connaisseur will automatically fail because of an unreachable notary, before one tries to deploy an image.","title":"Option 1 (Ignore Notary)"},{"location":"adr/ADR-3_multi_notary_config/#option-2-health-check-on-all","text":"In order for connaisseur to be ready, all configured notaries must be healthy and reachable. Problem: A single unreachable notary will \"disable\" Connaisseur's access to all others.","title":"Option 2 (Health check on all)"},{"location":"adr/ADR-3_multi_notary_config/#option-3-log-notary-status","text":"A mix of option 1 and 2, whereas the readiness of Connaisseur is independent of the notaries health check, but they are still being made, so unhealthy notaries can be logged. Problem: At what interval should be logged?","title":"Option 3 (Log Notary status)"},{"location":"adr/ADR-3_multi_notary_config/#decision-outcome","text":"","title":"Decision outcome"},{"location":"adr/ADR-3_multi_notary_config/#choice-1_1","text":"Option 1 was chosen, to keep configurational duplication at a minimum.","title":"Choice 1"},{"location":"adr/ADR-3_multi_notary_config/#choice-2_1","text":"Option 4 was chosen. If more than one notary configuration or key within a configuration are present, one of those can be called \"default\" (setting the name field). That way it should be obvious enough, which configuration or key will be used, if not further specified within the image policy, while keeping configuration effort low.","title":"Choice 2"},{"location":"adr/ADR-3_multi_notary_config/#choice-3_1","text":"Option 3 was chosen. Notary and Connaisseur will be completely decoupled, with Connaisseur logging all notaries it can't reach. This way Connaisseur can still be operational, even with all notaries being unreachable. Otherwise Connaisseur would have blocked even images that were allowlisted. This is a breaking change, but we agreed that it is better as it allows e.g. deployments for which the respective image policy specifies verify: false .","title":"Choice 3"},{"location":"adr/ADR-4_modular/","text":"ADR 4: Modular Validation \u2693\ufe0e Status \u2693\ufe0e Accepted Context \u2693\ufe0e With the upcoming of notaryv2 and similar projects like cosign the opportunity for Connaisseur arises to support multiple signing mechanisms, and combine all into a single validation tool. For that to work, the internal validation mechanism of connaisseur needs to be more modular, so we can easily swap in and out different methods. Considered Options \u2693\ufe0e Configuration Changes (Choice 1) \u2693\ufe0e Obviously some changes have to be made to the configuration of Connaisseur, but this splits into changes for the previous notary configurations and the image policy. \"Notary\" Configuration (1.1) \u2693\ufe0e With notaryv1 all trust data always resided in a notary server for which Connaisseur needed the URL, authentication credentials, etc. This isn't true anymore for notaryv2 or cosign. Here Connaisseur may need other data, meaning the configuration is dependent on the type of validation method used here. Also other mechanisms such as digest whitelisting which doesn't even include cryptographic material might be considered in the future. 1.1.1 Structure \u2693\ufe0e Option 1.1.1.1 \u2693\ufe0e The previous notaries section in the values.yaml changes to validators , in which different validation methods (validators) can be defined. The least required fields a validator needs are a name for later referencing and a type for knowing its correct kind. validators : - name : \"dockerhub-nv2\" type : \"notaryv2\" ... - name : \"harbor-nv1\" type : \"notaryv1\" host : \"notary.harbor.io\" root_keys : - name : \"default\" key : \"...\" - name : \"cosign\" type : \"cosign\" ... Depending on the type, additional fields might be required, e.g. the notaryv1 type requires a host and root_keys field. NB: JSON schema validation works for the above and can easily handle various configurations based on type in there. Decision \u2693\ufe0e We are going with this structure ( option 1.1.1.1 ) due to the lack of other alternatives. It provides all needed information and the flexibility to use multiple validation methods, as needed. 1.1.2 Sensitive values \u2693\ufe0e If we allow multiple validators that may contain different forms of sensitive values, i.e. notary credentials, symmetric keys, service principals, ..., they need to be properly handled within the Helm chart with respect to ConfigMaps and Secrets. Currently, the distinction is hard-coded. Option 1.1.2.1 \u2693\ufe0e Add an optional sensitive([-_]fields) field at the validator config top level. Any sensitive values go in there and will be handled by the Helm chart to go into a secret. Any other values are treated as public and go into the ConfigMap. Advantages: - Generic configuration - Could be used by potential plugin validators to have their data properly handled (potential future) - Hard to forget the configuration for newly implemented validators Disadvantage: If implemented in a config = merge(secret, configmap) way, might allow sensitive values in configmap and Connaisseur still working Option 1.1.2.2 \u2693\ufe0e Hard-code sensitive values based on validator type Advantages: Can do very strict validation on fields without extra work Disadvantages: - Helm chart change might be forgotten for new validator - Helm chart release required for new validator - Does not \"natively\" allow plugins Decision \u2693\ufe0e We are going with option 1.1.2.2 and hard code the sensitive fields, to prevent users from misconfigure and accidentally but sensitive parts into configmaps. Image policy (1.2) \u2693\ufe0e For the image policy similar changes to the notary configuration have to be made. Proposition \u2693\ufe0e The previous notary field in the image policy will be changed to validator , referencing a name field of one item in the validators list. Any additional fields, e.g. required delegation roles for a notaryv1 validator will be given in a with field. This will look similar to this: policy : - pattern : \"docker.harbor.io/*:*\" validator : \"harbor-nv1\" with : key : \"default\" delegations : - lou - max - pattern : \"docker.io/*:*\" validator : \"dockerhub-nv2\" Option 1.2.1.1 \u2693\ufe0e Besides the self configured validator, two additional validators will be available: allow and deny . The allow validator will allow any image and the deny validator will deny anything. Advantages: More powerful than verify flag, i.e. has explicit deny option. Disadvantages: More config changes for users Option 1.2.1.2 \u2693\ufe0e Stick with current verify flag. Advantages: Config known for current users Disadvantages: No explicit deny option Decision \u2693\ufe0e We are going with option 1.2.1.1 , as we don't have to use additional fields and offer more powerful configuration options. Option 1.2.2.1 \u2693\ufe0e When no validator given, default to deny validator. Advantages: Easy Disadvantages: Not explicit Option 1.2.2.2 \u2693\ufe0e Require validator in policy config. Advantages: Explicit configuration, no accidental denying images Disadvantages: ? Decision \u2693\ufe0e We are going with option 1.2.2.1 as it reduces configurational effort and is consistent with the key selection behavior. Option 1.2.3.1 \u2693\ufe0e The validators from option 1.2.1.1 ( allow and deny ) will be purely internal, and additional validator can not be named \"allow\" or \"deny\". Advantages: Less configurational effort Disadvantage: A bit obscure for users Option 1.2.3.2 \u2693\ufe0e The allow and deny validator will be added to the default configuration as type: static with an extra argument (name up for discussion) that specifies whether everything should be denied or allowed. E.g.: validators : - name : allow type : static approve : true - name : deny type : static approve : false - ... Advantages: No obscurity, if user don't need these they can delete them. Disadvantage: Bigger config file ...? Decision \u2693\ufe0e We are going with option 1.2.3.2 as we favor less obscurity over the \"bigger\" configurational \"effort\". Validator interface (Choice 2) \u2693\ufe0e See validator interface Should validation return JSON patch or digest? Option 2.1.1 \u2693\ufe0e Validator.validate creates a JSON patch for the k8s request. Hence, different validators might make changes in addition to transforming tag to digest. Advantages: More flexibility in the future Disadvantages: We open the door to changes that are not core to Connaisseur functionality Option 2.1.2 \u2693\ufe0e Validator.validate returns a digest and Connaisseur uses the digest in a \"standardized\" way to create a JSON patch for the k8s request. Advantage: No code duplication and we stay with core feature of translating input data to trusted digest Disadvantages: Allowing additional changes would require additional work if we wanted to allow them in the future Decision \u2693\ufe0e We are going with option 2.1.2 as all current and upcoming validation methods return a digest.","title":"ADR 4: Modular Validation"},{"location":"adr/ADR-4_modular/#adr-4-modular-validation","text":"","title":"ADR 4: Modular Validation"},{"location":"adr/ADR-4_modular/#status","text":"Accepted","title":"Status"},{"location":"adr/ADR-4_modular/#context","text":"With the upcoming of notaryv2 and similar projects like cosign the opportunity for Connaisseur arises to support multiple signing mechanisms, and combine all into a single validation tool. For that to work, the internal validation mechanism of connaisseur needs to be more modular, so we can easily swap in and out different methods.","title":"Context"},{"location":"adr/ADR-4_modular/#considered-options","text":"","title":"Considered Options"},{"location":"adr/ADR-4_modular/#configuration-changes-choice-1","text":"Obviously some changes have to be made to the configuration of Connaisseur, but this splits into changes for the previous notary configurations and the image policy.","title":"Configuration Changes (Choice 1)"},{"location":"adr/ADR-4_modular/#notary-configuration-11","text":"With notaryv1 all trust data always resided in a notary server for which Connaisseur needed the URL, authentication credentials, etc. This isn't true anymore for notaryv2 or cosign. Here Connaisseur may need other data, meaning the configuration is dependent on the type of validation method used here. Also other mechanisms such as digest whitelisting which doesn't even include cryptographic material might be considered in the future.","title":"\"Notary\" Configuration (1.1)"},{"location":"adr/ADR-4_modular/#111-structure","text":"","title":"1.1.1 Structure"},{"location":"adr/ADR-4_modular/#option-1111","text":"The previous notaries section in the values.yaml changes to validators , in which different validation methods (validators) can be defined. The least required fields a validator needs are a name for later referencing and a type for knowing its correct kind. validators : - name : \"dockerhub-nv2\" type : \"notaryv2\" ... - name : \"harbor-nv1\" type : \"notaryv1\" host : \"notary.harbor.io\" root_keys : - name : \"default\" key : \"...\" - name : \"cosign\" type : \"cosign\" ... Depending on the type, additional fields might be required, e.g. the notaryv1 type requires a host and root_keys field. NB: JSON schema validation works for the above and can easily handle various configurations based on type in there.","title":"Option 1.1.1.1"},{"location":"adr/ADR-4_modular/#decision","text":"We are going with this structure ( option 1.1.1.1 ) due to the lack of other alternatives. It provides all needed information and the flexibility to use multiple validation methods, as needed.","title":"Decision"},{"location":"adr/ADR-4_modular/#112-sensitive-values","text":"If we allow multiple validators that may contain different forms of sensitive values, i.e. notary credentials, symmetric keys, service principals, ..., they need to be properly handled within the Helm chart with respect to ConfigMaps and Secrets. Currently, the distinction is hard-coded.","title":"1.1.2 Sensitive values"},{"location":"adr/ADR-4_modular/#option-1121","text":"Add an optional sensitive([-_]fields) field at the validator config top level. Any sensitive values go in there and will be handled by the Helm chart to go into a secret. Any other values are treated as public and go into the ConfigMap. Advantages: - Generic configuration - Could be used by potential plugin validators to have their data properly handled (potential future) - Hard to forget the configuration for newly implemented validators Disadvantage: If implemented in a config = merge(secret, configmap) way, might allow sensitive values in configmap and Connaisseur still working","title":"Option 1.1.2.1"},{"location":"adr/ADR-4_modular/#option-1122","text":"Hard-code sensitive values based on validator type Advantages: Can do very strict validation on fields without extra work Disadvantages: - Helm chart change might be forgotten for new validator - Helm chart release required for new validator - Does not \"natively\" allow plugins","title":"Option 1.1.2.2"},{"location":"adr/ADR-4_modular/#decision_1","text":"We are going with option 1.1.2.2 and hard code the sensitive fields, to prevent users from misconfigure and accidentally but sensitive parts into configmaps.","title":"Decision"},{"location":"adr/ADR-4_modular/#image-policy-12","text":"For the image policy similar changes to the notary configuration have to be made.","title":"Image policy (1.2)"},{"location":"adr/ADR-4_modular/#proposition","text":"The previous notary field in the image policy will be changed to validator , referencing a name field of one item in the validators list. Any additional fields, e.g. required delegation roles for a notaryv1 validator will be given in a with field. This will look similar to this: policy : - pattern : \"docker.harbor.io/*:*\" validator : \"harbor-nv1\" with : key : \"default\" delegations : - lou - max - pattern : \"docker.io/*:*\" validator : \"dockerhub-nv2\"","title":"Proposition"},{"location":"adr/ADR-4_modular/#option-1211","text":"Besides the self configured validator, two additional validators will be available: allow and deny . The allow validator will allow any image and the deny validator will deny anything. Advantages: More powerful than verify flag, i.e. has explicit deny option. Disadvantages: More config changes for users","title":"Option 1.2.1.1"},{"location":"adr/ADR-4_modular/#option-1212","text":"Stick with current verify flag. Advantages: Config known for current users Disadvantages: No explicit deny option","title":"Option 1.2.1.2"},{"location":"adr/ADR-4_modular/#decision_2","text":"We are going with option 1.2.1.1 , as we don't have to use additional fields and offer more powerful configuration options.","title":"Decision"},{"location":"adr/ADR-4_modular/#option-1221","text":"When no validator given, default to deny validator. Advantages: Easy Disadvantages: Not explicit","title":"Option 1.2.2.1"},{"location":"adr/ADR-4_modular/#option-1222","text":"Require validator in policy config. Advantages: Explicit configuration, no accidental denying images Disadvantages: ?","title":"Option 1.2.2.2"},{"location":"adr/ADR-4_modular/#decision_3","text":"We are going with option 1.2.2.1 as it reduces configurational effort and is consistent with the key selection behavior.","title":"Decision"},{"location":"adr/ADR-4_modular/#option-1231","text":"The validators from option 1.2.1.1 ( allow and deny ) will be purely internal, and additional validator can not be named \"allow\" or \"deny\". Advantages: Less configurational effort Disadvantage: A bit obscure for users","title":"Option 1.2.3.1"},{"location":"adr/ADR-4_modular/#option-1232","text":"The allow and deny validator will be added to the default configuration as type: static with an extra argument (name up for discussion) that specifies whether everything should be denied or allowed. E.g.: validators : - name : allow type : static approve : true - name : deny type : static approve : false - ... Advantages: No obscurity, if user don't need these they can delete them. Disadvantage: Bigger config file ...?","title":"Option 1.2.3.2"},{"location":"adr/ADR-4_modular/#decision_4","text":"We are going with option 1.2.3.2 as we favor less obscurity over the \"bigger\" configurational \"effort\".","title":"Decision"},{"location":"adr/ADR-4_modular/#validator-interface-choice-2","text":"See validator interface Should validation return JSON patch or digest?","title":"Validator interface (Choice 2)"},{"location":"adr/ADR-4_modular/#option-211","text":"Validator.validate creates a JSON patch for the k8s request. Hence, different validators might make changes in addition to transforming tag to digest. Advantages: More flexibility in the future Disadvantages: We open the door to changes that are not core to Connaisseur functionality","title":"Option 2.1.1"},{"location":"adr/ADR-4_modular/#option-212","text":"Validator.validate returns a digest and Connaisseur uses the digest in a \"standardized\" way to create a JSON patch for the k8s request. Advantage: No code duplication and we stay with core feature of translating input data to trusted digest Disadvantages: Allowing additional changes would require additional work if we wanted to allow them in the future","title":"Option 2.1.2"},{"location":"adr/ADR-4_modular/#decision_5","text":"We are going with option 2.1.2 as all current and upcoming validation methods return a digest.","title":"Decision"},{"location":"features/","text":"Overview \u2693\ufe0e Besides Connaisseur's central functionality, several additional features are available: Detection Mode : warn but do not block invalid images Namespaced Validation : restrict validation to dedicated namespaces Alerting : send alerts based on verification result In combination, these features help to improve usability and might better support the DevOps workflow. Switching Connaisseur to detection mode and alerting on non-compliant images can for example avoid service interruptions while still benefitting from improved supply-chain security. Feel free to propose new features that would make Connaisseur an even better experience","title":"Overview"},{"location":"features/#overview","text":"Besides Connaisseur's central functionality, several additional features are available: Detection Mode : warn but do not block invalid images Namespaced Validation : restrict validation to dedicated namespaces Alerting : send alerts based on verification result In combination, these features help to improve usability and might better support the DevOps workflow. Switching Connaisseur to detection mode and alerting on non-compliant images can for example avoid service interruptions while still benefitting from improved supply-chain security. Feel free to propose new features that would make Connaisseur an even better experience","title":"Overview"},{"location":"features/alerting/","text":"Alerting \u2693\ufe0e Connaisseur can send notifications on admission decisions to basically every REST endpoint that accepts JSON payloads. Supported Interfaces \u2693\ufe0e Slack, Opsgenie and Keybase have preconfigured payloads that are ready to use. However, you can use the existing payload templates as an example how to model your own custom one. It is also possible to configure multiple interfaces to receive alerts on at the same time. Configuration options \u2693\ufe0e Currently, Connaisseur supports alerting on either admittance of images, denial of images or both. These event categories can be configured independently of each other under the relevant category (e.g. admit_request or reject_request ): key accepted values default required description alerting.cluster_identifier string \"not specified\" cluster identifier used in alert payload to distinguish between alerts from different clusters alerting.<category>.template opsgenie , slack , keybase or custom * - specifies which file in helm/alert_payload_templates/ to use as alert payload template alerting.<category>.receiver_url string - url of alert-receiving endpoint alerting.<category>.priority int 3 priority of alert (to enable fitting Connaisseur alerts into alerts from other sources) alerting.<category>.custom_headers list[string] - additional headers required by alert-receiving endpoint alerting.<category>.payload_fields subyaml - enables specifying additional ( yaml ) key-value pairs to be appended to alert payload (as json ) alerting.<category>.fail_if_alert_sending_fails bool False true will make Connaisseur deny images if the corresponding alert cannot be successfully sent *basename of the custom template file in helm/alerting_payload_templates without file extension Notes : The value for template needs to match an existing file of the pattern helm/alert_payload_templates/<template>.json ; so if you want to use a predefined one it needs to be one of slack , keybase or opsgenie . For Opsgenie you need to configure an additional [\"Authorization: GenieKey <Your-Genie-Key>\"] header. fail_if_alert_sending_fails makes sense only for requests that Connaisseur would have admitted as other requests would have been denied in the first place. The setting can come handy if you want to run Connaisseur in detection mode but still make sure that you get notified about what is going on in your cluster. However, this setting will block everyone from contributing if the alert sending fails permanently. (Accidental deletion of your Slack Webhook App, GenieKey expired...) Example \u2693\ufe0e If you for example would like to receive notifications in Keybase whenever Connaisseur admits a request to your cluster, your alerting configuration would look similar to to following snippet: alerting: admit_request: templates: - template: keybase receiver_url: https://bots.keybase.io/webhookbot/<Your-Keybase-Hook-Token> Creating a custom template \u2693\ufe0e Along the lines of the templates that are already there you can easily define custom templates for other endpoints. The following variables can be rendered during runtime into the payload: alert_message priority connaisseur_pod_id cluster timestamp request_id images Referring to any of these variables in the templates works by Jinja2 notation (e.g. {{ timestamp }} ). You can update your payload dynamically by adding payload fields in yaml presentation in the payload_fields key which will be translated to JSON by helm as is. If your REST endpoint requires particular headers, your can specify them as described above in custom_headers . Feel free to open a PR if you add new neat templates for other third parties!","title":"Alerting"},{"location":"features/alerting/#alerting","text":"Connaisseur can send notifications on admission decisions to basically every REST endpoint that accepts JSON payloads.","title":"Alerting"},{"location":"features/alerting/#supported-interfaces","text":"Slack, Opsgenie and Keybase have preconfigured payloads that are ready to use. However, you can use the existing payload templates as an example how to model your own custom one. It is also possible to configure multiple interfaces to receive alerts on at the same time.","title":"Supported Interfaces"},{"location":"features/alerting/#configuration-options","text":"Currently, Connaisseur supports alerting on either admittance of images, denial of images or both. These event categories can be configured independently of each other under the relevant category (e.g. admit_request or reject_request ): key accepted values default required description alerting.cluster_identifier string \"not specified\" cluster identifier used in alert payload to distinguish between alerts from different clusters alerting.<category>.template opsgenie , slack , keybase or custom * - specifies which file in helm/alert_payload_templates/ to use as alert payload template alerting.<category>.receiver_url string - url of alert-receiving endpoint alerting.<category>.priority int 3 priority of alert (to enable fitting Connaisseur alerts into alerts from other sources) alerting.<category>.custom_headers list[string] - additional headers required by alert-receiving endpoint alerting.<category>.payload_fields subyaml - enables specifying additional ( yaml ) key-value pairs to be appended to alert payload (as json ) alerting.<category>.fail_if_alert_sending_fails bool False true will make Connaisseur deny images if the corresponding alert cannot be successfully sent *basename of the custom template file in helm/alerting_payload_templates without file extension Notes : The value for template needs to match an existing file of the pattern helm/alert_payload_templates/<template>.json ; so if you want to use a predefined one it needs to be one of slack , keybase or opsgenie . For Opsgenie you need to configure an additional [\"Authorization: GenieKey <Your-Genie-Key>\"] header. fail_if_alert_sending_fails makes sense only for requests that Connaisseur would have admitted as other requests would have been denied in the first place. The setting can come handy if you want to run Connaisseur in detection mode but still make sure that you get notified about what is going on in your cluster. However, this setting will block everyone from contributing if the alert sending fails permanently. (Accidental deletion of your Slack Webhook App, GenieKey expired...)","title":"Configuration options"},{"location":"features/alerting/#example","text":"If you for example would like to receive notifications in Keybase whenever Connaisseur admits a request to your cluster, your alerting configuration would look similar to to following snippet: alerting: admit_request: templates: - template: keybase receiver_url: https://bots.keybase.io/webhookbot/<Your-Keybase-Hook-Token>","title":"Example"},{"location":"features/alerting/#creating-a-custom-template","text":"Along the lines of the templates that are already there you can easily define custom templates for other endpoints. The following variables can be rendered during runtime into the payload: alert_message priority connaisseur_pod_id cluster timestamp request_id images Referring to any of these variables in the templates works by Jinja2 notation (e.g. {{ timestamp }} ). You can update your payload dynamically by adding payload fields in yaml presentation in the payload_fields key which will be translated to JSON by helm as is. If your REST endpoint requires particular headers, your can specify them as described above in custom_headers . Feel free to open a PR if you add new neat templates for other third parties!","title":"Creating a custom template"},{"location":"features/detection_mode/","text":"Detection Mode \u2693\ufe0e A detection mode is available in order to avoid interruptions of a running cluster, to support initial rollout or for testing purposes. In detection mode, Connaisseur admits all images to the cluster, but issues a warning and logs an error message for images that do not comply with the policy or in case of other unexpected failures. To activate the detection mode, set the detectionMode flag to true in helm/values.yaml . Configuration options \u2693\ufe0e detectionMode in helm/values.yaml supports the following keys: key default required description detectionMode false true / false ; when detection mode is enabled, Connaisseur will warn but not deny requests with untrusted images Example \u2693\ufe0e In helm/values.yaml : detectionMode: true","title":"Detection Mode"},{"location":"features/detection_mode/#detection-mode","text":"A detection mode is available in order to avoid interruptions of a running cluster, to support initial rollout or for testing purposes. In detection mode, Connaisseur admits all images to the cluster, but issues a warning and logs an error message for images that do not comply with the policy or in case of other unexpected failures. To activate the detection mode, set the detectionMode flag to true in helm/values.yaml .","title":"Detection Mode"},{"location":"features/detection_mode/#configuration-options","text":"detectionMode in helm/values.yaml supports the following keys: key default required description detectionMode false true / false ; when detection mode is enabled, Connaisseur will warn but not deny requests with untrusted images","title":"Configuration options"},{"location":"features/detection_mode/#example","text":"In helm/values.yaml : detectionMode: true","title":"Example"},{"location":"features/namespaced_validation/","text":"Namespaced Validation \u2693\ufe0e Namespaced validation allows restricting validation to specific namespaces. Connaisseur will only verify trust of images deployed to the configured namespaces. This can greatly support initial rollout by stepwise extending the validated namespaces or excluding specific namespaces for which signatures are unfeasible. Enabling namespaced validation, allows roles with edit permissions on namespaces to disable validation for those namespaces. Namespaced validation offers two modes: ignore : ignore all namespaces with label securesystemsengineering.connaisseur/webhook: ignore validate : only validate namespaces with label securesystemsengineering.connaisseur/webhook: validate The desired namespaces must be labelled accordingly, e.g. via: # either kubectl namespaces <namespace> securesystemsengineering.connaisseur/webhook=ignore # or kubectl namespaces <namespace> securesystemsengineering.connaisseur/webhook=validate Configure namespaced validation via the namespacedValidation in helm/values.yaml . Configuration options \u2693\ufe0e namespacedValidation in helm/values.yaml supports the following keys: key default required description enabled false true / false ; enable namespaced validation otherwise images in all namespaces will be validated mode ignore ignore / validate ; configure mode of exclusion to either ignore all namespaces with label securesystemsengineering.connaisseur/webhook set to ignore or only validate namespaces with the label set to validate Example \u2693\ufe0e In helm/values.yaml : namespacedValidation: enabled: true mode: validate Labelling target namespace to be validated: kubectl namespaces validateme securesystemsengineering.connaisseur/webhook=validate","title":"Namespaced Validation"},{"location":"features/namespaced_validation/#namespaced-validation","text":"Namespaced validation allows restricting validation to specific namespaces. Connaisseur will only verify trust of images deployed to the configured namespaces. This can greatly support initial rollout by stepwise extending the validated namespaces or excluding specific namespaces for which signatures are unfeasible. Enabling namespaced validation, allows roles with edit permissions on namespaces to disable validation for those namespaces. Namespaced validation offers two modes: ignore : ignore all namespaces with label securesystemsengineering.connaisseur/webhook: ignore validate : only validate namespaces with label securesystemsengineering.connaisseur/webhook: validate The desired namespaces must be labelled accordingly, e.g. via: # either kubectl namespaces <namespace> securesystemsengineering.connaisseur/webhook=ignore # or kubectl namespaces <namespace> securesystemsengineering.connaisseur/webhook=validate Configure namespaced validation via the namespacedValidation in helm/values.yaml .","title":"Namespaced Validation"},{"location":"features/namespaced_validation/#configuration-options","text":"namespacedValidation in helm/values.yaml supports the following keys: key default required description enabled false true / false ; enable namespaced validation otherwise images in all namespaces will be validated mode ignore ignore / validate ; configure mode of exclusion to either ignore all namespaces with label securesystemsengineering.connaisseur/webhook set to ignore or only validate namespaces with the label set to validate","title":"Configuration options"},{"location":"features/namespaced_validation/#example","text":"In helm/values.yaml : namespacedValidation: enabled: true mode: validate Labelling target namespace to be validated: kubectl namespaces validateme securesystemsengineering.connaisseur/webhook=validate","title":"Example"},{"location":"validators/","text":"Overview \u2693\ufe0e Connaisseur is built to be extendable and currently aims to support the following signing solutions: Docker Content Trust (DCT) / Notary V1 Sigstore / Cosign (EXPERIMENTAL) Notary V2 (PLANNED) Feel free to use any or a combination of all solutions. The integration with Connaisseur is detailed on the following pages. For advantages and disadvantages of each solution, please refer to the respective docs.","title":"Overview"},{"location":"validators/#overview","text":"Connaisseur is built to be extendable and currently aims to support the following signing solutions: Docker Content Trust (DCT) / Notary V1 Sigstore / Cosign (EXPERIMENTAL) Notary V2 (PLANNED) Feel free to use any or a combination of all solutions. The integration with Connaisseur is detailed on the following pages. For advantages and disadvantages of each solution, please refer to the respective docs.","title":"Overview"},{"location":"validators/notaryv1/","text":"Notary (V1) / DCT \u2693\ufe0e Notary works as an external service holding signatures and trust data based on TUF of any kind of data. Official DockerHub uses \"Docker Content Trust\" (DCT) to sign its docker images and pushes the repository's signature data to its associated Notary server (notary.docker.io). Validating a single docker image requires the public repository root key as well as fetching the repository's trust data from the associated Notary server. Besides of public Notary instaces as e.g. notary.docker.io being ready to use, Notary can as well be run as a private instance. Harbor for instance comes along with an associated Notary instance. Notary (v1) will be the default validator when signing images using Docker Content Trust (DCT). Basic usage \u2693\ufe0e Creating the signature \u2693\ufe0e Before you can start validating images using the Notary (v1) validator, you'll first need an image which has been signed using DCT. Easiest way to do this is by pushing an image of your choice (e.g. busybox:stable ) to your private DockerHub repository while having the DOCKER_CONTENT_TRUST environment variable set to 1 . If you haven't created any signatures for images in the current repository yet, you'll be asked to enter a passphrase for a root key and targets key, which get generated on your machine. Have a look into the TUF documentation to read more about TUF roles and their meanings. If you already have these keys, just enter the required passphrase. > DOCKER_CONTENT_TRUST = 1 docker push <your-private-repo>/busybox:stable The push refers to repository [ <your-private-repo>/busybox ] 5b8c72934dfc: Pushed stable: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527 Signing and pushing trust metadata You are about to create a new root signing key passphrase. This passphrase will be used to protect the most sensitive key in your signing system. Please choose a long, complex passphrase and be careful to keep the password and the key file itself secure and backed up. It is highly recommended that you use a password manager to generate the passphrase and keep it safe. There will be no way to recover this key. You can find the key in your config directory. Enter passphrase for new root key with ID 5fb3e1e: Repeat passphrase for new root key with ID 5fb3e1e: Enter passphrase for new repository key with ID 6c2a04c: Repeat passphrase for new repository key with ID 6c2a04c: Finished initializing \"<your-private-repo>/busybox\" After doing so, the signature for your image will reside in the public DockerHub Notary (notary.docker.io). Your newly created private keys will be needed whenever you push a new version of your image. They reside in your ~/.docker/trust/private directory. Getting the Root Key \u2693\ufe0e For verifying the signatures of images, a public root key is needed to verify the signature data with it. But from where do you get this, especially for public images whose signatures you didn't create? For this we created the get_root_key utility. You can use it by building the docker image with docker build -t get-root-key -f docker/Dockerfile.getRoot . and the running the image: $ docker run --rm get-root-key -i securesystemsengineering/testimage KeyID: 76d211ff8d2317d78ee597dbc43888599d691dbfd073b8226512f0e9848f2508 Key: -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsx28WV7BsQfnHF1kZmpdCTTLJaWe d0CA+JOi8H4REuBaWSZ5zPDe468WuOJ6f71E7WFg3CVEVYHuoZt2UYbN/Q == -----END PUBLIC KEY----- The -i ( --image ) option is required and takes the image, for which you want the public key. There is also the -s ( --server ) option, which defines the notary server that should be used and which defaults to notary.docker.io . The public repository root key resides with the signature data in the notary instance, so what the get_root_key utility does in the background is just fetching, locating and parsing the public repository root key for the given image. Configuration options \u2693\ufe0e The Notary (V1) validator supports the following fields (refer to basics for more information on default keys): key default required description name - Unique name of the validator type - This must be set to notaryv1 host - URL of the Notary instance, in which the signatures reside. trust_roots[*].name - see basics trust_roots[*].key - see basics auth - - Should the validator need to authenticate against some service, the credentials can be entered here. auth.secret_name - - (Preferred over username + password combination.) References a Kubernetes secret that must exist beforehand. Create a file auth.yaml containing username: <user> password: <password> and run kubectl create secret generic <kube-secret-name> --from-file auth.yaml . auth.username - - The username to authenticate with. It is recommended to use auth.secret_name instead. auth.password - - The password or access token to authenticate with. It is recommended to use auth.secret_name instead. cert - - If the notary instance uses a self-signed certificate, that cert must be supplied here in .pem format. is_acr false - Using Azure Container Registry (ACR) must be specified as ACR does not offer a health probe according to notary API specs. Validator mapping configuration options \u2693\ufe0e How to map validators on images is documented here . However, the image policy also has some additional fields regarding the delegation feature of Notary (V1), which is further explained down below . key default required description with.delegations - - A list of delegation names, which are required to be present as signers. Example \u2693\ufe0e validators : - name : docker_essentials type : notaryv1 host : notary.docker.io trust_roots : - name : sse key : | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA== -----END PUBLIC KEY----- policy : - pattern : \"docker.io/securesystemsengineering/connaisseur:*\" validator : docker_essentials with : key : sse delegations : - belitzphilipp - starkteetje Additional Features \u2693\ufe0e Delegations \u2693\ufe0e Notary (V1) offers the functionality to delegate trust. To better understand this feature, it's best to have a basic understanding of the TUF key hierarchy, or more specifically the purpose of the root, targets and delegation keys. If you are more interested in this topic, please read the TUF documentation . When creating the signatures of your docker images earlier, two keys were generated -- the root key and the targets key. The root key is the root of all trust and will be used whenever a new image repository is created and needs to be signed. It's also used to rotate all other kinds of keys, thus there is usually only one root key present. The targets key is needed for new signatures on one specific image repository, hence every image repository has its own targets key. Hierarchically speaking, the targets keys are below the root key, as the root key can be used to rotate the targets keys should they get compromised. Delegations will now go one level deeper, meaning they can be used to sign individual image repositories and only need the targets key for rotation purposes, instead of the root key. Also delegation keys are not bound to individual image repositories, so they can be reused multiple times over different image repositories. So in a sense they can be understood as keys for individual signers. To create a delegation key run: > docker trust key generate <your-name> Generating key for <your-name>... Enter passphrase for new <your-name> key with ID 9deed25: Repeat passphrase for new <your-name> key with ID 9deed25: Successfully generated and loaded private key. Corresponding public key available: /home/ubuntu/Documents/mytrustdir/<your-name>.pub This delegation key now needs to be added as a signer to a respective image repository, like the busybox one from above. In doing so, you'll be ask for the targets key. > docker trust signer add --key <your-name>.pub <your-name> <your-private-repo>/busybox Adding signer \"<your-name>\" to <your-private-repo>/busybox... Enter passphrase for repository key with ID b0014f8: Successfully added signer: <your-name> to <your-private-repo>/busybox If you create a new signature for the image now, you'll be ask for your delegation key instead of the targets key, therefore creating a signature using the delegation. > DOCKER_CONTENT_TRUST = 1 docker push <your-private-repo>/busybox:stable To tell Connaisseur that this signer/delegation is required to be present on an images' signature, the with.delegations list inside an image policy rule can be used. Simply add the signers name to the list. You can also add multiple signer names to the list, but be aware that Connaisseur currently only supports an AND conjunction, meaning every signer listed must have signed the matching image. policy : - pattern : \"*:*\" with : delegations : - <your-name> Additional notes \u2693\ufe0e Using Harbor container registry \u2693\ufe0e Using Azure Container Registry \u2693\ufe0e Using Azure Container Registry (ACR) must be specified in the validator configuration by setting is_acr to true . Moreover, you need to provide credentials of an Azure Identity having at least read access to the ACR (and, thus, to the associated notary instance). Assuming you have the az cli installed you can create a Service Principal for this by running: # Retrieve the ID of your registry REGISTRY_ID = $( az acr show --name <ACR-NAME> --query 'id' -otsv ) # Create a service principal with the Reader role on your registry az ad sp create-for-rbac --name \"<SERVICE-PRINCIPLE-NAME>\" --role Reader --scopes ${ REGISTRY_ID } Use the resulting applicationID as auth.username , the resulting password as auth.password and set <ACR>.azurecr.io as host in the helm/values.yaml and you're ready to go! Delegation feature \u2693\ufe0e Signatures in Docker Content Trust or Notary v1 are based on The Update Framework (TUF) . TUF allows delegating signature permissions to other people. Connaisseur supports requiring delegations for Notary v1 via the policy definition: Image Policy Example \u2693\ufe0e Let's say you have an organizational root key pinned in the helm/values.yaml , which you use to delegate to individual developers, i.e. via docker trust key load delegation.key --name charlie (see the Docker documentation for the docker CLI commands). Now, given an image policy policy : - pattern : \"*:*\" the signature of every single developer to whom you delegated signature rights will be considered valid. That might be fine for most use-cases, but for trusted-thing , which runs as a privileged container on all k8s nodes, you really want to limit yourself to Alice from the security team, whom you trust to have properly reviewed it. And also to Bob, your most senior developer, because they got that eye for detail and they really don't do the \"It's friday, force push master, I'm outta here\" stuff, that Charlie did last month... Anyways, Connaisseur can handle this: policy : - pattern : \"*:*\" - pattern : \"your.org/trusted-thing:*\" delegations : [ \"alice\" , \"bob\" ] Connaisseur will make sure, that both Alice and Bob have a current signature on the same digest for any tagged trusted-thing image you deploy. If one of their signatures is missing or if they point to different digests for the same tag, Connaisseur will block the deployment. In the meantime, for all other images any delegated individual key, repository key or the root key will be accepted. The list you add in the delegations field can contain any number of delegations. Just remember that it is a logical AND , so all delegations will have to verify correctly. Another potential use-case is to use delegations to sign an image in various stages of your CI, i.e. requiring the signature of your linter, your security scanner and your software lisence compliance check. Limits \u2693\ufe0e Q: Is a logical OR possible? Are there n-of-m thresholds? Is there insert-arbitrary-logical-function ? A: No, currently not. If you have the use-case, feel free to hit us up , open an issue or open a pull request. Q: Will delegations work with Cosign or Notary v2? A: Currently, Cosign does not natively support delegations, so Connaisseur doesn't either. For Notary v2, it looks like they will support delegations and so Connaisseur will most likely support them as in Notary v1.","title":"Notary (V1) / DCT"},{"location":"validators/notaryv1/#notary-v1-dct","text":"Notary works as an external service holding signatures and trust data based on TUF of any kind of data. Official DockerHub uses \"Docker Content Trust\" (DCT) to sign its docker images and pushes the repository's signature data to its associated Notary server (notary.docker.io). Validating a single docker image requires the public repository root key as well as fetching the repository's trust data from the associated Notary server. Besides of public Notary instaces as e.g. notary.docker.io being ready to use, Notary can as well be run as a private instance. Harbor for instance comes along with an associated Notary instance. Notary (v1) will be the default validator when signing images using Docker Content Trust (DCT).","title":"Notary (V1) / DCT"},{"location":"validators/notaryv1/#basic-usage","text":"","title":"Basic usage"},{"location":"validators/notaryv1/#creating-the-signature","text":"Before you can start validating images using the Notary (v1) validator, you'll first need an image which has been signed using DCT. Easiest way to do this is by pushing an image of your choice (e.g. busybox:stable ) to your private DockerHub repository while having the DOCKER_CONTENT_TRUST environment variable set to 1 . If you haven't created any signatures for images in the current repository yet, you'll be asked to enter a passphrase for a root key and targets key, which get generated on your machine. Have a look into the TUF documentation to read more about TUF roles and their meanings. If you already have these keys, just enter the required passphrase. > DOCKER_CONTENT_TRUST = 1 docker push <your-private-repo>/busybox:stable The push refers to repository [ <your-private-repo>/busybox ] 5b8c72934dfc: Pushed stable: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527 Signing and pushing trust metadata You are about to create a new root signing key passphrase. This passphrase will be used to protect the most sensitive key in your signing system. Please choose a long, complex passphrase and be careful to keep the password and the key file itself secure and backed up. It is highly recommended that you use a password manager to generate the passphrase and keep it safe. There will be no way to recover this key. You can find the key in your config directory. Enter passphrase for new root key with ID 5fb3e1e: Repeat passphrase for new root key with ID 5fb3e1e: Enter passphrase for new repository key with ID 6c2a04c: Repeat passphrase for new repository key with ID 6c2a04c: Finished initializing \"<your-private-repo>/busybox\" After doing so, the signature for your image will reside in the public DockerHub Notary (notary.docker.io). Your newly created private keys will be needed whenever you push a new version of your image. They reside in your ~/.docker/trust/private directory.","title":"Creating the signature"},{"location":"validators/notaryv1/#getting-the-root-key","text":"For verifying the signatures of images, a public root key is needed to verify the signature data with it. But from where do you get this, especially for public images whose signatures you didn't create? For this we created the get_root_key utility. You can use it by building the docker image with docker build -t get-root-key -f docker/Dockerfile.getRoot . and the running the image: $ docker run --rm get-root-key -i securesystemsengineering/testimage KeyID: 76d211ff8d2317d78ee597dbc43888599d691dbfd073b8226512f0e9848f2508 Key: -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsx28WV7BsQfnHF1kZmpdCTTLJaWe d0CA+JOi8H4REuBaWSZ5zPDe468WuOJ6f71E7WFg3CVEVYHuoZt2UYbN/Q == -----END PUBLIC KEY----- The -i ( --image ) option is required and takes the image, for which you want the public key. There is also the -s ( --server ) option, which defines the notary server that should be used and which defaults to notary.docker.io . The public repository root key resides with the signature data in the notary instance, so what the get_root_key utility does in the background is just fetching, locating and parsing the public repository root key for the given image.","title":"Getting the Root Key"},{"location":"validators/notaryv1/#configuration-options","text":"The Notary (V1) validator supports the following fields (refer to basics for more information on default keys): key default required description name - Unique name of the validator type - This must be set to notaryv1 host - URL of the Notary instance, in which the signatures reside. trust_roots[*].name - see basics trust_roots[*].key - see basics auth - - Should the validator need to authenticate against some service, the credentials can be entered here. auth.secret_name - - (Preferred over username + password combination.) References a Kubernetes secret that must exist beforehand. Create a file auth.yaml containing username: <user> password: <password> and run kubectl create secret generic <kube-secret-name> --from-file auth.yaml . auth.username - - The username to authenticate with. It is recommended to use auth.secret_name instead. auth.password - - The password or access token to authenticate with. It is recommended to use auth.secret_name instead. cert - - If the notary instance uses a self-signed certificate, that cert must be supplied here in .pem format. is_acr false - Using Azure Container Registry (ACR) must be specified as ACR does not offer a health probe according to notary API specs.","title":"Configuration options"},{"location":"validators/notaryv1/#validator-mapping-configuration-options","text":"How to map validators on images is documented here . However, the image policy also has some additional fields regarding the delegation feature of Notary (V1), which is further explained down below . key default required description with.delegations - - A list of delegation names, which are required to be present as signers.","title":"Validator mapping configuration options"},{"location":"validators/notaryv1/#example","text":"validators : - name : docker_essentials type : notaryv1 host : notary.docker.io trust_roots : - name : sse key : | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA== -----END PUBLIC KEY----- policy : - pattern : \"docker.io/securesystemsengineering/connaisseur:*\" validator : docker_essentials with : key : sse delegations : - belitzphilipp - starkteetje","title":"Example"},{"location":"validators/notaryv1/#additional-features","text":"","title":"Additional Features"},{"location":"validators/notaryv1/#delegations","text":"Notary (V1) offers the functionality to delegate trust. To better understand this feature, it's best to have a basic understanding of the TUF key hierarchy, or more specifically the purpose of the root, targets and delegation keys. If you are more interested in this topic, please read the TUF documentation . When creating the signatures of your docker images earlier, two keys were generated -- the root key and the targets key. The root key is the root of all trust and will be used whenever a new image repository is created and needs to be signed. It's also used to rotate all other kinds of keys, thus there is usually only one root key present. The targets key is needed for new signatures on one specific image repository, hence every image repository has its own targets key. Hierarchically speaking, the targets keys are below the root key, as the root key can be used to rotate the targets keys should they get compromised. Delegations will now go one level deeper, meaning they can be used to sign individual image repositories and only need the targets key for rotation purposes, instead of the root key. Also delegation keys are not bound to individual image repositories, so they can be reused multiple times over different image repositories. So in a sense they can be understood as keys for individual signers. To create a delegation key run: > docker trust key generate <your-name> Generating key for <your-name>... Enter passphrase for new <your-name> key with ID 9deed25: Repeat passphrase for new <your-name> key with ID 9deed25: Successfully generated and loaded private key. Corresponding public key available: /home/ubuntu/Documents/mytrustdir/<your-name>.pub This delegation key now needs to be added as a signer to a respective image repository, like the busybox one from above. In doing so, you'll be ask for the targets key. > docker trust signer add --key <your-name>.pub <your-name> <your-private-repo>/busybox Adding signer \"<your-name>\" to <your-private-repo>/busybox... Enter passphrase for repository key with ID b0014f8: Successfully added signer: <your-name> to <your-private-repo>/busybox If you create a new signature for the image now, you'll be ask for your delegation key instead of the targets key, therefore creating a signature using the delegation. > DOCKER_CONTENT_TRUST = 1 docker push <your-private-repo>/busybox:stable To tell Connaisseur that this signer/delegation is required to be present on an images' signature, the with.delegations list inside an image policy rule can be used. Simply add the signers name to the list. You can also add multiple signer names to the list, but be aware that Connaisseur currently only supports an AND conjunction, meaning every signer listed must have signed the matching image. policy : - pattern : \"*:*\" with : delegations : - <your-name>","title":"Delegations"},{"location":"validators/notaryv1/#additional-notes","text":"","title":"Additional notes"},{"location":"validators/notaryv1/#using-harbor-container-registry","text":"","title":"Using Harbor container registry"},{"location":"validators/notaryv1/#using-azure-container-registry","text":"Using Azure Container Registry (ACR) must be specified in the validator configuration by setting is_acr to true . Moreover, you need to provide credentials of an Azure Identity having at least read access to the ACR (and, thus, to the associated notary instance). Assuming you have the az cli installed you can create a Service Principal for this by running: # Retrieve the ID of your registry REGISTRY_ID = $( az acr show --name <ACR-NAME> --query 'id' -otsv ) # Create a service principal with the Reader role on your registry az ad sp create-for-rbac --name \"<SERVICE-PRINCIPLE-NAME>\" --role Reader --scopes ${ REGISTRY_ID } Use the resulting applicationID as auth.username , the resulting password as auth.password and set <ACR>.azurecr.io as host in the helm/values.yaml and you're ready to go!","title":"Using Azure Container Registry"},{"location":"validators/notaryv1/#delegation-feature","text":"Signatures in Docker Content Trust or Notary v1 are based on The Update Framework (TUF) . TUF allows delegating signature permissions to other people. Connaisseur supports requiring delegations for Notary v1 via the policy definition:","title":"Delegation feature"},{"location":"validators/notaryv1/#image-policy-example","text":"Let's say you have an organizational root key pinned in the helm/values.yaml , which you use to delegate to individual developers, i.e. via docker trust key load delegation.key --name charlie (see the Docker documentation for the docker CLI commands). Now, given an image policy policy : - pattern : \"*:*\" the signature of every single developer to whom you delegated signature rights will be considered valid. That might be fine for most use-cases, but for trusted-thing , which runs as a privileged container on all k8s nodes, you really want to limit yourself to Alice from the security team, whom you trust to have properly reviewed it. And also to Bob, your most senior developer, because they got that eye for detail and they really don't do the \"It's friday, force push master, I'm outta here\" stuff, that Charlie did last month... Anyways, Connaisseur can handle this: policy : - pattern : \"*:*\" - pattern : \"your.org/trusted-thing:*\" delegations : [ \"alice\" , \"bob\" ] Connaisseur will make sure, that both Alice and Bob have a current signature on the same digest for any tagged trusted-thing image you deploy. If one of their signatures is missing or if they point to different digests for the same tag, Connaisseur will block the deployment. In the meantime, for all other images any delegated individual key, repository key or the root key will be accepted. The list you add in the delegations field can contain any number of delegations. Just remember that it is a logical AND , so all delegations will have to verify correctly. Another potential use-case is to use delegations to sign an image in various stages of your CI, i.e. requiring the signature of your linter, your security scanner and your software lisence compliance check.","title":"Image Policy Example"},{"location":"validators/notaryv1/#limits","text":"Q: Is a logical OR possible? Are there n-of-m thresholds? Is there insert-arbitrary-logical-function ? A: No, currently not. If you have the use-case, feel free to hit us up , open an issue or open a pull request. Q: Will delegations work with Cosign or Notary v2? A: Currently, Cosign does not natively support delegations, so Connaisseur doesn't either. For Notary v2, it looks like they will support delegations and so Connaisseur will most likely support them as in Notary v1.","title":"Limits"},{"location":"validators/notaryv2/","text":"Notary V2 \u2693\ufe0e TBD - Notary V2 has not yet been integrated with Connaisseur.","title":"Notary V2"},{"location":"validators/notaryv2/#notary-v2","text":"TBD - Notary V2 has not yet been integrated with Connaisseur.","title":"Notary V2"},{"location":"validators/sigstore_cosign/","text":"SigStore / Cosign \u2693\ufe0e Sigstore is a Linux Foundation project that aims to provide public software signing and transparency to improve open source supply chain security. As part of the Sigstore project, Cosign allows seamless container signing, verification and storage. You can read more about it here . Connaisseur currently supports the elementary function of verifying Cosign-generated signatures against the locally created corresponding public keys. We plan to expose further features of Cosign and Sigstore in upcoming releases, so stay tuned! Sigstore and Cosign are currently in pre-release state and under heavy development and so is our support for them. We therefore consider this an experimental feature that might unstable over time. As such, it is not part of our semantic versioning guarantees and we take the liberty to adjust or remove it with any version at any time without incrementing MAJOR or MINOR. Basic usage \u2693\ufe0e Getting started with Cosign is very well described in the docs . Please check there for detailed instructions. The currently supported version can be found in our Makefile . In short: After installation, a keypair is generated via: cosign generate-key-pair You will be prompted to set a password, after which a private ( cosign.key ) and public ( cosign.pub ) key are created. You can then use Cosign to sign a container image using: # Here, $IMAGE is REPOSITORY/IMAGE_NAME:TAG cosign sign -key cosign.key $IMAGE The created signature can be verfied via: cosign verify -key cosign.pub $IMAGE To use Connaisseur with Cosign, configure a validator in helm/values.yaml with the generated public key ( cosign.pub ) as a trust root. The entry in .validators should look something like this (make sure to add your own public key to trust root default ): - name : customvalidator type : cosign trust_roots : - name : default key : | # YOUR KEY BELOW -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA== -----END PUBLIC KEY----- In .policy , add a pattern to match your public key to your own repository: - pattern : \"docker.io/securesystemsengineering/testimage:co*\" # YOUR REPOSITORY validator : customvalidator After installation, you are ready to verify your images against your public key: helm install connaisseur helm --atomic --create-namespace --namespace connaisseur A quick guide for installation and testing is available in getting started . In case you just use the default values for the validator and image policy given above, you are able to validate our testimage docker.io/securesystemsengineering/testimage:co-signed : kubectl run signed --image = docker.io/securesystemsengineering/testimage:co-signed And compare this to the unsigned image: kubectl run unsigned --image = docker.io/securesystemsengineering/testimage:co-unsigned Or signed with a different key: kubectl run unsigned --image = docker.io/securesystemsengineering/testimage:co-signed-alt Configuration options \u2693\ufe0e .validators[*] in helm/values.yaml supports the following keys for cosign (refer to basics for more information on default keys): key default required description name see basics type cosign ; the validator type must be set to cosign host NOT YET IMPLEMENTED auth Provide authentication credentials for private registries. auth.secret_name Specify a Kubernetes secret that contains dockerconfigjson for registry authentication. See additional notes below . trust_roots[*].name see basics trust_roots[*].key ECDSA public key from cosign.pub file Additional notes \u2693\ufe0e Authentication \u2693\ufe0e When using a private registry for images and signature data, the credentials need to be provided to Connaisseur. This is done by creating a dockerconfigjson Kubernetes secret and passing the secret name to Connaisseur as auth.secret_name . The secret can for example be created directly from your local config.json (for docker this resides in ~/.docker/config.json ): kubectl create secret generic my-secret \\ --from-file = .dockerconfigjson = path/to/config.json \\ --type = kubernetes.io/dockerconfigjson -n connaisseur In the above case, in the secret name in Connaisseur configuration would be secret_name: my-secret . It is possible to provide one Kubernetes secret with a config.json for authentication to multiple private registries and referencing this in multiple validators. Verification against Transparency Log \u2693\ufe0e Connaisseur already verifies signatures against the transparency log. However, optional enforcement of transparency log is only planned in upcoming releases. Keyless signatures \u2693\ufe0e Keyless signatures have not yet been implementd but are planned in upcoming releases.","title":"SigStore / Cosign"},{"location":"validators/sigstore_cosign/#sigstore-cosign","text":"Sigstore is a Linux Foundation project that aims to provide public software signing and transparency to improve open source supply chain security. As part of the Sigstore project, Cosign allows seamless container signing, verification and storage. You can read more about it here . Connaisseur currently supports the elementary function of verifying Cosign-generated signatures against the locally created corresponding public keys. We plan to expose further features of Cosign and Sigstore in upcoming releases, so stay tuned! Sigstore and Cosign are currently in pre-release state and under heavy development and so is our support for them. We therefore consider this an experimental feature that might unstable over time. As such, it is not part of our semantic versioning guarantees and we take the liberty to adjust or remove it with any version at any time without incrementing MAJOR or MINOR.","title":"SigStore / Cosign"},{"location":"validators/sigstore_cosign/#basic-usage","text":"Getting started with Cosign is very well described in the docs . Please check there for detailed instructions. The currently supported version can be found in our Makefile . In short: After installation, a keypair is generated via: cosign generate-key-pair You will be prompted to set a password, after which a private ( cosign.key ) and public ( cosign.pub ) key are created. You can then use Cosign to sign a container image using: # Here, $IMAGE is REPOSITORY/IMAGE_NAME:TAG cosign sign -key cosign.key $IMAGE The created signature can be verfied via: cosign verify -key cosign.pub $IMAGE To use Connaisseur with Cosign, configure a validator in helm/values.yaml with the generated public key ( cosign.pub ) as a trust root. The entry in .validators should look something like this (make sure to add your own public key to trust root default ): - name : customvalidator type : cosign trust_roots : - name : default key : | # YOUR KEY BELOW -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEvtc/qpHtx7iUUj+rRHR99a8mnGni qiGkmUb9YpWWTS4YwlvwdmMDiGzcsHiDOYz6f88u2hCRF5GUCvyiZAKrsA== -----END PUBLIC KEY----- In .policy , add a pattern to match your public key to your own repository: - pattern : \"docker.io/securesystemsengineering/testimage:co*\" # YOUR REPOSITORY validator : customvalidator After installation, you are ready to verify your images against your public key: helm install connaisseur helm --atomic --create-namespace --namespace connaisseur A quick guide for installation and testing is available in getting started . In case you just use the default values for the validator and image policy given above, you are able to validate our testimage docker.io/securesystemsengineering/testimage:co-signed : kubectl run signed --image = docker.io/securesystemsengineering/testimage:co-signed And compare this to the unsigned image: kubectl run unsigned --image = docker.io/securesystemsengineering/testimage:co-unsigned Or signed with a different key: kubectl run unsigned --image = docker.io/securesystemsengineering/testimage:co-signed-alt","title":"Basic usage"},{"location":"validators/sigstore_cosign/#configuration-options","text":".validators[*] in helm/values.yaml supports the following keys for cosign (refer to basics for more information on default keys): key default required description name see basics type cosign ; the validator type must be set to cosign host NOT YET IMPLEMENTED auth Provide authentication credentials for private registries. auth.secret_name Specify a Kubernetes secret that contains dockerconfigjson for registry authentication. See additional notes below . trust_roots[*].name see basics trust_roots[*].key ECDSA public key from cosign.pub file","title":"Configuration options"},{"location":"validators/sigstore_cosign/#additional-notes","text":"","title":"Additional notes"},{"location":"validators/sigstore_cosign/#authentication","text":"When using a private registry for images and signature data, the credentials need to be provided to Connaisseur. This is done by creating a dockerconfigjson Kubernetes secret and passing the secret name to Connaisseur as auth.secret_name . The secret can for example be created directly from your local config.json (for docker this resides in ~/.docker/config.json ): kubectl create secret generic my-secret \\ --from-file = .dockerconfigjson = path/to/config.json \\ --type = kubernetes.io/dockerconfigjson -n connaisseur In the above case, in the secret name in Connaisseur configuration would be secret_name: my-secret . It is possible to provide one Kubernetes secret with a config.json for authentication to multiple private registries and referencing this in multiple validators.","title":"Authentication"},{"location":"validators/sigstore_cosign/#verification-against-transparency-log","text":"Connaisseur already verifies signatures against the transparency log. However, optional enforcement of transparency log is only planned in upcoming releases.","title":"Verification against Transparency Log"},{"location":"validators/sigstore_cosign/#keyless-signatures","text":"Keyless signatures have not yet been implementd but are planned in upcoming releases.","title":"Keyless signatures"}]}