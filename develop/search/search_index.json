{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Connaisseur \u2693\ufe0e A Kubernetes admission controller to integrate container image signature verification and trust pinning into a cluster. What is Connaisseur? \u2693\ufe0e Connaisseur ensures integrity and provenance of container images in a Kubernetes cluster. To do so, it intercepts resource creation or update requests sent to the Kubernetes cluster, identifies all container images and verifies their signatures against pre-configured public keys. Based on the result, it either accepts or denies those requests. Connaisseur is developed under three core values: Security , Usability , Compatibility . It is built to be extendable and currently aims to support the following signing solutions: Notary V1 / Docker Content Trust Sigstore / Cosign (EXPERIMENTAL) Notary V2 (PLANNED) It provides several additional features: Detection Mode : warn but do not block invalid images Namespaced Validation : restrict validation to dedicated namespaces Alerting : send alerts based on verification result Feel free to reach out to us via GitHub Discussions ! Quick Start \u2693\ufe0e Getting started to verify image signatures is only a matter of minutes: Only try this out on a test cluster as deployments with unsigned images will be blocked. Connaisseur comes pre-configured with public keys for its own repository and Docker's official images (for a list of official images check here ). It can be fully configured via helm/values.yaml . For a quick start, clone the Connaisseur repository: git clone https://github.com/sse-secure-systems/connaisseur.git Next, install Connaisseur via Helm : helm install connaisseur helm --atomic --create-namespace --namespace connaisseur Once installation has finished, you are good to go. Successful verification can be tested via official Docker images like hello-world : kubectl run hello-world --image = docker.io/hello-world Or our signed testimage : kubectl run demo --image = docker.io/securesystemsengineering/testimage:signed Both will return pod/<name> created . However, when trying to deploy an unsigned image: kubectl run demo --image = docker.io/securesystemsengineering/testimage:unsigned Connaisseur returns an error (...) Unable to find signed digest (...) . Since the images above are signed using Docker Content Trust, you can inspect the trust data using docker trust inspect --pretty <image-name> . To uninstall Connaisseur use: helm uninstall connaisseur --namespace connaisseur To uninstall all components add the --purge flag. Congrats you just validated the first images in your cluster! To get started configuring and verifying your own images and signatures, please follow our full setup guide . How does it work? \u2693\ufe0e Integrity and provenance of container images deployed to a Kubernetes cluster can be ensured via digital signatures. On a very basic level, this requires two steps: Signing container images after building Verifying the image signatures before deployment Connaisseur aims to solve step two. This is achieved by implementing several validators , i.e. configurable signature verification modules for different signing schemes (e.g. Notary V1). While the detailed security considerations mainly depend on the applied scheme, Connaisseur in general verifies the signature over the container image content against a trust anchor (e.g. public key) and thus let's you ensure that images have not been tampered with (integrity) and come from a valid source (provenance). Trusted digests \u2693\ufe0e But what is actually verified? Container images can be referenced in two different ways based on their registry, repository, image name ( <registry>/<repository>/<image name> ) followed by either tag or digest: tag: docker.io/library/nginx: 1.20.1 digest: docker.io/library/nginx@ sha256:af9c...69ce While the tag is a mutable, human readable description, the digest is an immutable, inherent property of the image, namely the SHA256 hash of its content. This also means that a tag can correspond to multiple digests whereas digests are unique for each image. The container runtime (e.g. containerd) compares the image content against the received digest before spinning up the container (CHECK!!). As a result, Connaisseur just needs to make sure that only trusted digests (signed by a trusted entity) are passed to the container runtime. Depending on how an image for deployment is referenced, it will either attempt to translate the tag to a trusted digest or validate whether the digest is trusted. How the digest is signed in detail, where the signature is stored, what it is verfied against and how different image distribution and updating attacks are mitigated depends on the signature schemes. Mutating Admission controller \u2693\ufe0e How to validate images before deployment to a cluster? The Kubernetes API is the fundamental fabric behind the control plane. It allows operators and cluster components to communicate with each other and, for example, query, create, modify or delete Kubernetes resources. Each request passes through several phases such as authentication and authorization before it is persisted. Among those phases are two steps of admission control : mutating and validating admission. In those phases the API sends admission requests to configured webhooks (admission controllers) and receives admission responses (admit, deny, or modify). Connaisseur uses a mutating admission webhook, as requests are not only admitted or denied based on the validation result but might also require modification of contained images referenced by tags to trusted digests. The webhook is configured to only forward resource creation or update requests to the Connaisseur pods (SERVICE??) running inside the cluster, since only deployments of images to the cluster are relevant for signature verification. This allows Connaisseur to intercept requests before deployment and based on the validation: admit if all images are referenced by trusted digests (CHECK!) modify if all images can be translated to trusted digests deny if at least one of the requested images does not have a trusted digest Image Policy and Validators \u2693\ufe0e Compatibility \u2693\ufe0e Supported signature solutions and configuration options are documented under Validators . Connaisseur is expected to be compatible with most Kubernetes services. It has been successfully tested with: K3s \u2705 kind \u2705 MicroK8s \u2705 minikube \u2705 Amazon Elastic Kubernetes Service (EKS) \u2705 Azure Kubernetes Service (AKS) \u2705 Google Kubernetes Engine \u2705 SysEleven MetaKube \u2705 All registry interactions use the OCI Distribution Specification that is based on the Docker Registry HTTP API V2 which is the standard for all common image registries. For using Notary V1 as a signature solution, some registries provide the required Notary server attached to the registry with e.g. shared authentication. Connaisseur has been tested with the following Notary V1 supporting image registries: Docker Hub \u2705 Harbor \u2705 (check our configuration notes ) Azure Container Registry (ACR) \u2705 (check our configuration notes ) In case you identify any incompatibilities, please create an issue Development \u2693\ufe0e Connaisseur is open source and open development. We try to make major changes transparent via Architecture Decision Records (ADRs) and announce developments via GitHub Discussions . We hope to get as many direct contributions and insights from the community as possible to steer further development. Please refer to our contributing guide , create an issue or reach out to us via GitHub Discussions Resources \u2693\ufe0e Several resources are available to learn more about connaisseur and related topics: \" Container Image Signatures in Kubernetes \" - blog post (full introduction) \" Integrity of Docker images \" - talk at Berlin Crypto Meetup ( The Update Framework , Notary , Docker Content Trust & Connaisseur [live demo]) \" Verifying Container Image Signatures from an OCI Registry in Kubernetes \" - blog post (experimental support of SigStore/Cosign)","title":"Overview"},{"location":"#welcome-to-connaisseur","text":"A Kubernetes admission controller to integrate container image signature verification and trust pinning into a cluster.","title":"Welcome to Connaisseur"},{"location":"#what-is-connaisseur","text":"Connaisseur ensures integrity and provenance of container images in a Kubernetes cluster. To do so, it intercepts resource creation or update requests sent to the Kubernetes cluster, identifies all container images and verifies their signatures against pre-configured public keys. Based on the result, it either accepts or denies those requests. Connaisseur is developed under three core values: Security , Usability , Compatibility . It is built to be extendable and currently aims to support the following signing solutions: Notary V1 / Docker Content Trust Sigstore / Cosign (EXPERIMENTAL) Notary V2 (PLANNED) It provides several additional features: Detection Mode : warn but do not block invalid images Namespaced Validation : restrict validation to dedicated namespaces Alerting : send alerts based on verification result Feel free to reach out to us via GitHub Discussions !","title":"What is Connaisseur?"},{"location":"#quick-start","text":"Getting started to verify image signatures is only a matter of minutes: Only try this out on a test cluster as deployments with unsigned images will be blocked. Connaisseur comes pre-configured with public keys for its own repository and Docker's official images (for a list of official images check here ). It can be fully configured via helm/values.yaml . For a quick start, clone the Connaisseur repository: git clone https://github.com/sse-secure-systems/connaisseur.git Next, install Connaisseur via Helm : helm install connaisseur helm --atomic --create-namespace --namespace connaisseur Once installation has finished, you are good to go. Successful verification can be tested via official Docker images like hello-world : kubectl run hello-world --image = docker.io/hello-world Or our signed testimage : kubectl run demo --image = docker.io/securesystemsengineering/testimage:signed Both will return pod/<name> created . However, when trying to deploy an unsigned image: kubectl run demo --image = docker.io/securesystemsengineering/testimage:unsigned Connaisseur returns an error (...) Unable to find signed digest (...) . Since the images above are signed using Docker Content Trust, you can inspect the trust data using docker trust inspect --pretty <image-name> . To uninstall Connaisseur use: helm uninstall connaisseur --namespace connaisseur To uninstall all components add the --purge flag. Congrats you just validated the first images in your cluster! To get started configuring and verifying your own images and signatures, please follow our full setup guide .","title":"Quick Start"},{"location":"#how-does-it-work","text":"Integrity and provenance of container images deployed to a Kubernetes cluster can be ensured via digital signatures. On a very basic level, this requires two steps: Signing container images after building Verifying the image signatures before deployment Connaisseur aims to solve step two. This is achieved by implementing several validators , i.e. configurable signature verification modules for different signing schemes (e.g. Notary V1). While the detailed security considerations mainly depend on the applied scheme, Connaisseur in general verifies the signature over the container image content against a trust anchor (e.g. public key) and thus let's you ensure that images have not been tampered with (integrity) and come from a valid source (provenance).","title":"How does it work?"},{"location":"#trusted-digests","text":"But what is actually verified? Container images can be referenced in two different ways based on their registry, repository, image name ( <registry>/<repository>/<image name> ) followed by either tag or digest: tag: docker.io/library/nginx: 1.20.1 digest: docker.io/library/nginx@ sha256:af9c...69ce While the tag is a mutable, human readable description, the digest is an immutable, inherent property of the image, namely the SHA256 hash of its content. This also means that a tag can correspond to multiple digests whereas digests are unique for each image. The container runtime (e.g. containerd) compares the image content against the received digest before spinning up the container (CHECK!!). As a result, Connaisseur just needs to make sure that only trusted digests (signed by a trusted entity) are passed to the container runtime. Depending on how an image for deployment is referenced, it will either attempt to translate the tag to a trusted digest or validate whether the digest is trusted. How the digest is signed in detail, where the signature is stored, what it is verfied against and how different image distribution and updating attacks are mitigated depends on the signature schemes.","title":"Trusted digests"},{"location":"#mutating-admission-controller","text":"How to validate images before deployment to a cluster? The Kubernetes API is the fundamental fabric behind the control plane. It allows operators and cluster components to communicate with each other and, for example, query, create, modify or delete Kubernetes resources. Each request passes through several phases such as authentication and authorization before it is persisted. Among those phases are two steps of admission control : mutating and validating admission. In those phases the API sends admission requests to configured webhooks (admission controllers) and receives admission responses (admit, deny, or modify). Connaisseur uses a mutating admission webhook, as requests are not only admitted or denied based on the validation result but might also require modification of contained images referenced by tags to trusted digests. The webhook is configured to only forward resource creation or update requests to the Connaisseur pods (SERVICE??) running inside the cluster, since only deployments of images to the cluster are relevant for signature verification. This allows Connaisseur to intercept requests before deployment and based on the validation: admit if all images are referenced by trusted digests (CHECK!) modify if all images can be translated to trusted digests deny if at least one of the requested images does not have a trusted digest","title":"Mutating Admission controller"},{"location":"#image-policy-and-validators","text":"","title":"Image Policy and Validators"},{"location":"#compatibility","text":"Supported signature solutions and configuration options are documented under Validators . Connaisseur is expected to be compatible with most Kubernetes services. It has been successfully tested with: K3s \u2705 kind \u2705 MicroK8s \u2705 minikube \u2705 Amazon Elastic Kubernetes Service (EKS) \u2705 Azure Kubernetes Service (AKS) \u2705 Google Kubernetes Engine \u2705 SysEleven MetaKube \u2705 All registry interactions use the OCI Distribution Specification that is based on the Docker Registry HTTP API V2 which is the standard for all common image registries. For using Notary V1 as a signature solution, some registries provide the required Notary server attached to the registry with e.g. shared authentication. Connaisseur has been tested with the following Notary V1 supporting image registries: Docker Hub \u2705 Harbor \u2705 (check our configuration notes ) Azure Container Registry (ACR) \u2705 (check our configuration notes ) In case you identify any incompatibilities, please create an issue","title":"Compatibility"},{"location":"#development","text":"Connaisseur is open source and open development. We try to make major changes transparent via Architecture Decision Records (ADRs) and announce developments via GitHub Discussions . We hope to get as many direct contributions and insights from the community as possible to steer further development. Please refer to our contributing guide , create an issue or reach out to us via GitHub Discussions","title":"Development"},{"location":"#resources","text":"Several resources are available to learn more about connaisseur and related topics: \" Container Image Signatures in Kubernetes \" - blog post (full introduction) \" Integrity of Docker images \" - talk at Berlin Crypto Meetup ( The Update Framework , Notary , Docker Content Trust & Connaisseur [live demo]) \" Verifying Container Image Signatures from an OCI Registry in Kubernetes \" - blog post (experimental support of SigStore/Cosign)","title":"Resources"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct \u2693\ufe0e Our Pledge \u2693\ufe0e In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u2693\ufe0e Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u2693\ufe0e Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u2693\ufe0e This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u2693\ufe0e Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at connaisseur@securesystems.dev . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u2693\ufe0e This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at connaisseur@securesystems.dev . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing \u2693\ufe0e We hope to steer development of Connaisseur from demand of the community and are excited about direct contributions to improve the tool! The following guide is meant to help you get started with contributing to Connaisseur. In case of questions or feedback, feel free to reach out to us . We are committed to positive interactions between all contributors of the project. To ensure this, please follow the Code of Conduct in all communications. Contents \u2693\ufe0e Discuss Problems, Raise Bugs and Propose Feature Ideas Contribute to Source Code Setup the Environment Test Changes Signed Commits and Pull Requests Semantic and Conventional Commits Enjoy! Discuss Problems, Raise Bugs and Propose Feature Ideas \u2693\ufe0e We are happy you made it here! In case you want to share your feedback, need support, want to discuss issues from using Connaisseur in your own projects, have ideas for new features or just want to connect with us, please reach out via GitHub Discussions . If you want to raise any bugs you found or make a feature request, feel free to create an issue with an informative title and description. While issues are a great way to discuss problems, bugs and new features, a direct proposal via a pull request can sometimes say more than a thousand words. So be bold and contribute to the code as described in the next section ! In case you require a more private communication, you can reach us via connaisseur@securesystems.dev . Contribute to Source Code \u2693\ufe0e The following steps will help you make code contributions to Connaisseur and ensure good code quality and workflow. This includes the following steps: Setup your environment : Setup up your local environment to best interact with the code. Further information is given below . Make atomic changes : Changes should be atomic. As such, pull requests should contain only few commits, and each commit should only fix one issue or implement one feature, with a concise commit message. Test your changes : Test any changes locally for code quality and functionality and add new tests for any additional code. How to test is described below . Create semantic, conventional and signed commits : Any commits should follow a simple semantic convention to help structure the work on Connaisseur. The convention is described below . For security reasons and since integrity is at the core of this project, code merged into master must be signed. How we achieve this is described below . Create Pull Request : We consider code review central to quality and security of code. Therefore, a pull request (PR) should be created for each contribution. It will be reviewed, and potential improvements may be discussed within the PR. After approval, changes will be merged into the master branch. Setup the Environment \u2693\ufe0e To start contributing, you will need to setup your local environment. First step is to get the source code by cloning this repository: git clone git@github.com:sse-secure-systems/connaisseur.git In order to review the effects of your changes, you should create your own Kubernetes cluster and install Connaisseur. This is described in the setup guide . A simple starting point may be a minikube cluster with e.g. a Docker Hub repository for maintaining your test images and trust data. In case you make changes to the Connaisseur Docker image itself or code for that matter, you need to re-build the image and install it locally for testing. This requires a few steps: In helm/values.yaml , set imagePullPolicy to IfNotPresent . Configure your local environment to use the Kubernetes Docker daemon. In minikube, this can be done via eval (minikube docker-env) . Build the Connaisseur Docker image via make docker . Install Connaisseur as usual via make install . Test Changes \u2693\ufe0e Tests and linting are important to ensure code quality, functionality and security. We therefore aim to keep the code coverage high. We are running several automated tests in the CI pipeline . Application code is tested via pytest and linted via pylint . When making changes to the application code, please directly provide tests for your changes. We recommend using black for autoformatting to simplify linting and reduce review effort. It can be installed via: pip3 install black To autoformat the code: black <path-to-repository>/connaisseur Changes can also be tested locally. We recommend the following approach for running pytest in a docker container: docker run -it --rm -v <path-to-repository>:/data --entrypoint=ash python:alpine cd data pip3 install -r requirements_dev.txt pip3 install . cd connaisseur pytest --cov=connaisseur --cov-report=xml tests/ This helps identify bugs in changes before pushing. INFO We believe that testing should not only ensure functionality, but also aim to test for expected security issues like injections and appreciate if security tests are added with new functionalities. Besides the unit testing and before any PR can be merged, an integration test is carried out whereby: - Connaisseur is successfully installed in a test cluster - a non-signed image is deployed to the cluster and denied - an image signed with an unrelated key is denied - a signed image is deployed to the cluster and passed - Connaisseur is successfully uninstalled You can also run this integration test on a local cluster after setting the necessary environment variables. If you are changing documentation, you can simply inspect your changes locally via: docker run --rm -it -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material Signed Commits and Pull Requests \u2693\ufe0e All changes to the master branch must be signed which is enforced via branch protection . This can be achieved by only fast-forwarding signed commits or signing of merge commits by a contributor. Consequently, while we appreciate signed commits in PRs, we do not require it. A general introduction into signing commits can for example be found in the With Blue Ink blog . For details on setting everything up for GitHub, please follow the steps in the Documentation . Once you have generated your local GPG key, added it to your GitHub account and informed Git about it, you are setup to create signed commits. We recommend to configure Git to sign commits by default via: git config commit.gpgsign true This avoids forgetting to use the -S flag when committing changes. In case it happens anyways, you can always rebase to sign earlier commits: git rebase -i master You can then mark all commits that need to be signed as edit and sign them without any other changes via: git commit -S --amend --no-edit Finally, you force push to overwrite the unsigned commits via git push -f . Semantic and Conventional Commits \u2693\ufe0e For Connaisseur, we want to use semantic and conventional commits to ensure good readability of code changes. A good introduction to the topic can be found in this blog post . Commit messages should consist of header, body and footer. Such a commit message takes the following form: git commit -m \"<header>\" -m \"<body>\" -m \"<footer>\" The three parts should consist of the following: - header : Comprises of a commit type (common types are described below) and a concise description of the actual change, e.g. fix: extend registry validation regex to custom ports . - body (optional): Contains information on the motivation behind the change and considerations for the resolution, The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number. . - footer (optional): Used to reference PRs, issues or contributors and mark consequences such as breaking changes, e.g. Fix #<issue-number> We want to use the following common types in the header: - build : changes to development and building - ci : CI related changes - docs : changes in the documentation - feat : adding of new features - fix : fixing an issue or bug - refactor : adjustment of code base to improve code quality or performance but not adding a feature or fixing a bug - test : testing related changes A complete commit message could therefore look as follows: git commit - m \"fix: extend registry validation regex to custom ports\" - m \"The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number.\" - m \"Fix #3\" Enjoy! \u2693\ufe0e Please be bold and contribute!","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We hope to steer development of Connaisseur from demand of the community and are excited about direct contributions to improve the tool! The following guide is meant to help you get started with contributing to Connaisseur. In case of questions or feedback, feel free to reach out to us . We are committed to positive interactions between all contributors of the project. To ensure this, please follow the Code of Conduct in all communications.","title":"Contributing"},{"location":"CONTRIBUTING/#contents","text":"Discuss Problems, Raise Bugs and Propose Feature Ideas Contribute to Source Code Setup the Environment Test Changes Signed Commits and Pull Requests Semantic and Conventional Commits Enjoy!","title":"Contents"},{"location":"CONTRIBUTING/#discuss-problems-raise-bugs-and-propose-feature-ideas","text":"We are happy you made it here! In case you want to share your feedback, need support, want to discuss issues from using Connaisseur in your own projects, have ideas for new features or just want to connect with us, please reach out via GitHub Discussions . If you want to raise any bugs you found or make a feature request, feel free to create an issue with an informative title and description. While issues are a great way to discuss problems, bugs and new features, a direct proposal via a pull request can sometimes say more than a thousand words. So be bold and contribute to the code as described in the next section ! In case you require a more private communication, you can reach us via connaisseur@securesystems.dev .","title":"Discuss Problems, Raise Bugs and Propose Feature Ideas"},{"location":"CONTRIBUTING/#contribute-to-source-code","text":"The following steps will help you make code contributions to Connaisseur and ensure good code quality and workflow. This includes the following steps: Setup your environment : Setup up your local environment to best interact with the code. Further information is given below . Make atomic changes : Changes should be atomic. As such, pull requests should contain only few commits, and each commit should only fix one issue or implement one feature, with a concise commit message. Test your changes : Test any changes locally for code quality and functionality and add new tests for any additional code. How to test is described below . Create semantic, conventional and signed commits : Any commits should follow a simple semantic convention to help structure the work on Connaisseur. The convention is described below . For security reasons and since integrity is at the core of this project, code merged into master must be signed. How we achieve this is described below . Create Pull Request : We consider code review central to quality and security of code. Therefore, a pull request (PR) should be created for each contribution. It will be reviewed, and potential improvements may be discussed within the PR. After approval, changes will be merged into the master branch.","title":"Contribute to Source Code"},{"location":"CONTRIBUTING/#setup-the-environment","text":"To start contributing, you will need to setup your local environment. First step is to get the source code by cloning this repository: git clone git@github.com:sse-secure-systems/connaisseur.git In order to review the effects of your changes, you should create your own Kubernetes cluster and install Connaisseur. This is described in the setup guide . A simple starting point may be a minikube cluster with e.g. a Docker Hub repository for maintaining your test images and trust data. In case you make changes to the Connaisseur Docker image itself or code for that matter, you need to re-build the image and install it locally for testing. This requires a few steps: In helm/values.yaml , set imagePullPolicy to IfNotPresent . Configure your local environment to use the Kubernetes Docker daemon. In minikube, this can be done via eval (minikube docker-env) . Build the Connaisseur Docker image via make docker . Install Connaisseur as usual via make install .","title":"Setup the Environment"},{"location":"CONTRIBUTING/#test-changes","text":"Tests and linting are important to ensure code quality, functionality and security. We therefore aim to keep the code coverage high. We are running several automated tests in the CI pipeline . Application code is tested via pytest and linted via pylint . When making changes to the application code, please directly provide tests for your changes. We recommend using black for autoformatting to simplify linting and reduce review effort. It can be installed via: pip3 install black To autoformat the code: black <path-to-repository>/connaisseur Changes can also be tested locally. We recommend the following approach for running pytest in a docker container: docker run -it --rm -v <path-to-repository>:/data --entrypoint=ash python:alpine cd data pip3 install -r requirements_dev.txt pip3 install . cd connaisseur pytest --cov=connaisseur --cov-report=xml tests/ This helps identify bugs in changes before pushing. INFO We believe that testing should not only ensure functionality, but also aim to test for expected security issues like injections and appreciate if security tests are added with new functionalities. Besides the unit testing and before any PR can be merged, an integration test is carried out whereby: - Connaisseur is successfully installed in a test cluster - a non-signed image is deployed to the cluster and denied - an image signed with an unrelated key is denied - a signed image is deployed to the cluster and passed - Connaisseur is successfully uninstalled You can also run this integration test on a local cluster after setting the necessary environment variables. If you are changing documentation, you can simply inspect your changes locally via: docker run --rm -it -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material","title":"Test Changes"},{"location":"CONTRIBUTING/#signed-commits-and-pull-requests","text":"All changes to the master branch must be signed which is enforced via branch protection . This can be achieved by only fast-forwarding signed commits or signing of merge commits by a contributor. Consequently, while we appreciate signed commits in PRs, we do not require it. A general introduction into signing commits can for example be found in the With Blue Ink blog . For details on setting everything up for GitHub, please follow the steps in the Documentation . Once you have generated your local GPG key, added it to your GitHub account and informed Git about it, you are setup to create signed commits. We recommend to configure Git to sign commits by default via: git config commit.gpgsign true This avoids forgetting to use the -S flag when committing changes. In case it happens anyways, you can always rebase to sign earlier commits: git rebase -i master You can then mark all commits that need to be signed as edit and sign them without any other changes via: git commit -S --amend --no-edit Finally, you force push to overwrite the unsigned commits via git push -f .","title":"Signed Commits and Pull Requests"},{"location":"CONTRIBUTING/#semantic-and-conventional-commits","text":"For Connaisseur, we want to use semantic and conventional commits to ensure good readability of code changes. A good introduction to the topic can be found in this blog post . Commit messages should consist of header, body and footer. Such a commit message takes the following form: git commit -m \"<header>\" -m \"<body>\" -m \"<footer>\" The three parts should consist of the following: - header : Comprises of a commit type (common types are described below) and a concise description of the actual change, e.g. fix: extend registry validation regex to custom ports . - body (optional): Contains information on the motivation behind the change and considerations for the resolution, The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number. . - footer (optional): Used to reference PRs, issues or contributors and mark consequences such as breaking changes, e.g. Fix #<issue-number> We want to use the following common types in the header: - build : changes to development and building - ci : CI related changes - docs : changes in the documentation - feat : adding of new features - fix : fixing an issue or bug - refactor : adjustment of code base to improve code quality or performance but not adding a feature or fixing a bug - test : testing related changes A complete commit message could therefore look as follows: git commit - m \"fix: extend registry validation regex to custom ports\" - m \"The current regex used for validation of the image name does not allow using non-default ports for the image repository name. The regex is extended to optionally provide a port number.\" - m \"Fix #3\"","title":"Semantic and Conventional Commits"},{"location":"CONTRIBUTING/#enjoy","text":"Please be bold and contribute!","title":"Enjoy!"},{"location":"RELEASING/","text":"How to release like a real pro \u2693\ufe0e Releasing a new version of Connaisseur includes the following steps: adding a new version tag creating a changelog from commit messages creating a PR from develop (new version) to master (current version) pushing a new version of the Connaisseur image to Dockerhub merging in the PR & push tag creating release page shoot some trouble Adding new Tag \u2693\ufe0e Before adding the new tag, make sure the Connaisseur version is updated in the helm/values.yaml and applies the semantic versioning guidelines: fixes increment PATCH version, non-breaking features increment MINOR version, breaking features increment MAJOR version. Then add the tag (on develop branch) with git tag v<new-conny-version> (e.g. git tag v1.4.6 ). Creating Changelog \u2693\ufe0e A changelog text, including all new commits from one to another version, can be automatically generated using the scrips/changelogger.py script. You have to update the two ref1 and ref2 variables in the main method with the the old and new git tags. So if you e.g. want to release a new version from v1.4.5 to v1.4.6 , then you have to set ref1 = \"v1.4.5\" and ref2 = \"v1.4.6\" . Then simply run python scripts/changelogger.py > CHANGELOG.md , storing the changelog in a new file CHANGELOG.md (we won't keep this file, it's just for convenient storing purpose). This file will include all new commits, categorized by their type (e.g. fix, feat, docs, etc.), but may include some mistakes so take a manual look if everything looks in order. Things to look out for: multiple headings for the same category broken pull request links None appended on end of line Creating PR \u2693\ufe0e Create a PR from develop to master , putting the changelog text as description and wait for someone to approve it. Push new Connaisseur Image \u2693\ufe0e When the PR is approved and ready to be merged, first push the new Connaisseur image to Dockerhub, as it will be used in the release pipeline. Run make docker to build the new version of the docker image and then DOCKER_CONTENT_TRUST=1 docker image push securesystemsengineering/connaisseur:<new-version> to push and sign it. You'll obviously need the right private key and passphrase for doing so. You also need to be in the list of valid signers for Connaisseur. If not already (you can check with docker trust inspect securesystemsengineering/connaisseur --pretty ) you'll need to contact Philipp Belitz . Merge PR \u2693\ufe0e Run git checkout master to switch to the master branch and then run git merge develop to merge develop in. Then run git push and git push --tags to publish all changes and the new tag. Create Release Page \u2693\ufe0e Finally a release on Github should be created. Go to the Connaisseur releases page , then click Draft a new release . There you have to enter the new tag version, a title (usually Version <new-version> ) and the changelog text as description. Then click Publish release and you're done! (You can delete the CHANGELOG.md file now. Go and do it.) Shoot Trouble \u2693\ufe0e Be aware that this isn't a completely fleshed out, highly available, hyper scalable and fully automated workflow, backed up by state-of-the-art blockchain technology and 24/7 incident response team coverage with global dominance! Not yet at least. For now things will probably break, so make sure that in the end everything looks to be in order and the new release can be seen on the Github page, tagged with Latest release and pointing to the correct version of Connaisseur. Good Luck!","title":"How to release like a real pro"},{"location":"RELEASING/#how-to-release-like-a-real-pro","text":"Releasing a new version of Connaisseur includes the following steps: adding a new version tag creating a changelog from commit messages creating a PR from develop (new version) to master (current version) pushing a new version of the Connaisseur image to Dockerhub merging in the PR & push tag creating release page shoot some trouble","title":"How to release like a real pro"},{"location":"RELEASING/#adding-new-tag","text":"Before adding the new tag, make sure the Connaisseur version is updated in the helm/values.yaml and applies the semantic versioning guidelines: fixes increment PATCH version, non-breaking features increment MINOR version, breaking features increment MAJOR version. Then add the tag (on develop branch) with git tag v<new-conny-version> (e.g. git tag v1.4.6 ).","title":"Adding new Tag"},{"location":"RELEASING/#creating-changelog","text":"A changelog text, including all new commits from one to another version, can be automatically generated using the scrips/changelogger.py script. You have to update the two ref1 and ref2 variables in the main method with the the old and new git tags. So if you e.g. want to release a new version from v1.4.5 to v1.4.6 , then you have to set ref1 = \"v1.4.5\" and ref2 = \"v1.4.6\" . Then simply run python scripts/changelogger.py > CHANGELOG.md , storing the changelog in a new file CHANGELOG.md (we won't keep this file, it's just for convenient storing purpose). This file will include all new commits, categorized by their type (e.g. fix, feat, docs, etc.), but may include some mistakes so take a manual look if everything looks in order. Things to look out for: multiple headings for the same category broken pull request links None appended on end of line","title":"Creating Changelog"},{"location":"RELEASING/#creating-pr","text":"Create a PR from develop to master , putting the changelog text as description and wait for someone to approve it.","title":"Creating PR"},{"location":"RELEASING/#push-new-connaisseur-image","text":"When the PR is approved and ready to be merged, first push the new Connaisseur image to Dockerhub, as it will be used in the release pipeline. Run make docker to build the new version of the docker image and then DOCKER_CONTENT_TRUST=1 docker image push securesystemsengineering/connaisseur:<new-version> to push and sign it. You'll obviously need the right private key and passphrase for doing so. You also need to be in the list of valid signers for Connaisseur. If not already (you can check with docker trust inspect securesystemsengineering/connaisseur --pretty ) you'll need to contact Philipp Belitz .","title":"Push new Connaisseur Image"},{"location":"RELEASING/#merge-pr","text":"Run git checkout master to switch to the master branch and then run git merge develop to merge develop in. Then run git push and git push --tags to publish all changes and the new tag.","title":"Merge PR"},{"location":"RELEASING/#create-release-page","text":"Finally a release on Github should be created. Go to the Connaisseur releases page , then click Draft a new release . There you have to enter the new tag version, a title (usually Version <new-version> ) and the changelog text as description. Then click Publish release and you're done! (You can delete the CHANGELOG.md file now. Go and do it.)","title":"Create Release Page"},{"location":"RELEASING/#shoot-trouble","text":"Be aware that this isn't a completely fleshed out, highly available, hyper scalable and fully automated workflow, backed up by state-of-the-art blockchain technology and 24/7 incident response team coverage with global dominance! Not yet at least. For now things will probably break, so make sure that in the end everything looks to be in order and the new release can be seen on the Github page, tagged with Latest release and pointing to the correct version of Connaisseur. Good Luck!","title":"Shoot Trouble"},{"location":"SECURITY/","text":"Security Policy \u2693\ufe0e Supported Versions \u2693\ufe0e The last known significant security vulnerability was in version 1.3.0 (Connaisseur not validating initContainers). However, since both Python packages and OS packages in the Connaisseur image may become known to be vulnerable over time, we suggest either frequently rebuilding the Connaisseur image from source yourself or updating to the latest Connaisseur image. We strictly stick to semantic versioning, so unless the major version changes, updating Conaisseur should never brake your installation. Reporting a Vulnerability \u2693\ufe0e We are very grateful for reports on vulnerabilities discovered in the project, specifically as it is intended to increase security for the community. We aim to investigate and fix these as soon as possible. Please submit vulnerabilities to connaisseur@securesystems.dev .","title":"Security Policy"},{"location":"SECURITY/#security-policy","text":"","title":"Security Policy"},{"location":"SECURITY/#supported-versions","text":"The last known significant security vulnerability was in version 1.3.0 (Connaisseur not validating initContainers). However, since both Python packages and OS packages in the Connaisseur image may become known to be vulnerable over time, we suggest either frequently rebuilding the Connaisseur image from source yourself or updating to the latest Connaisseur image. We strictly stick to semantic versioning, so unless the major version changes, updating Conaisseur should never brake your installation.","title":"Supported Versions"},{"location":"SECURITY/#reporting-a-vulnerability","text":"We are very grateful for reports on vulnerabilities discovered in the project, specifically as it is intended to increase security for the community. We aim to investigate and fix these as soon as possible. Please submit vulnerabilities to connaisseur@securesystems.dev .","title":"Reporting a Vulnerability"},{"location":"core_concepts/","text":"Core Concepts \u2693\ufe0e description of core concepts (image policy, validators), scope, ...","title":"Core Concepts"},{"location":"core_concepts/#core-concepts","text":"description of core concepts (image policy, validators), scope, ...","title":"Core Concepts"},{"location":"getting_started/","text":"Getting Started \u2693\ufe0e getting started guide","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"getting started guide","title":"Getting Started"},{"location":"threat_model/","text":"Threat Model \u2693\ufe0e description of threat model and mitigations","title":"Threat Model"},{"location":"threat_model/#threat-model","text":"description of threat model and mitigations","title":"Threat Model"},{"location":"validation/","text":"Validation \u2693\ufe0e This document describes the image validation process as employed by Connaisseur. Since Connaisseur is a Kubernetes AdmissionController it operates on requests and has these requests, the Kubernetes API, the registry and the notary to work with. First off, Connaisseur does some light checks on the semantics of the request it receives, e.g. whether it is capable of using the API version of the admission request. Then it parses images to deploy from the request. Connaisseur does this for Pods, Jobs, CronJobs, Deployments, DaemonSets, StatefulSets, ReplicaSets and ReplicationControllers. These images are then validated as follows: If the image is deployed as part of an already deployed object (i.e. a Pod gets deployed as a child of a Deployment and the Deployment was already validated), it will be admitted. This is done as the parent (and thus the child) might have been mutated, which could lead to duplicate validation or rule mismatch. For example, given a Deployment which contains Pods with image:tag that gets mutated to contain Pods with image@sha256:digest . Then a) the Pod would not need another validation as the image was validated during the admittance of the Deployment and b) if there exists a specific rule for image:tag and another for image:* , then after mutating the Deployment, the Pod would be falsely validated against image:* instead of image:tag . To ensure the child resource is legit in this case, the parent resource is requested via the Kubernetes API and only those images it lists are accepted. The best matching rule from the image policy is looked up and if verify: false , the image is admitted. Trust data of the image is consulted as described in the next section. Validation of trust data \u2693\ufe0e Validation of trust data on a high level boils down to two steps: Get all trusted (i.e. signed) image digests related to the tag or digest of the image. If there is exactly one, admit the image. Regarding the latter step, rejection upon no matching trusted digests is obvious. However, Connaisseur also needs to reject the image if there is more than one trusted digest, since at this point in time Connaisseur doesn't have the ability to distinguish between the right and wrong trusted digest. This only occurs in some edge cases, but nonetheless has to be addressed. Let's now focus on the integral part of Connaisseur that is how to get all trusted digests for an image:tag or image:digest combination: Connaisseur looks up trust data of the image in the root , snapshot , timestamp and targets files by querying the API of the notary server. Trust data syntax is validated against their known schemas (for an introduction to the trust data formats see the presentation of our colleague Philipp Belitz ). Then, the files' signatures are validated against the pinned root key for the root file and against the respective keys validated in previous steps for later files. Connaisseur further gathers trust data of potential delegations linked in the targets file. At this point, Connaisseur is left with a set of potentially disjoint or overlapping sets of trust data. Connaisseur filters the trust data for digests that actually relate to the image under validation. If the image policy rule that governs the image to be validated does contain a delegations field, Connaisseur makes sure that all delegations' sets of trust data do contain an entry for the image. If that is not the case, the request is rejected. Subsequently, Connaisseur builds a set over the union of the digests and proceeds with step 2., i.e. accepeting if the size of the resulting set equals 1.","title":"Validation"},{"location":"validation/#validation","text":"This document describes the image validation process as employed by Connaisseur. Since Connaisseur is a Kubernetes AdmissionController it operates on requests and has these requests, the Kubernetes API, the registry and the notary to work with. First off, Connaisseur does some light checks on the semantics of the request it receives, e.g. whether it is capable of using the API version of the admission request. Then it parses images to deploy from the request. Connaisseur does this for Pods, Jobs, CronJobs, Deployments, DaemonSets, StatefulSets, ReplicaSets and ReplicationControllers. These images are then validated as follows: If the image is deployed as part of an already deployed object (i.e. a Pod gets deployed as a child of a Deployment and the Deployment was already validated), it will be admitted. This is done as the parent (and thus the child) might have been mutated, which could lead to duplicate validation or rule mismatch. For example, given a Deployment which contains Pods with image:tag that gets mutated to contain Pods with image@sha256:digest . Then a) the Pod would not need another validation as the image was validated during the admittance of the Deployment and b) if there exists a specific rule for image:tag and another for image:* , then after mutating the Deployment, the Pod would be falsely validated against image:* instead of image:tag . To ensure the child resource is legit in this case, the parent resource is requested via the Kubernetes API and only those images it lists are accepted. The best matching rule from the image policy is looked up and if verify: false , the image is admitted. Trust data of the image is consulted as described in the next section.","title":"Validation"},{"location":"validation/#validation-of-trust-data","text":"Validation of trust data on a high level boils down to two steps: Get all trusted (i.e. signed) image digests related to the tag or digest of the image. If there is exactly one, admit the image. Regarding the latter step, rejection upon no matching trusted digests is obvious. However, Connaisseur also needs to reject the image if there is more than one trusted digest, since at this point in time Connaisseur doesn't have the ability to distinguish between the right and wrong trusted digest. This only occurs in some edge cases, but nonetheless has to be addressed. Let's now focus on the integral part of Connaisseur that is how to get all trusted digests for an image:tag or image:digest combination: Connaisseur looks up trust data of the image in the root , snapshot , timestamp and targets files by querying the API of the notary server. Trust data syntax is validated against their known schemas (for an introduction to the trust data formats see the presentation of our colleague Philipp Belitz ). Then, the files' signatures are validated against the pinned root key for the root file and against the respective keys validated in previous steps for later files. Connaisseur further gathers trust data of potential delegations linked in the targets file. At this point, Connaisseur is left with a set of potentially disjoint or overlapping sets of trust data. Connaisseur filters the trust data for digests that actually relate to the image under validation. If the image policy rule that governs the image to be validated does contain a delegations field, Connaisseur makes sure that all delegations' sets of trust data do contain an entry for the image. If that is not the case, the request is rejected. Subsequently, Connaisseur builds a set over the union of the digests and proceeds with step 2., i.e. accepeting if the size of the resulting set equals 1.","title":"Validation of trust data"},{"location":"adr/","text":"Architecture Decision Records \u2693\ufe0e We strive to make decisions taken during the devlopment of Connaisseur transparent, whenever they may seem weird or unintuitive towards someone new to the project. Hence, when encountering a problem that took either considerable time to find a solution for or that spawned a lot of discussion, be it internal or from the community, the decision with the factors leading up to the particular choice should be documented. Additionally, we should make clear what other options were under consideration and why they were discarded to help both with making the decision comprehensible to people not involved at the time but also to not repeat discussions at a later point in time. Since each Architecture Decision may be slightly different, the format is not completely set in stone. However, you should give at least title, status, some context, decisions taken and options discarded and some reasoning as to why one option was deemed better than the others.","title":"Architecture Decision Records"},{"location":"adr/#architecture-decision-records","text":"We strive to make decisions taken during the devlopment of Connaisseur transparent, whenever they may seem weird or unintuitive towards someone new to the project. Hence, when encountering a problem that took either considerable time to find a solution for or that spawned a lot of discussion, be it internal or from the community, the decision with the factors leading up to the particular choice should be documented. Additionally, we should make clear what other options were under consideration and why they were discarded to help both with making the decision comprehensible to people not involved at the time but also to not repeat discussions at a later point in time. Since each Architecture Decision may be slightly different, the format is not completely set in stone. However, you should give at least title, status, some context, decisions taken and options discarded and some reasoning as to why one option was deemed better than the others.","title":"Architecture Decision Records"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/","text":"ADR 1: Bootstrap Sentinel \u2693\ufe0e Status \u2693\ufe0e Amended in ADR-3 . Context \u2693\ufe0e Connaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods. In #3 it was noted that prior to version 1.1.5 of Connaisseur when looking at the Ready status of Connaisseur Pods, they could report Ready while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed after the Connaisseur Pods, which was solved by checking the Ready state of said Pods. If one were to add a dependency to this Ready state, such that it only shows Ready when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration. Considered options \u2693\ufe0e Option 1 \u2693\ufe0e At the start of the Helm deployment, one can create a Pod named connaisseur-bootstrap-sentinel that will run for 5 minutes (which is also the installation timeout by helm). Connaisseur Pods will report Ready if they can 1) access notary AND 2) the MutatingWebhookConfiguration exists OR 3) the connaisseur-bootstrap-sentinel Pod is still running. If 1) AND 2) both hold true, the sentinel is killed even if the 5 minutes have not passed yet. Option 2 \u2693\ufe0e Let Connaisseur's Pod readiness stay non-indicative of Connaisseur functioning and advertise that someone running Connaisseur has to monitor the MutatingWebhookConfiguration in order to ensure proper working. Option 3 \u2693\ufe0e Deploy MutatingWebhookConfiguration through Helm when Connaisseur Pods are healthy instead of when ready. Require Pod started and working notary connection for health and require additionally the existence of the MutatingWebhookConfiguration for readiness. Decision outcome \u2693\ufe0e We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it. Positive Consequences \u2693\ufe0e If the Connaisseur Pods report Ready during the connaisseur-bootstrap-sentinel 's runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the helm deployment will fail after its timeout period (default: 5min), since there won't be a running connaisseur-bootstrap-sentinel Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the Ready state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working. Negative Consequences \u2693\ufe0e On the other hand, if an adversary can deploy a Pod named connaisseur-bootstrap-sentinel to Connaisseur's Namespace, the Connaisseur Pods will always show Ready regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the connaisseur-bootstrap-sentinel Pod being left behind, however since it has a very limited use-case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.","title":"ADR 1: Bootstrap Sentinel"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#adr-1-bootstrap-sentinel","text":"","title":"ADR 1: Bootstrap Sentinel"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#status","text":"Amended in ADR-3 .","title":"Status"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#context","text":"Connaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods. In #3 it was noted that prior to version 1.1.5 of Connaisseur when looking at the Ready status of Connaisseur Pods, they could report Ready while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed after the Connaisseur Pods, which was solved by checking the Ready state of said Pods. If one were to add a dependency to this Ready state, such that it only shows Ready when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.","title":"Context"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#considered-options","text":"","title":"Considered options"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#option-1","text":"At the start of the Helm deployment, one can create a Pod named connaisseur-bootstrap-sentinel that will run for 5 minutes (which is also the installation timeout by helm). Connaisseur Pods will report Ready if they can 1) access notary AND 2) the MutatingWebhookConfiguration exists OR 3) the connaisseur-bootstrap-sentinel Pod is still running. If 1) AND 2) both hold true, the sentinel is killed even if the 5 minutes have not passed yet.","title":"Option 1"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#option-2","text":"Let Connaisseur's Pod readiness stay non-indicative of Connaisseur functioning and advertise that someone running Connaisseur has to monitor the MutatingWebhookConfiguration in order to ensure proper working.","title":"Option 2"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#option-3","text":"Deploy MutatingWebhookConfiguration through Helm when Connaisseur Pods are healthy instead of when ready. Require Pod started and working notary connection for health and require additionally the existence of the MutatingWebhookConfiguration for readiness.","title":"Option 3"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#decision-outcome","text":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.","title":"Decision outcome"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#positive-consequences","text":"If the Connaisseur Pods report Ready during the connaisseur-bootstrap-sentinel 's runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the helm deployment will fail after its timeout period (default: 5min), since there won't be a running connaisseur-bootstrap-sentinel Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the Ready state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.","title":"Positive Consequences"},{"location":"adr/ADR-1_connaisseur-bootstrap-sentinel/#negative-consequences","text":"On the other hand, if an adversary can deploy a Pod named connaisseur-bootstrap-sentinel to Connaisseur's Namespace, the Connaisseur Pods will always show Ready regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the connaisseur-bootstrap-sentinel Pod being left behind, however since it has a very limited use-case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.","title":"Negative Consequences"},{"location":"adr/ADR-2_release-management/","text":"ADR 2: Release Management \u2693\ufe0e Status \u2693\ufe0e Proposed Context \u2693\ufe0e During its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart. A single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility. Considered options \u2693\ufe0e Choice 1 \u2693\ufe0e What branches to maintain Option 1 \u2693\ufe0e Continue with PRs from personal feature branches to master . Option 2 \u2693\ufe0e Have a development branch against which to create pull requests (during usual development, hotfixes may be different). Sub-options: - a develop (or similar) branch that will exist continuously - a v.1.5.0_dev (or similar) branch for each respective version Choice 2 \u2693\ufe0e Where to sign the images Option 1 \u2693\ufe0e Have the pipeline build, sign and push the images. Option 2 \u2693\ufe0e Have a maintainer build, sign and push the images. Decision outcome \u2693\ufe0e For choice 1, we decided to go for two branches. On the one hand, master being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a develop branch that hosts the current state of development and will be merged to master whenever we want to create a new release. This way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release. In the process of automating most of the release process, we will run an integration test with locally built images for pull requests to master . Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the master branch referencing the new release version. After the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working. We decided for this option as it does not expose credentials to Github Actions, which we wanted to avoid especially in light of the recent Github Actions injection attacks and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys. Positive Consequences \u2693\ufe0e We can develop without having to ship changes immediatly. Release process does not expose credentials to Github Actions. Code gets git tags. Negative Consequences \u2693\ufe0e Process from code to release for a single change is more cumbersome than right now. Release still requires human intervention.","title":"ADR 2: Release Management"},{"location":"adr/ADR-2_release-management/#adr-2-release-management","text":"","title":"ADR 2: Release Management"},{"location":"adr/ADR-2_release-management/#status","text":"Proposed","title":"Status"},{"location":"adr/ADR-2_release-management/#context","text":"During its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart. A single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.","title":"Context"},{"location":"adr/ADR-2_release-management/#considered-options","text":"","title":"Considered options"},{"location":"adr/ADR-2_release-management/#choice-1","text":"What branches to maintain","title":"Choice 1"},{"location":"adr/ADR-2_release-management/#option-1","text":"Continue with PRs from personal feature branches to master .","title":"Option 1"},{"location":"adr/ADR-2_release-management/#option-2","text":"Have a development branch against which to create pull requests (during usual development, hotfixes may be different). Sub-options: - a develop (or similar) branch that will exist continuously - a v.1.5.0_dev (or similar) branch for each respective version","title":"Option 2"},{"location":"adr/ADR-2_release-management/#choice-2","text":"Where to sign the images","title":"Choice 2"},{"location":"adr/ADR-2_release-management/#option-1_1","text":"Have the pipeline build, sign and push the images.","title":"Option 1"},{"location":"adr/ADR-2_release-management/#option-2_1","text":"Have a maintainer build, sign and push the images.","title":"Option 2"},{"location":"adr/ADR-2_release-management/#decision-outcome","text":"For choice 1, we decided to go for two branches. On the one hand, master being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a develop branch that hosts the current state of development and will be merged to master whenever we want to create a new release. This way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release. In the process of automating most of the release process, we will run an integration test with locally built images for pull requests to master . Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the master branch referencing the new release version. After the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working. We decided for this option as it does not expose credentials to Github Actions, which we wanted to avoid especially in light of the recent Github Actions injection attacks and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.","title":"Decision outcome"},{"location":"adr/ADR-2_release-management/#positive-consequences","text":"We can develop without having to ship changes immediatly. Release process does not expose credentials to Github Actions. Code gets git tags.","title":"Positive Consequences"},{"location":"adr/ADR-2_release-management/#negative-consequences","text":"Process from code to release for a single change is more cumbersome than right now. Release still requires human intervention.","title":"Negative Consequences"},{"location":"adr/ADR-3_multi_notary_config/","text":"ADR 3: Multiple Notary Configuration \u2693\ufe0e Status \u2693\ufe0e Accepted Context \u2693\ufe0e Previously Connaisseur only supported the configuration of a single notary, where all signature data had to reside in. Unfortunately this is rather impractical, as one doesn't create all signatures for all images one uses in a cluster. There is a need to access signature data from multiple places, like in a setup where most images come from a private registry + notary and some from DockerHub and their notary. There is also the problem that a single notary instance could use multiple root keys, used for creating the signatures, like in the case of DockerHub. Connaisseur also only supports a single root key to be trust pinned, thus making it impractical. That's why the decision was made to support more than one notary and multiple keys per notary, which leads to the question how the new configuration should look like. This also has implications on the notary health check, which is important for Connaisseur's own readiness check. Considered options \u2693\ufe0e Choice 1 \u2693\ufe0e The overall notary configuration setup in helm/values.yaml . Option 1 (Per Notary) \u2693\ufe0e The notary field becomes a list and changes to notaries . Per to be used notary instance, there will be one entry in this list. The entry will have the following data fields ( bold are mandatory): name -- A unique identifier for the notary configuration, which will be used in the image policy. host -- The host address of the notary instance. pub_root_keys -- A list of public root keys, which are to be used for signature verification. name -- An identifier for a single public root key, which will be used in the image policy. key -- The actual public root key in PEM format. selfsigned_cert -- A self-signed certificate in PEM format, for making secure TLS connection to the notary. auth -- Authentication details, should the notary require some. user -- Username to authenticate with. password -- Password to authenticate with. secretName -- Kubernetes secret reference to use INSTEAD of user/password combination. is_acr -- Marks the notary as being part of an Azure Container Registry. The image policy will have two additional fields per rule entry (in \"quotes\" are already present fields): \" pattern \" -- Image pattern to match against, for rule to apply. \" verify \" -- Whether the images should be verified or not. \" delegations \" -- List of required delegation roles. notary -- Which notary to use for any matching image. This has to correspond to a name field of one configured notary. What happens if none is given, is defined by the result of choice 2. key -- Which key to use for doing the signature verification. This has to correspond to a name field of one of the public keys configured for the notary corresponding to the image policy's notary field. What happens if none is given, is defined by the result of choice 2. Option 2 (Per Notary + Key) \u2693\ufe0e The notary field becomes a list and changes to notaries . Per notary + public root key combination, there is one entry. Meaning, for example, there will be one entry for DockerHub and the public key for all official images and there will be another entry for DockerHub and the public key for some private images. The entries will look identical to the one's from option 1, with two exceptions. The pub_root_keys field of the notary configurations won't be a list and only has a single entry, without needing to specify a key name. The image policy will only address the notary configuration to be chosen with the notary field, without the need for a key field. Choice 2 \u2693\ufe0e Default values for notary (and key ) inside the image policy. Option 1 (First item) \u2693\ufe0e When no notary is specified in a image policy rule, the first entry in the notaries configuration list is taken. The same goes for the public root key list, should option 1 for choice 1 be chosen. Problem: Might get inconsistent, should list ordering in python get shuffled around Option 2 (Explicit default) \u2693\ufe0e One of the notary configuration will be given a default field, which marks it as the default value. Problem: No real problems here, just an extra field that the user has to care about. Option 3 (Mandatory Notary) \u2693\ufe0e The notary (and potentially key ) field is mandatory for the image policy. Problem: Creates configuration overhead if many image policies use the same notary/key combination. Option 4 (Default name) \u2693\ufe0e If no notary or key are given in the image policy, it is assumed that one of the elements in the notary list or key list has name: \"default\" , which will then be taken. Should the assumption be wrong, an error is raised. Choice 3 \u2693\ufe0e Previously, the readiness probe for connaisseur also considered the notary's health for its own status. With multiple notary instances configured, this behavior changes. Option 1 (Ignore Notary) \u2693\ufe0e The readiness probe of Connaisseur will no longer be dependent on any notary health checks. The are completely decoupled. Problem: No knowledge that Connaisseur will automatically fail because of an unreachable notary, before one tries to deploy an image. Option 2 (Health check on all) \u2693\ufe0e In order for connaisseur to be ready, all configured notaries must be healthy and reachable. Problem: A single unreachable notary will \"disable\" Connaisseur's access to all others. Option 3 (Log Notary status) \u2693\ufe0e A mix of option 1 and 2, whereas the readiness of Connaisseur is independent of the notaries health check, but they are still being made, so unhealthy notaries can be logged. Problem: At what interval should be logged? Decision outcome \u2693\ufe0e Choice 1 \u2693\ufe0e Option 1 was chosen, to keep configurational duplication at a minimum. Choice 2 \u2693\ufe0e Option 4 was chosen. If more than one notary configuration or key within a configuration are present, one of those can be called \"default\" (setting the name field). That way it should be obvious enough, which configuration or key will be used, if not further specified within the image policy, while keeping configuration effort low. Choice 3 \u2693\ufe0e Option 3 was chosen. Notary and Connaisseur will be completely decoupled, with Connaisseur logging all notaries it can't reach. This way Connaisseur can still be operational, even with all notaries being unreachable. Otherwise Connaisseur would have blocked even images that were allowlisted. This is a breaking change, but we agreed that it is better as it allows e.g. deployments for which the respective image policy specifies verify: false .","title":"ADR 3: Multiple Notary Configuration"},{"location":"adr/ADR-3_multi_notary_config/#adr-3-multiple-notary-configuration","text":"","title":"ADR 3: Multiple Notary Configuration"},{"location":"adr/ADR-3_multi_notary_config/#status","text":"Accepted","title":"Status"},{"location":"adr/ADR-3_multi_notary_config/#context","text":"Previously Connaisseur only supported the configuration of a single notary, where all signature data had to reside in. Unfortunately this is rather impractical, as one doesn't create all signatures for all images one uses in a cluster. There is a need to access signature data from multiple places, like in a setup where most images come from a private registry + notary and some from DockerHub and their notary. There is also the problem that a single notary instance could use multiple root keys, used for creating the signatures, like in the case of DockerHub. Connaisseur also only supports a single root key to be trust pinned, thus making it impractical. That's why the decision was made to support more than one notary and multiple keys per notary, which leads to the question how the new configuration should look like. This also has implications on the notary health check, which is important for Connaisseur's own readiness check.","title":"Context"},{"location":"adr/ADR-3_multi_notary_config/#considered-options","text":"","title":"Considered options"},{"location":"adr/ADR-3_multi_notary_config/#choice-1","text":"The overall notary configuration setup in helm/values.yaml .","title":"Choice 1"},{"location":"adr/ADR-3_multi_notary_config/#option-1-per-notary","text":"The notary field becomes a list and changes to notaries . Per to be used notary instance, there will be one entry in this list. The entry will have the following data fields ( bold are mandatory): name -- A unique identifier for the notary configuration, which will be used in the image policy. host -- The host address of the notary instance. pub_root_keys -- A list of public root keys, which are to be used for signature verification. name -- An identifier for a single public root key, which will be used in the image policy. key -- The actual public root key in PEM format. selfsigned_cert -- A self-signed certificate in PEM format, for making secure TLS connection to the notary. auth -- Authentication details, should the notary require some. user -- Username to authenticate with. password -- Password to authenticate with. secretName -- Kubernetes secret reference to use INSTEAD of user/password combination. is_acr -- Marks the notary as being part of an Azure Container Registry. The image policy will have two additional fields per rule entry (in \"quotes\" are already present fields): \" pattern \" -- Image pattern to match against, for rule to apply. \" verify \" -- Whether the images should be verified or not. \" delegations \" -- List of required delegation roles. notary -- Which notary to use for any matching image. This has to correspond to a name field of one configured notary. What happens if none is given, is defined by the result of choice 2. key -- Which key to use for doing the signature verification. This has to correspond to a name field of one of the public keys configured for the notary corresponding to the image policy's notary field. What happens if none is given, is defined by the result of choice 2.","title":"Option 1 (Per Notary)"},{"location":"adr/ADR-3_multi_notary_config/#option-2-per-notary-key","text":"The notary field becomes a list and changes to notaries . Per notary + public root key combination, there is one entry. Meaning, for example, there will be one entry for DockerHub and the public key for all official images and there will be another entry for DockerHub and the public key for some private images. The entries will look identical to the one's from option 1, with two exceptions. The pub_root_keys field of the notary configurations won't be a list and only has a single entry, without needing to specify a key name. The image policy will only address the notary configuration to be chosen with the notary field, without the need for a key field.","title":"Option 2 (Per Notary + Key)"},{"location":"adr/ADR-3_multi_notary_config/#choice-2","text":"Default values for notary (and key ) inside the image policy.","title":"Choice 2"},{"location":"adr/ADR-3_multi_notary_config/#option-1-first-item","text":"When no notary is specified in a image policy rule, the first entry in the notaries configuration list is taken. The same goes for the public root key list, should option 1 for choice 1 be chosen. Problem: Might get inconsistent, should list ordering in python get shuffled around","title":"Option 1 (First item)"},{"location":"adr/ADR-3_multi_notary_config/#option-2-explicit-default","text":"One of the notary configuration will be given a default field, which marks it as the default value. Problem: No real problems here, just an extra field that the user has to care about.","title":"Option 2 (Explicit default)"},{"location":"adr/ADR-3_multi_notary_config/#option-3-mandatory-notary","text":"The notary (and potentially key ) field is mandatory for the image policy. Problem: Creates configuration overhead if many image policies use the same notary/key combination.","title":"Option 3 (Mandatory Notary)"},{"location":"adr/ADR-3_multi_notary_config/#option-4-default-name","text":"If no notary or key are given in the image policy, it is assumed that one of the elements in the notary list or key list has name: \"default\" , which will then be taken. Should the assumption be wrong, an error is raised.","title":"Option 4 (Default name)"},{"location":"adr/ADR-3_multi_notary_config/#choice-3","text":"Previously, the readiness probe for connaisseur also considered the notary's health for its own status. With multiple notary instances configured, this behavior changes.","title":"Choice 3"},{"location":"adr/ADR-3_multi_notary_config/#option-1-ignore-notary","text":"The readiness probe of Connaisseur will no longer be dependent on any notary health checks. The are completely decoupled. Problem: No knowledge that Connaisseur will automatically fail because of an unreachable notary, before one tries to deploy an image.","title":"Option 1 (Ignore Notary)"},{"location":"adr/ADR-3_multi_notary_config/#option-2-health-check-on-all","text":"In order for connaisseur to be ready, all configured notaries must be healthy and reachable. Problem: A single unreachable notary will \"disable\" Connaisseur's access to all others.","title":"Option 2 (Health check on all)"},{"location":"adr/ADR-3_multi_notary_config/#option-3-log-notary-status","text":"A mix of option 1 and 2, whereas the readiness of Connaisseur is independent of the notaries health check, but they are still being made, so unhealthy notaries can be logged. Problem: At what interval should be logged?","title":"Option 3 (Log Notary status)"},{"location":"adr/ADR-3_multi_notary_config/#decision-outcome","text":"","title":"Decision outcome"},{"location":"adr/ADR-3_multi_notary_config/#choice-1_1","text":"Option 1 was chosen, to keep configurational duplication at a minimum.","title":"Choice 1"},{"location":"adr/ADR-3_multi_notary_config/#choice-2_1","text":"Option 4 was chosen. If more than one notary configuration or key within a configuration are present, one of those can be called \"default\" (setting the name field). That way it should be obvious enough, which configuration or key will be used, if not further specified within the image policy, while keeping configuration effort low.","title":"Choice 2"},{"location":"adr/ADR-3_multi_notary_config/#choice-3_1","text":"Option 3 was chosen. Notary and Connaisseur will be completely decoupled, with Connaisseur logging all notaries it can't reach. This way Connaisseur can still be operational, even with all notaries being unreachable. Otherwise Connaisseur would have blocked even images that were allowlisted. This is a breaking change, but we agreed that it is better as it allows e.g. deployments for which the respective image policy specifies verify: false .","title":"Choice 3"},{"location":"features/","text":"Overview \u2693\ufe0e intro to features","title":"Overview"},{"location":"features/#overview","text":"intro to features","title":"Overview"},{"location":"features/alerting/","text":"Alerting \u2693\ufe0e description of alerting","title":"Alerting"},{"location":"features/alerting/#alerting","text":"description of alerting","title":"Alerting"},{"location":"features/detection_mode/","text":"Detection Mode \u2693\ufe0e description of detection mode","title":"Detection Mode"},{"location":"features/detection_mode/#detection-mode","text":"description of detection mode","title":"Detection Mode"},{"location":"features/namespaced_validation/","text":"Namespaced Validation \u2693\ufe0e description of namespaced validation","title":"Namespaced Validation"},{"location":"features/namespaced_validation/#namespaced-validation","text":"description of namespaced validation","title":"Namespaced Validation"},{"location":"validators/","text":"Overview \u2693\ufe0e intro to the validators","title":"Overview"},{"location":"validators/#overview","text":"intro to the validators","title":"Overview"},{"location":"validators/ADVANCED/","text":"Advanced Use-Cases \u2693\ufe0e This document details some more advanced use-cases of Connaisseur. Delegation feature \u2693\ufe0e Signatures in Docker Content Trust or Notary v1 are based on The Update Framework (TUF) . TUF allows delegating signature permissions to other people. Connaisseur supports requiring delegations for Notary v1 via the policy definition: Image Policy Example \u2693\ufe0e Let's say you have an organizational root key pinned in the helm/values.yaml , which you use to delegate to individual developers, i.e. via docker trust key load delegation.key --name charlie (see the Docker documentation for the docker CLI commands). Now, given an image policy policy : - pattern : \"*:*\" the signature of every single developer to whom you delegated signature rights will be considered valid. That might be fine for most use-cases, but for trusted-thing , which runs as a privileged container on all k8s nodes, you really want to limit yourself to Alice from the security team, whom you trust to have properly reviewed it. And also to Bob, your most senior developer, because they got that eye for detail and they really don't do the \"It's friday, force push master, I'm outta here\" stuff, that Charlie did last month... Anyways, Connaisseur can handle this: policy : - pattern : \"*:*\" - pattern : \"your.org/trusted-thing:*\" delegations : [ \"alice\" , \"bob\" ] Connaisseur will make sure, that both Alice and Bob have a current signature on the same digest for any tagged trusted-thing image you deploy. If one of their signatures is missing or if they point to different digests for the same tag, Connaisseur will block the deployment. In the meantime, for all other images any delegated individual key, repository key or the root key will be accepted. The list you add in the delegations field can contain any number of delegations. Just remember that it is a logical AND , so all delegations will have to verify correctly. Another potential use-case is to use delegations to sign an image in various stages of your CI, i.e. requiring the signature of your linter, your security scanner and your software lisence compliance check. Limits \u2693\ufe0e Q: Is a logical OR possible? Are there n-of-m thresholds? Is there insert-arbitrary-logical-function ? A: No, currently not. If you have the use-case, feel free to hit us up , open an issue or open a pull request. Q: Will delegations work with Cosign or Notary v2? A: Currently, Cosign does not natively support delegations, so Connaisseur doesn't either. For Notary v2, it looks like they will support delegations and so Connaisseur will most likely support them as in Notary v1.","title":"Advanced Use-Cases"},{"location":"validators/ADVANCED/#advanced-use-cases","text":"This document details some more advanced use-cases of Connaisseur.","title":"Advanced Use-Cases"},{"location":"validators/ADVANCED/#delegation-feature","text":"Signatures in Docker Content Trust or Notary v1 are based on The Update Framework (TUF) . TUF allows delegating signature permissions to other people. Connaisseur supports requiring delegations for Notary v1 via the policy definition:","title":"Delegation feature"},{"location":"validators/ADVANCED/#image-policy-example","text":"Let's say you have an organizational root key pinned in the helm/values.yaml , which you use to delegate to individual developers, i.e. via docker trust key load delegation.key --name charlie (see the Docker documentation for the docker CLI commands). Now, given an image policy policy : - pattern : \"*:*\" the signature of every single developer to whom you delegated signature rights will be considered valid. That might be fine for most use-cases, but for trusted-thing , which runs as a privileged container on all k8s nodes, you really want to limit yourself to Alice from the security team, whom you trust to have properly reviewed it. And also to Bob, your most senior developer, because they got that eye for detail and they really don't do the \"It's friday, force push master, I'm outta here\" stuff, that Charlie did last month... Anyways, Connaisseur can handle this: policy : - pattern : \"*:*\" - pattern : \"your.org/trusted-thing:*\" delegations : [ \"alice\" , \"bob\" ] Connaisseur will make sure, that both Alice and Bob have a current signature on the same digest for any tagged trusted-thing image you deploy. If one of their signatures is missing or if they point to different digests for the same tag, Connaisseur will block the deployment. In the meantime, for all other images any delegated individual key, repository key or the root key will be accepted. The list you add in the delegations field can contain any number of delegations. Just remember that it is a logical AND , so all delegations will have to verify correctly. Another potential use-case is to use delegations to sign an image in various stages of your CI, i.e. requiring the signature of your linter, your security scanner and your software lisence compliance check.","title":"Image Policy Example"},{"location":"validators/ADVANCED/#limits","text":"Q: Is a logical OR possible? Are there n-of-m thresholds? Is there insert-arbitrary-logical-function ? A: No, currently not. If you have the use-case, feel free to hit us up , open an issue or open a pull request. Q: Will delegations work with Cosign or Notary v2? A: Currently, Cosign does not natively support delegations, so Connaisseur doesn't either. For Notary v2, it looks like they will support delegations and so Connaisseur will most likely support them as in Notary v1.","title":"Limits"},{"location":"validators/notaryv1/","text":"Notary v1 \u2693\ufe0e nv1 guide Additional notes \u2693\ufe0e Using Harbor container registry \u2693\ufe0e Using Azure Container Registry \u2693\ufe0e Azure Container Registry (ACR)","title":"Notary v1"},{"location":"validators/notaryv1/#notary-v1","text":"nv1 guide","title":"Notary v1"},{"location":"validators/notaryv1/#additional-notes","text":"","title":"Additional notes"},{"location":"validators/notaryv1/#using-harbor-container-registry","text":"","title":"Using Harbor container registry"},{"location":"validators/notaryv1/#using-azure-container-registry","text":"Azure Container Registry (ACR)","title":"Using Azure Container Registry"},{"location":"validators/notaryv2/","text":"Notary v2 \u2693\ufe0e nv2 guide","title":"Notary v2"},{"location":"validators/notaryv2/#notary-v2","text":"nv2 guide","title":"Notary v2"},{"location":"validators/sigstore_cosign/","text":"SigStore / Cosign \u2693\ufe0e Sigstore / Cosign guide","title":"SigStore / Cosign"},{"location":"validators/sigstore_cosign/#sigstore-cosign","text":"Sigstore / Cosign guide","title":"SigStore / Cosign"}]}